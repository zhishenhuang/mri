{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e097de06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.fft as F\n",
    "from importlib import reload\n",
    "from torch.nn.functional import relu\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Func\n",
    "import torch.optim as optim\n",
    "import utils\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import copy\n",
    "\n",
    "from utils import *\n",
    "from mnet import MNet\n",
    "import mask_backward_v4\n",
    "from mask_backward_v4 import *\n",
    "import alternate_train_v2\n",
    "from alternate_train_v2 import alternating_update_with_unetRecon\n",
    "sys.path.insert(0,'/home/huangz78/mri/unet/')\n",
    "from unet_model import UNet\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e67e0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size: torch.Size([12649, 320, 320])\n",
      "validation data size: torch.Size([1287, 320, 320])\n"
     ]
    }
   ],
   "source": [
    "# load training data\n",
    "train_dir = '/mnt/shared_a/data/fastMRI/knee_singlecoil_train.npz'\n",
    "train_xfull = torch.tensor(np.load(train_dir)['data']).to(torch.float)\n",
    "train_yfull = None\n",
    "for ind in range(train_xfull.shape[0]):\n",
    "    train_xfull[ind,:,:] = train_xfull[ind,:,:]/torch.max(train_xfull[ind,:,:])\n",
    "print('train data size:', train_xfull.shape)\n",
    "\n",
    "# load validation data \n",
    "val_dir = '/mnt/shared_a/data/fastMRI/knee_singlecoil_val.npz'\n",
    "val_xfull = torch.tensor(np.load(val_dir)['data']).to(torch.float)\n",
    "val_yfull = None\n",
    "for ind in range(val_xfull.shape[0]):\n",
    "    val_xfull[ind,:,:] = val_xfull[ind,:,:]/torch.max(val_xfull[ind,:,:])\n",
    "print('validation data size:', val_xfull.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07323063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn's are ready\n"
     ]
    }
   ],
   "source": [
    "skip = True\n",
    "ngpu = 1\n",
    "base_freq = 8\n",
    "budget = 32\n",
    "ngpu = 1\n",
    "maxItermb = 30\n",
    "alpha = 2e-5\n",
    "c = 1e-3\n",
    "lrb = 1e-2\n",
    "mnRep = 100\n",
    "lrn = 1e-4\n",
    "epochs = 1\n",
    "batchsize = 5\n",
    "validate_every = 100\n",
    "n_channels = 1\n",
    "epoch_start = 0\n",
    "batchind_start = 0\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "histpath = None\n",
    "### load a mnet\n",
    "mnet = MNet(beta=1,in_channels=2,out_size=320-base_freq, imgsize=(320,320),poolk=3).to(device)\n",
    "# mnetpath = '/home/huangz78/checkpoints/mnet_split_trained_cf_8_bg_32_unet_in_chan_1.pt'\n",
    "# checkpoint = torch.load(mnetpath)\n",
    "# mnet.load_state_dict(checkpoint['model_state_dict'])\n",
    "# print('MNet loaded successfully from: ' + mnetpath)\n",
    "mnet.eval()\n",
    "\n",
    "### load a unet for maskbackward\n",
    "unet = UNet(n_channels=n_channels,n_classes=1,bilinear=(not skip),skip=skip).to(device)\n",
    "# unetpath = '/home/huangz78/checkpoints/unet_split_trained_cf_8_bg_32_unet_in_chan_1.pt'\n",
    "# checkpoint = torch.load(unetpath)\n",
    "# unet.load_state_dict(checkpoint['model_state_dict'])\n",
    "# print('Unet loaded successfully from: ' + unetpath )\n",
    "unet.train()\n",
    "print('nn\\'s are ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76072e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(alternate_train_v2)\n",
    "from alternate_train_v2 import alternating_update_with_unetRecon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bb933e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corefreq = 8, budget = 32, this is a 8-fold training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/pyenv/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448224956/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss of the input mask:  0.6517341732978821\n",
      "Iter 1, rows added: 0.0, rows reducted: 23.0, current samp. ratio: 0.053125\n",
      "Iter 2, rows added: 37.0, rows reducted: 2.6, current samp. ratio: 0.160625\n",
      "Iter 3, rows added: 2.8, rows reducted: 26.0, current samp. ratio: 0.088125\n",
      "Iter 4, rows added: 9.6, rows reducted: 7.6, current samp. ratio: 0.094375\n",
      "Iter 5, rows added: 8.0, rows reducted: 1.8, current samp. ratio: 0.11375\n",
      "Iter 6, rows added: 1.6, rows reducted: 13.4, current samp. ratio: 0.076875\n",
      "Iter 7, rows added: 5.0, rows reducted: 0.2, current samp. ratio: 0.091875\n",
      "Iter 8, rows added: 1.6, rows reducted: 7.2, current samp. ratio: 0.074375\n",
      "Iter 9, rows added: 2.0, rows reducted: 4.2, current samp. ratio: 0.0675\n",
      "Iter 10, rows added: 1.2, rows reducted: 6.2, current samp. ratio: 0.051875\n",
      "Iter 11, rows added: 3.0, rows reducted: 0.0, current samp. ratio: 0.06125\n",
      "Iter 12, rows added: 2.0, rows reducted: 7.0, current samp. ratio: 0.045625\n",
      "Iter 13, rows added: 1.0, rows reducted: 0.4, current samp. ratio: 0.0475\n",
      "Iter 14, rows added: 0.4, rows reducted: 1.8, current samp. ratio: 0.043125\n",
      "Iter 15, rows added: 1.4, rows reducted: 0.2, current samp. ratio: 0.046875\n",
      "Iter 16, rows added: 0.0, rows reducted: 4.2, current samp. ratio: 0.03375\n",
      "Iter 17, rows added: 1.0, rows reducted: 0.0, current samp. ratio: 0.036875\n",
      "Iter 18, rows added: 0.0, rows reducted: 1.4, current samp. ratio: 0.0325\n",
      "Iter 19, rows added: 0.0, rows reducted: 0.4, current samp. ratio: 0.03125\n",
      "Iter 20, rows added: 0.2, rows reducted: 1.2, current samp. ratio: 0.028125\n",
      "Iter 21, rows added: 1.4, rows reducted: 0.2, current samp. ratio: 0.031875\n",
      "Iter 22, rows added: 0.0, rows reducted: 1.4, current samp. ratio: 0.0275\n",
      "Iter 23, rows added: 1.6, rows reducted: 0.2, current samp. ratio: 0.031875\n",
      "Iter 24, rows added: 1.2, rows reducted: 1.8, current samp. ratio: 0.03\n",
      "Iter 25, rows added: 2.0, rows reducted: 1.2, current samp. ratio: 0.0325\n",
      "Iter 26, rows added: 1.0, rows reducted: 1.2, current samp. ratio: 0.031875\n",
      "Iter 27, rows added: 2.2, rows reducted: 1.4, current samp. ratio: 0.034375\n",
      "Iter 28, rows added: 1.2, rows reducted: 1.6, current samp. ratio: 0.033125\n",
      "Iter 29, rows added: 3.0, rows reducted: 1.8, current samp. ratio: 0.036875\n",
      "Iter 30, rows added: 1.0, rows reducted: 1.2, current samp. ratio: 0.03625\n",
      "\n",
      "return at Iter ind:  30\n",
      "samp. ratio: 0.125, loss of returned mask: 0.10585986375808716 \n",
      "\n",
      "[1][1/1][5/12649], quality of old mnet mask : 0.6517341732978821\n",
      "[1][1/1][5/12649], quality of refined  mask : 0.10585986375808716\n",
      "[1][1/1][5/12649], quality of random   mask : 0.7970766425132751\n",
      "[1][1/1][5/12649] is a VALID step!\n",
      "\n",
      "loss of the input mask:  0.8666940927505493\n",
      "Iter 1, rows added: 0.0, rows reducted: 4.2, current samp. ratio: 0.111875\n",
      "Iter 2, rows added: 0.0, rows reducted: 0.8, current samp. ratio: 0.109375\n",
      "Iter 3, rows added: 0.2, rows reducted: 0.0, current samp. ratio: 0.11\n",
      "Iter 4, rows added: 0.0, rows reducted: 0.2, current samp. ratio: 0.109375\n",
      "Iter 8, rows added: 0.0, rows reducted: 0.2, current samp. ratio: 0.10875\n",
      "Iter 14, rows added: 0.0, rows reducted: 0.2, current samp. ratio: 0.108125\n",
      "Iter 17, rows added: 0.2, rows reducted: 0.0, current samp. ratio: 0.10875\n",
      "Iter 18, rows added: 0.0, rows reducted: 0.4, current samp. ratio: 0.1075\n",
      "Iter 19, rows added: 0.2, rows reducted: 0.0, current samp. ratio: 0.108125\n",
      "Iter 20, rows added: 0.0, rows reducted: 0.2, current samp. ratio: 0.1075\n",
      "Iter 29, rows added: 0.4, rows reducted: 0.2, current samp. ratio: 0.108125\n",
      "Iter 30, rows added: 0.0, rows reducted: 0.4, current samp. ratio: 0.106875\n",
      "\n",
      "return at Iter ind:  30\n",
      "samp. ratio: 0.125, loss of returned mask: inf \n",
      "\n",
      "[2][1/1][10/12649], quality of old mnet mask : 0.8666940927505493\n",
      "[2][1/1][10/12649], quality of refined  mask : inf\n",
      "[2][1/1][10/12649], quality of random   mask : 0.48156219720840454\n",
      "[2][1/1][10/12649] is an invalid step!\n",
      "\n",
      "loss of the input mask:  0.53263258934021\n",
      "Iter 1, rows added: 0.0, rows reducted: 2.4, current samp. ratio: 0.1175\n",
      "Iter 2, rows added: 0.0, rows reducted: 0.4, current samp. ratio: 0.11625\n",
      "Iter 6, rows added: 0.0, rows reducted: 0.2, current samp. ratio: 0.115625\n",
      "Iter 11, rows added: 0.0, rows reducted: 0.2, current samp. ratio: 0.115\n",
      "Iter 15, rows added: 0.0, rows reducted: 0.2, current samp. ratio: 0.114375\n",
      "Iter 16, rows added: 0.4, rows reducted: 0.0, current samp. ratio: 0.115625\n",
      "Iter 17, rows added: 0.0, rows reducted: 0.4, current samp. ratio: 0.114375\n",
      "Iter 19, rows added: 0.0, rows reducted: 0.2, current samp. ratio: 0.11375\n",
      "Iter 20, rows added: 0.2, rows reducted: 0.0, current samp. ratio: 0.114375\n",
      "Iter 21, rows added: 0.2, rows reducted: 0.2, current samp. ratio: 0.114375\n",
      "Iter 23, rows added: 0.2, rows reducted: 0.2, current samp. ratio: 0.114375\n",
      "Iter 25, rows added: 0.2, rows reducted: 0.2, current samp. ratio: 0.114375\n",
      "Iter 26, rows added: 0.2, rows reducted: 0.0, current samp. ratio: 0.115\n",
      "Iter 27, rows added: 0.0, rows reducted: 0.4, current samp. ratio: 0.11375\n",
      "Iter 28, rows added: 0.2, rows reducted: 0.0, current samp. ratio: 0.114375\n",
      "Iter 29, rows added: 0.2, rows reducted: 0.2, current samp. ratio: 0.114375\n",
      "Iter 30, rows added: 0.2, rows reducted: 0.0, current samp. ratio: 0.115\n",
      "\n",
      "return at Iter ind:  30\n",
      "samp. ratio: 0.125, loss of returned mask: 0.4394339621067047 \n",
      "\n",
      "[3][1/1][15/12649], quality of old mnet mask : 0.53263258934021\n",
      "[3][1/1][15/12649], quality of refined  mask : 0.4394339621067047\n",
      "[3][1/1][15/12649], quality of random   mask : 0.4463469684123993\n",
      "[3][1/1][15/12649] is a VALID step!\n",
      "\n",
      "loss of the input mask:  1.8704233169555664\n"
     ]
    }
   ],
   "source": [
    "acceleration_fold = str(int(train_xfull.shape[1]/(base_freq+budget)))\n",
    "print(f'corefreq = {base_freq}, budget = {budget}, this is a {acceleration_fold}-fold training!')\n",
    "\n",
    "alpha = 2e-5\n",
    "c     = 1e-3\n",
    "lrb   = 5e-3 # 5e-3\n",
    "\n",
    "alternating_update_with_unetRecon(mnet,unet,train_xfull,val_xfull,train_yfulls=train_yfull,val_yfulls=val_yfull,\\\n",
    "                              maxIter_mb=maxItermb,alpha=alpha,c=c,lr_mb=lrb,\\\n",
    "                              maxRep=mnRep,lr_mn=lrn,\\\n",
    "                              corefreq=base_freq,budget=budget,\\\n",
    "                              epoch=epochs,batchsize=batchsize,\\\n",
    "#                               validate_every=validate_every,\\\n",
    "                              verbose=True,save_cp=False,count_start=(epoch_start,batchind_start),\\\n",
    "                              histpath=histpath,hfen=False,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978647c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "binarize = ThresholdBinarizeMask().apply\n",
    "fullmask = binarize(mask_complete( raw_normalize(torch.sigmoid(1.*highmask_refined),budget,device=device),320,dtyp=dtyp,device=device))\n",
    "kplot(fullmask[0,:].cpu().detach())\n",
    "p (fullmask[0,:]-fullmask[1,:]).abs().sum()\n",
    "\n",
    "hmask = highmask_refined.clone().detach()\n",
    "fmask = mask_complete( hmask,320,rolled=True,device='cuda:0')\n",
    "kplot(fmask[0,:].cpu().detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9de5da",
   "metadata": {},
   "source": [
    "## arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b22250",
   "metadata": {},
   "outputs": [],
   "source": [
    "### load a mnet\n",
    "mnet = MNet(beta=1,in_channels=2,out_size=320-24, imgsize=(320,320),poolk=3)\n",
    "mnetpath = '/home/huangz78/checkpoints/mnet.pth'\n",
    "checkpoint = torch.load(mnetpath)\n",
    "mnet.load_state_dict(checkpoint['model_state_dict'])\n",
    "mnet.eval()\n",
    "print('MNet loaded successfully from: ' + mnetpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11ae978",
   "metadata": {},
   "outputs": [],
   "source": [
    "### load a unet for maskbackward\n",
    "# UNET = UNet(n_channels=1,n_classes=1,bilinear=True,skip=False)\n",
    "# unetpath = '/home/huangz78/checkpoints/unet_'+ str(UNET.n_channels) +'.pth'\n",
    "# unetpath = '/home/huangz78/checkpoints/unet_1_False.pth'\n",
    "\n",
    "UNET = UNet(n_channels=1,n_classes=1,bilinear=False,skip=True)\n",
    "unetpath = '/home/huangz78/checkpoints/unet_1_True.pth'\n",
    "checkpoint = torch.load(unetpath)\n",
    "UNET.load_state_dict(checkpoint['model_state_dict'])\n",
    "print('Unet loaded successfully from: ' + unetpath )\n",
    "UNET.train()\n",
    "print('nn\\'s are ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bbac7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '/home/huangz78/data/traindata_x.npz'\n",
    "# train_sub = np.load(train_dir)['x']\n",
    "train_full = torch.tensor(np.load(train_dir)['xfull'])\n",
    "train_dir = '/home/huangz78/data/traindata_y.npz'\n",
    "# train_sub = np.load(train_dir)['x']\n",
    "yfull = torch.tensor(np.load(train_dir)['yfull'])\n",
    "print('train data fft size:', yfull.shape)\n",
    "print('train data size:', train_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545ddf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fullmask = torch.fft.fftshift(torch.tensor(np.load(train_dir)['mask'])) # roll the input mask\n",
    "\n",
    "test_dir = '/home/huangz78/data/testdata_x.npz'\n",
    "testimg  = torch.tensor(np.load(test_dir)['x']) \n",
    "print(testimg.shape)\n",
    "# test_sub  = test_sub[0:10,:,:]\n",
    "# test_full = torch.tensor(np.load(test_dir)['xfull']) \n",
    "mask_greedy = np.load('/home/huangz78/data/data_gt_greedymask.npz')\n",
    "mask_greedy = mask_greedy['mask'].T # this greedy mask is rolled\n",
    "print(mask_greedy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae02bd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select an image whose greedy mask we have\n",
    "# test_dir  = '/home/huangz78/data/data_gt.npz'\n",
    "# test_full = torch.tensor( np.transpose(np.load(test_dir)['imgdata'],axes=(2,0,1)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726fde37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mask_backward_new\n",
    "reload(mask_backward_new)\n",
    "from mask_backward_new import mask_backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859ec165",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alternating_update_with_unetRecon(mnet,UNET,train_full,\\\n",
    "                                  maxIter_mb=15,alpha=10**(-5.5),c=5e-2,lr_mb=1e-2,\\\n",
    "                                  maxRep=2,lr_mn=1e-4,\\\n",
    "                                  corefreq=24,budget=56,\\\n",
    "                                  epoch=1,batchsize=5,\\\n",
    "                                  verbose=True,save_cp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcf90c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alternating_update_with_unetRecon(mnet,unet,trainfulls,yfulls=None,\\\n",
    "                                      maxIter_mb=20,alpha=2.8*1e-5,c=0.05, lr_mb=1e-2,\\\n",
    "                                      maxRep=5,lr_mn=1e-4,\\\n",
    "                                      epoch=1,batchsize=5,\\\n",
    "                                      corefreq=24,budget=56,\\\n",
    "                                      verbose=False,hfen=False,dtyp=torch.float,\\\n",
    "                                      save_cp=False):\n",
    "    '''\n",
    "    alpha: magnitude of l1 penalty for high-frequency mask\n",
    "    mnet : the input mnet must match corefreq exactly\n",
    "    '''\n",
    "\n",
    "    DTyp = torch.cfloat if dtyp==torch.float else torch.cdouble\n",
    "    dir_checkpoint = '/home/huangz78/checkpoints/'\n",
    "    criterion_mnet = nn.BCEWithLogitsLoss()\n",
    "    optimizer_m = optim.RMSprop(mnet.parameters(), lr=lr_mn, weight_decay=0, momentum=0)\n",
    "    \n",
    "    # training loop\n",
    "    global_step = 0; epoch_count = 0\n",
    "    batch_nums  = int(np.ceil(trainfulls.shape[0]/batchsize))\n",
    "    loss_before = list([]); loss_after = list([]); loss_rand = list([]);\n",
    "    while epoch_count<epoch:\n",
    "        for batchind in range(batch_nums):\n",
    "            batch = np.arange(batchsize*batchind, min(batchsize*(batchind+1),trainfulls.shape[0]))\n",
    "            xstar = trainfulls[batch,:,:]\n",
    "            if yfulls is None:\n",
    "                yfull = torch.fft.fftshift(F.fftn(xstar,dim=(1,2),norm='ortho')) # y is ROLLED!\n",
    "            else:\n",
    "                yfull = torch.fft.fftshift(yfulls[batch,:,:],dim=(1,2))\n",
    "            lowfreqmask,_,_ = mask_naiveRand(xstar.shape[1],fix=corefreq,other=0,roll=True)\n",
    "            \n",
    "            ########################################  \n",
    "            ## (1) mask_backward\n",
    "            ######################################## \n",
    "            if mnet.in_channels == 1:\n",
    "                x_lf     = get_x_f_from_yfull(lowfreqmask,yfull)\n",
    "                highmask = mnet(x_lf.view(batch.size,1,xstar.shape[1],xstar.shape[2]))\n",
    "            elif mnet.in_channels == 2:\n",
    "                y = torch.zeros((yfull.shape[0],2,yfull.shape[1],yfull.shape[2]),dtype=torch.float)\n",
    "                y[:,0,lowfreqmask==1,:] = torch.real(yfull)[:,lowfreqmask==1,:]\n",
    "                y[:,1,lowfreqmask==1,:] = torch.imag(yfull)[:,lowfreqmask==1,:]\n",
    "                highmask = mnet(y)\n",
    "            highmask_refined,unet,loss_aft,loss_bef = mask_backward(highmask,xstar,unet=unet,mnet=mnet,\\\n",
    "                              beta=1.,alpha=alpha,c=c,\\\n",
    "                              maxIter=maxIter_mb,seed=0,break_limit=np.inf,\\\n",
    "                              lr=lr_mb,mode='UNET',testmode='UNET',\\\n",
    "                              budget=budget,normalize=True,\\\n",
    "                              verbose=verbose,dtyp=torch.float,\\\n",
    "                              hfen=False,return_loss_only=False)        \n",
    "            ########################################  \n",
    "            ## (2) update mnet\n",
    "            ########################################  \n",
    "            mnet.train()\n",
    "#             unet.eval()\n",
    "            rep = 0\n",
    "            while rep < maxRep:\n",
    "                if   mnet.in_channels == 1:\n",
    "                    mask_pred  = mnet(x_lf.view(batch.size,1,xstar.shape[1],xstar.shape[2]))\n",
    "                elif mnet.in_channels == 2:\n",
    "                    mask_pred  = mnet(y)\n",
    "\n",
    "                train_loss = criterion_mnet(mask_pred,highmask_refined)\n",
    "                optimizer_m.zero_grad()\n",
    "                # optimizer step wrt unet parameters?\n",
    "                train_loss.backward()\n",
    "                optimizer_m.step()\n",
    "                rep += 1\n",
    "            mnet.eval()\n",
    "            ########################################  \n",
    "            ## (3) check mnet performance: does it beat random sampling?\n",
    "            ########################################\n",
    "            breakpoint()\n",
    "            mask_rand,_,_ = mask_naiveRand(xstar.shape[1],fix=corefreq,other=budget,roll=True)\n",
    "            mask_rand     = mask_rand.repeat(xstar.shape[0],1)\n",
    "            randqual      = mask_eval(mask_rand,xstar,mode='UNET',UNET=UNET,dtyp=dtyp,hfen=hfen)\n",
    "            \n",
    "            iterprog = f'[{epoch_count+1}/{epoch}][{min(batchsize*(batchind+1),trainfulls.shape[0])}/{trainfulls.shape[0]}]'\n",
    "            print(iterprog + f', quality of random   mask : {randqual}') \n",
    "            print(iterprog + f', quality of old mnet mask : {loss_bef}')\n",
    "            print(iterprog + f', quality of refined  mask : {loss_aft}')\n",
    "                        \n",
    "            loss_rand.append(randqual); loss_after.append(loss_aft); loss_before.append(loss_bef)\n",
    "            \n",
    "            if (global_step%10==0) and save_cp:\n",
    "                torch.save({'model_state_dict': mnet.state_dict()}, dir_checkpoint + 'mnet_split_trained.pth')\n",
    "                torch.save({'model_state_dict': unet.state_dict()}, dir_checkpoint + 'unet_split_trained.pth')\n",
    "                print(f'\\t Checkpoint saved at epoch {epoch_count}, iter {global_step + 1}!')\n",
    "                filepath = '/home/huangz78/checkpoints/alternating_update_error_track.npz'\n",
    "                np.savez(filepath,loss_rand=loss_rand,loss_after=loss_after,loss_before=loss_before,freqs=(corefreq,budget))\n",
    "            global_step += 1\n",
    "        epoch_count+= 1\n",
    "#     return mnet, unet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
