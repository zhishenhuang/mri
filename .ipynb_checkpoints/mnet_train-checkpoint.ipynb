{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6cde6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.fft as F\n",
    "from importlib import reload\n",
    "from torch.nn.functional import relu\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Func\n",
    "import torch.optim as optim\n",
    "import utils\n",
    "import logging\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from utils import kplot,mask_naiveRand,mask_filter\n",
    "from mnet import MNet\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18e1dbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_binarize(M,threshold=0.6):\n",
    "    sigmoid = nn.Sigmoid()\n",
    "    mask = sigmoid(M)\n",
    "    mask_pred = torch.ones_like(mask)\n",
    "    mask_pred[mask<=threshold] = 0\n",
    "    return mask_pred\n",
    "    \n",
    "def trainMNet(trainimgs,trainlabels,testimgs,testlabels,\\\n",
    "              epochs=20,batchsize=5,\\\n",
    "              lr=0.01,lr_weight_decay=1e-8,opt_momentum=0,positive_weight=6,\\\n",
    "              lr_s_stepsize=5,lr_s_gamma=0.5,\\\n",
    "              model=None,save_cp=True,threshold=0.5,\\\n",
    "              beta=1,poolk=3,datatype=torch.float,print_every=10):\n",
    "    '''\n",
    "    trainimgs    : train data, with dimension (#imgs,height,width,layer)\n",
    "    '''\n",
    "    \n",
    "    train_shape  = trainimgs.shape; test_shape = testimgs.shape \n",
    "    trainimgs    = torch.tensor(trainimgs,dtype=datatype).view(train_shape[0],-1,train_shape[1],train_shape[2])\n",
    "    trainlabels  = torch.tensor(trainlabels,dtype=datatype)\n",
    "    testimgs     = torch.tensor(testimgs,dtype=datatype).view(test_shape[0],-1,test_shape[1],test_shape[2])\n",
    "    testlabels   = torch.tensor(testlabels ,dtype=datatype)\n",
    "    \n",
    "    train_shape = trainimgs.shape\n",
    "    dir_checkpoint = '/home/huangz78/mri/checkpoints/'\n",
    "    # add normalization for images here\n",
    "    \n",
    "    if model is None:\n",
    "        net = MNet(beta=beta,in_channels=train_shape[1],out_size=trainlabels.shape[1],\\\n",
    "                   imgsize=(train_shape[2],train_shape[3]),poolk=poolk)\n",
    "        optimizer = optim.RMSprop(net.parameters(), lr=lr, weight_decay=lr_weight_decay, momentum=opt_momentum)\n",
    "        epoch_init = 0\n",
    "    else:\n",
    "        net = model[0]\n",
    "        optimizer  = model[1]\n",
    "        epoch_init = model[2] + 1\n",
    "#     criterion = nn.MSELoss()\n",
    "    pos_weight = torch.ones([trainlabels.shape[1]]) * positive_weight # weight assigned to positive labels \n",
    "    criterion  = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    test_criterion = nn.BCELoss()\n",
    "    sigmoid = nn.Sigmoid()\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=lr_s_stepsize,factor=lr_s_gamma)\n",
    "\n",
    "    epoch_loss        = np.full((epochs),np.nan)\n",
    "    precision_history = np.full((epochs),np.nan)\n",
    "    recall_history    = np.full((epochs),np.nan)\n",
    "    for epoch in range(epoch_init,epoch_init + epochs):\n",
    "        batch_init = 0; step_count = 1\n",
    "        while batch_init < train_shape[0]:\n",
    "            batch = np.arange(batch_init,min(batch_init+batchsize,train_shape[0]))\n",
    "            imgbatch = trainimgs[batch,:,:,:] # maybe shuffling?\n",
    "            batchlabels = trainlabels[batch,:]\n",
    "            mask_pred   = net(imgbatch)\n",
    "            train_loss  = criterion(mask_pred,batchlabels)\n",
    "            batch_init += batchsize; step_count += 1\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            if (step_count%print_every)==0:\n",
    "                with torch.no_grad():\n",
    "                    net.eval()\n",
    "                    mask_test = sigmoid_binarize(net(testimgs),threshold=threshold)\n",
    "                    test_loss = test_criterion(mask_test,testlabels)\n",
    "                    print('epoch {} global step {}: train batch loss {}, test loss {} '.format(epoch+1,step_count,train_loss.item(),test_loss.item()))\n",
    "                    net.train()\n",
    "        net.eval()\n",
    "        mask_test = sigmoid_binarize(net(testimgs),threshold=threshold)\n",
    "        test_loss = test_criterion(mask_test,testlabels)\n",
    "        net.train()\n",
    "        scheduler.step(test_loss)\n",
    "        epoch_loss[epoch-epoch_init] = test_loss.item()\n",
    "        precision_history[epoch-epoch_init] = precision_score(torch.flatten(testlabels),torch.flatten(mask_test))\n",
    "        recall_history[epoch-epoch_init] = recall_score(torch.flatten(testlabels),torch.flatten(mask_test))\n",
    "        print('\\t epoch {} end: test loss {} '.format(epoch+1,test_loss.item()))\n",
    "        print('\\t epoch {} end: precision {} '.format(epoch+1,precision_history[epoch-epoch_init]))\n",
    "        print('\\t epoch {} end: recall    {} '.format(epoch+1,recall_history[epoch-epoch_init]))\n",
    "        if save_cp:\n",
    "            try:\n",
    "                os.mkdir(dir_checkpoint)\n",
    "                print('Created checkpoint directory')\n",
    "            except OSError:\n",
    "                pass\n",
    "            torch.save({'model_state_dict': net.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'epoch': epoch,\n",
    "                        'threshold':threshold\n",
    "                        }, dir_checkpoint + 'mnet.pth')\n",
    "#                         }, dir_checkpoint + f'CP_epoch{epoch + 1}.pth')\n",
    "            print(f'\\t Checkpoint saved after epoch {epoch + 1}!')\n",
    "    \n",
    "            np.savez(dir_checkpoint+'epoch_loss.npz', loss=epoch_loss,precision=precision_history,recall=recall_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b17dc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['imgdata']\n",
      "(320, 320, 199)\n",
      "(199, 320)\n"
     ]
    }
   ],
   "source": [
    "data_path = '/home/huangz78/data/'\n",
    "data_imgs = np.load(data_path+'data_gt.npz')\n",
    "data_labels = np.load(data_path+'data_gt_greedymask.npz')\n",
    "print(data_imgs.files)\n",
    "data    = data_imgs['imgdata']\n",
    "labels  = data_labels['mask'].T\n",
    "datashape = data.shape\n",
    "print(datashape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db91839",
   "metadata": {},
   "source": [
    "#### data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b26f118f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-5cbbedb90c5b>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mask = torch.tensor( mask_naiveRand(320,fix=base,other=0,roll=False)[0] ,dtype=torch.float )\n"
     ]
    }
   ],
   "source": [
    "base = 24\n",
    "mask = torch.tensor( mask_naiveRand(320,fix=base,other=0,roll=False)[0] ,dtype=torch.float )\n",
    "data_under = np.zeros((datashape[2],datashape[0],datashape[1]))\n",
    "for ind in range(data.shape[2]):\n",
    "    img = data[:,:,ind]\n",
    "    img = img/np.max(np.abs(img))\n",
    "    yfull = F.fftn(torch.tensor(img,dtype=torch.float),dim=(0,1),norm='ortho')\n",
    "    ypart = torch.tensordot(torch.diag(mask).to(torch.cfloat) , yfull,dims=([1],[0]))\n",
    "    data_under[ind,:,:] = torch.abs(F.ifftn(ypart,dim=(0,1),norm='ortho'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3c70a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "imgNum = 199\n",
    "traininds, testinds = train_test_split(np.arange(imgNum),random_state=0,shuffle=True,train_size=round(imgNum*0.8))\n",
    "test_total = testinds.size\n",
    "traindata    = data_under[traininds,:,:]\n",
    "trainlabels  = mask_filter(labels[traininds,:],base=base)\n",
    "valdata      = data_under[testinds[0:test_total//2],:,:]\n",
    "vallabels    = mask_filter(labels[testinds[0:test_total//2],:],base=base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6171d0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159, 320, 320)\n",
      "(159, 296)\n",
      "(20, 320, 320)\n",
      "(20, 296)\n"
     ]
    }
   ],
   "source": [
    "print(traindata.shape)\n",
    "print(trainlabels.shape)\n",
    "print(valdata.shape)\n",
    "print(vallabels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "559592c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sys.path.insert(0,'/home/huangz78/mri/mnet/')\n",
    "import mnet\n",
    "reload(mnet)\n",
    "from mnet import MNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1451786",
   "metadata": {},
   "source": [
    "#### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fc0c27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 global step 10: train batch loss 9.79659652709961, test loss 24.341217041015625 \n",
      "epoch 1 global step 20: train batch loss 2.8813672065734863, test loss 24.594594955444336 \n",
      "epoch 1 global step 30: train batch loss 7.909975528717041, test loss 22.027027130126953 \n",
      "\t epoch 1 end: test loss 36.08108139038086 \n",
      "\t epoch 1 end: precision 0.4134366925064599 \n",
      "\t epoch 1 end: recall    0.8 \n",
      "\t Checkpoint saved after epoch 1!\n",
      "epoch 2 global step 10: train batch loss 1.4218230247497559, test loss 21.756755828857422 \n",
      "epoch 2 global step 20: train batch loss 0.879325270652771, test loss 22.972972869873047 \n",
      "epoch 2 global step 30: train batch loss 2.503659725189209, test loss 21.79054069519043 \n",
      "\t epoch 2 end: test loss 25.405405044555664 \n",
      "\t epoch 2 end: precision 0.5210711150131695 \n",
      "\t epoch 2 end: recall    0.741875 \n",
      "\t Checkpoint saved after epoch 2!\n",
      "epoch 3 global step 10: train batch loss 0.9571739435195923, test loss 24.070945739746094 \n",
      "epoch 3 global step 20: train batch loss 0.6959943771362305, test loss 21.858108520507812 \n",
      "epoch 3 global step 30: train batch loss 1.8670746088027954, test loss 21.486486434936523 \n",
      "\t epoch 3 end: test loss 25.219594955444336 \n",
      "\t epoch 3 end: precision 0.523291249455812 \n",
      "\t epoch 3 end: recall    0.75125 \n",
      "\t Checkpoint saved after epoch 3!\n",
      "epoch 4 global step 10: train batch loss 0.7765476703643799, test loss 24.00337791442871 \n",
      "epoch 4 global step 20: train batch loss 0.633993923664093, test loss 21.942567825317383 \n",
      "epoch 4 global step 30: train batch loss 1.4444078207015991, test loss 21.148649215698242 \n",
      "\t epoch 4 end: test loss 26.300676345825195 \n",
      "\t epoch 4 end: precision 0.5090374106767549 \n",
      "\t epoch 4 end: recall    0.756875 \n",
      "\t Checkpoint saved after epoch 4!\n",
      "epoch 5 global step 10: train batch loss 0.7189694046974182, test loss 25.118244171142578 \n",
      "epoch 5 global step 20: train batch loss 0.5972141027450562, test loss 21.182432174682617 \n",
      "epoch 5 global step 30: train batch loss 1.7712700366973877, test loss 21.435810089111328 \n",
      "\t epoch 5 end: test loss 26.50337791442871 \n",
      "\t epoch 5 end: precision 0.5063655030800821 \n",
      "\t epoch 5 end: recall    0.770625 \n",
      "\t Checkpoint saved after epoch 5!\n",
      "epoch 6 global step 10: train batch loss 0.6662361025810242, test loss 23.597972869873047 \n",
      "epoch 6 global step 20: train batch loss 0.5610447525978088, test loss 21.351350784301758 \n",
      "epoch 6 global step 30: train batch loss 1.218824028968811, test loss 20.608108520507812 \n",
      "\t epoch 6 end: test loss 26.773649215698242 \n",
      "\t epoch 6 end: precision 0.5030599755201959 \n",
      "\t epoch 6 end: recall    0.770625 \n",
      "\t Checkpoint saved after epoch 6!\n",
      "epoch 7 global step 10: train batch loss 0.6663867831230164, test loss 21.925676345825195 \n",
      "epoch 7 global step 20: train batch loss 0.5421582460403442, test loss 20.658782958984375 \n",
      "epoch 7 global step 30: train batch loss 0.7448958158493042, test loss 22.060810089111328 \n",
      "\t epoch 7 end: test loss 25.82770347595215 \n",
      "\t epoch 7 end: precision 0.5149599662873999 \n",
      "\t epoch 7 end: recall    0.76375 \n",
      "\t Checkpoint saved after epoch 7!\n",
      "epoch 8 global step 10: train batch loss 0.6329814791679382, test loss 21.858108520507812 \n",
      "epoch 8 global step 20: train batch loss 0.5331172347068787, test loss 20.641891479492188 \n",
      "epoch 8 global step 30: train batch loss 0.7024616003036499, test loss 21.402027130126953 \n",
      "\t epoch 8 end: test loss 25.608108520507812 \n",
      "\t epoch 8 end: precision 0.5177364864864865 \n",
      "\t epoch 8 end: recall    0.76625 \n",
      "\t Checkpoint saved after epoch 8!\n",
      "epoch 9 global step 10: train batch loss 0.6230526566505432, test loss 21.53716278076172 \n",
      "epoch 9 global step 20: train batch loss 0.5235448479652405, test loss 20.42229652404785 \n",
      "epoch 9 global step 30: train batch loss 0.7595623731613159, test loss 20.658782958984375 \n",
      "\t epoch 9 end: test loss 25.929054260253906 \n",
      "\t epoch 9 end: precision 0.5136957437842393 \n",
      "\t epoch 9 end: recall    0.761875 \n",
      "\t Checkpoint saved after epoch 9!\n",
      "epoch 10 global step 10: train batch loss 0.62330561876297, test loss 20.489864349365234 \n",
      "epoch 10 global step 20: train batch loss 0.5223648548126221, test loss 20.118244171142578 \n",
      "epoch 10 global step 30: train batch loss 0.5819606781005859, test loss 20.844594955444336 \n",
      "\t epoch 10 end: test loss 24.17229652404785 \n",
      "\t epoch 10 end: precision 0.5372082782915015 \n",
      "\t epoch 10 end: recall    0.7625 \n",
      "\t Checkpoint saved after epoch 10!\n",
      "epoch 11 global step 10: train batch loss 0.5986398458480835, test loss 20.185810089111328 \n",
      "epoch 11 global step 20: train batch loss 0.5160456299781799, test loss 19.476350784301758 \n",
      "epoch 11 global step 30: train batch loss 0.5784591436386108, test loss 20.8952693939209 \n",
      "\t epoch 11 end: test loss 24.29054069519043 \n",
      "\t epoch 11 end: precision 0.5355887521968365 \n",
      "\t epoch 11 end: recall    0.761875 \n",
      "\t Checkpoint saved after epoch 11!\n",
      "epoch 12 global step 10: train batch loss 0.5945272445678711, test loss 20.08445930480957 \n",
      "epoch 12 global step 20: train batch loss 0.5116590261459351, test loss 19.324323654174805 \n",
      "epoch 12 global step 30: train batch loss 0.5705147981643677, test loss 20.861486434936523 \n",
      "\t epoch 12 end: test loss 24.17229652404785 \n",
      "\t epoch 12 end: precision 0.5377738042020563 \n",
      "\t epoch 12 end: recall    0.751875 \n",
      "\t Checkpoint saved after epoch 12!\n",
      "epoch 13 global step 10: train batch loss 0.5909345746040344, test loss 20.050676345825195 \n",
      "epoch 13 global step 20: train batch loss 0.5072219371795654, test loss 19.189189910888672 \n",
      "epoch 13 global step 30: train batch loss 0.578845739364624, test loss 20.82770347595215 \n",
      "\t epoch 13 end: test loss 24.324323654174805 \n",
      "\t epoch 13 end: precision 0.5352733686067019 \n",
      "\t epoch 13 end: recall    0.75875 \n",
      "\t Checkpoint saved after epoch 13!\n",
      "epoch 14 global step 10: train batch loss 0.5899649262428284, test loss 19.20608139038086 \n",
      "epoch 14 global step 20: train batch loss 0.5051611661911011, test loss 18.91891860961914 \n",
      "epoch 14 global step 30: train batch loss 0.5229471325874329, test loss 19.62837791442871 \n",
      "\t epoch 14 end: test loss 21.50337791442871 \n",
      "\t epoch 14 end: precision 0.5776722090261283 \n",
      "\t epoch 14 end: recall    0.76 \n",
      "\t Checkpoint saved after epoch 14!\n",
      "epoch 15 global step 10: train batch loss 0.5786724090576172, test loss 18.902027130126953 \n",
      "epoch 15 global step 20: train batch loss 0.5030332803726196, test loss 18.699323654174805 \n",
      "epoch 15 global step 30: train batch loss 0.5250087380409241, test loss 19.6452693939209 \n",
      "\t epoch 15 end: test loss 21.469594955444336 \n",
      "\t epoch 15 end: precision 0.578147268408551 \n",
      "\t epoch 15 end: recall    0.760625 \n",
      "\t Checkpoint saved after epoch 15!\n",
      "epoch 16 global step 10: train batch loss 0.5759749412536621, test loss 18.66554069519043 \n",
      "epoch 16 global step 20: train batch loss 0.501031219959259, test loss 18.58108139038086 \n",
      "epoch 16 global step 30: train batch loss 0.5232383012771606, test loss 19.864864349365234 \n",
      "\t epoch 16 end: test loss 21.50337791442871 \n",
      "\t epoch 16 end: precision 0.5775984812529663 \n",
      "\t epoch 16 end: recall    0.760625 \n",
      "\t Checkpoint saved after epoch 16!\n",
      "epoch 17 global step 10: train batch loss 0.5745511054992676, test loss 18.83445930480957 \n",
      "epoch 17 global step 20: train batch loss 0.49817338585853577, test loss 18.49662208557129 \n",
      "epoch 17 global step 30: train batch loss 0.5227808952331543, test loss 19.83108139038086 \n",
      "\t epoch 17 end: test loss 21.53716278076172 \n",
      "\t epoch 17 end: precision 0.5770507349454718 \n",
      "\t epoch 17 end: recall    0.760625 \n",
      "\t Checkpoint saved after epoch 17!\n",
      "epoch 18 global step 10: train batch loss 0.5727719068527222, test loss 18.66554069519043 \n",
      "epoch 18 global step 20: train batch loss 0.4963904619216919, test loss 18.648649215698242 \n",
      "epoch 18 global step 30: train batch loss 0.5226249694824219, test loss 19.966217041015625 \n",
      "\t epoch 18 end: test loss 21.841217041015625 \n",
      "\t epoch 18 end: precision 0.5720319099014547 \n",
      "\t epoch 18 end: recall    0.761875 \n",
      "\t Checkpoint saved after epoch 18!\n",
      "epoch 19 global step 10: train batch loss 0.570195734500885, test loss 18.766891479492188 \n",
      "epoch 19 global step 20: train batch loss 0.49311068654060364, test loss 18.260135650634766 \n",
      "epoch 19 global step 30: train batch loss 0.4887138605117798, test loss 19.00337791442871 \n",
      "\t epoch 19 end: test loss 19.780405044555664 \n",
      "\t epoch 19 end: precision 0.6081694402420574 \n",
      "\t epoch 19 end: recall    0.75375 \n",
      "\t Checkpoint saved after epoch 19!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20 global step 10: train batch loss 0.5642018914222717, test loss 18.648649215698242 \n",
      "epoch 20 global step 20: train batch loss 0.4912300407886505, test loss 18.530405044555664 \n",
      "epoch 20 global step 30: train batch loss 0.491893470287323, test loss 19.138513565063477 \n",
      "\t epoch 20 end: test loss 20.320945739746094 \n",
      "\t epoch 20 end: precision 0.5988053758088602 \n",
      "\t epoch 20 end: recall    0.751875 \n",
      "\t Checkpoint saved after epoch 20!\n",
      "epoch 21 global step 10: train batch loss 0.5627310276031494, test loss 18.4797306060791 \n",
      "epoch 21 global step 20: train batch loss 0.48933014273643494, test loss 18.29391860961914 \n",
      "epoch 21 global step 30: train batch loss 0.4920302927494049, test loss 19.1047306060791 \n",
      "\t epoch 21 end: test loss 19.966217041015625 \n",
      "\t epoch 21 end: precision 0.6047094188376754 \n",
      "\t epoch 21 end: recall    0.754375 \n",
      "\t Checkpoint saved after epoch 21!\n",
      "epoch 22 global step 10: train batch loss 0.5616095066070557, test loss 18.41216278076172 \n",
      "epoch 22 global step 20: train batch loss 0.48714154958724976, test loss 18.344594955444336 \n",
      "epoch 22 global step 30: train batch loss 0.492053359746933, test loss 19.324323654174805 \n",
      "\t epoch 22 end: test loss 19.966217041015625 \n",
      "\t epoch 22 end: precision 0.6047094188376754 \n",
      "\t epoch 22 end: recall    0.754375 \n",
      "\t Checkpoint saved after epoch 22!\n",
      "epoch 23 global step 10: train batch loss 0.5574415922164917, test loss 18.75 \n",
      "epoch 23 global step 20: train batch loss 0.48437264561653137, test loss 18.277027130126953 \n",
      "epoch 23 global step 30: train batch loss 0.46828049421310425, test loss 19.00337791442871 \n",
      "\t epoch 23 end: test loss 19.273649215698242 \n",
      "\t epoch 23 end: precision 0.6189735614307932 \n",
      "\t epoch 23 end: recall    0.74625 \n",
      "\t Checkpoint saved after epoch 23!\n",
      "epoch 24 global step 10: train batch loss 0.5543290376663208, test loss 18.54729652404785 \n",
      "epoch 24 global step 20: train batch loss 0.4826093912124634, test loss 18.260135650634766 \n",
      "epoch 24 global step 30: train batch loss 0.4715162217617035, test loss 19.0202693939209 \n",
      "\t epoch 24 end: test loss 19.29054069519043 \n",
      "\t epoch 24 end: precision 0.6186528497409326 \n",
      "\t epoch 24 end: recall    0.74625 \n",
      "\t Checkpoint saved after epoch 24!\n",
      "epoch 25 global step 10: train batch loss 0.553241491317749, test loss 18.445945739746094 \n",
      "epoch 25 global step 20: train batch loss 0.48090535402297974, test loss 18.260135650634766 \n",
      "epoch 25 global step 30: train batch loss 0.4722132086753845, test loss 19.054054260253906 \n",
      "\t epoch 25 end: test loss 19.307432174682617 \n",
      "\t epoch 25 end: precision 0.6182100362131402 \n",
      "\t epoch 25 end: recall    0.746875 \n",
      "\t Checkpoint saved after epoch 25!\n",
      "epoch 26 global step 10: train batch loss 0.5524984002113342, test loss 18.445945739746094 \n",
      "epoch 26 global step 20: train batch loss 0.4790962040424347, test loss 18.260135650634766 \n",
      "epoch 26 global step 30: train batch loss 0.47183138132095337, test loss 19.054054260253906 \n",
      "\t epoch 26 end: test loss 19.256755828857422 \n",
      "\t epoch 26 end: precision 0.6190476190476191 \n",
      "\t epoch 26 end: recall    0.7475 \n",
      "\t Checkpoint saved after epoch 26!\n",
      "epoch 27 global step 10: train batch loss 0.551992654800415, test loss 18.429054260253906 \n",
      "epoch 27 global step 20: train batch loss 0.47712376713752747, test loss 18.29391860961914 \n",
      "epoch 27 global step 30: train batch loss 0.4715498983860016, test loss 19.1047306060791 \n",
      "\t epoch 27 end: test loss 19.222972869873047 \n",
      "\t epoch 27 end: precision 0.6194415718717684 \n",
      "\t epoch 27 end: recall    0.74875 \n",
      "\t Checkpoint saved after epoch 27!\n",
      "epoch 28 global step 10: train batch loss 0.551478385925293, test loss 18.32770347595215 \n",
      "epoch 28 global step 20: train batch loss 0.47489964962005615, test loss 18.277027130126953 \n",
      "epoch 28 global step 30: train batch loss 0.4710017740726471, test loss 19.12162208557129 \n",
      "\t epoch 28 end: test loss 19.29054069519043 \n",
      "\t epoch 28 end: precision 0.6184074457083765 \n",
      "\t epoch 28 end: recall    0.7475 \n",
      "\t Checkpoint saved after epoch 28!\n",
      "epoch 29 global step 10: train batch loss 0.5510681867599487, test loss 18.344594955444336 \n",
      "epoch 29 global step 20: train batch loss 0.4727051556110382, test loss 18.243244171142578 \n",
      "epoch 29 global step 30: train batch loss 0.4706260859966278, test loss 19.138513565063477 \n",
      "\t epoch 29 end: test loss 19.408782958984375 \n",
      "\t epoch 29 end: precision 0.6161772282328697 \n",
      "\t epoch 29 end: recall    0.7475 \n",
      "\t Checkpoint saved after epoch 29!\n",
      "epoch 30 global step 10: train batch loss 0.5506139397621155, test loss 18.3952693939209 \n",
      "epoch 30 global step 20: train batch loss 0.4704202711582184, test loss 18.310810089111328 \n",
      "epoch 30 global step 30: train batch loss 0.4700271189212799, test loss 19.17229652404785 \n",
      "\t epoch 30 end: test loss 19.425676345825195 \n",
      "\t epoch 30 end: precision 0.6157407407407407 \n",
      "\t epoch 30 end: recall    0.748125 \n",
      "\t Checkpoint saved after epoch 30!\n",
      "epoch 31 global step 10: train batch loss 0.546436071395874, test loss 18.54729652404785 \n",
      "epoch 31 global step 20: train batch loss 0.4686049222946167, test loss 18.361486434936523 \n",
      "epoch 31 global step 30: train batch loss 0.4539053440093994, test loss 18.902027130126953 \n",
      "\t epoch 31 end: test loss 18.91891860961914 \n",
      "\t epoch 31 end: precision 0.6257861635220126 \n",
      "\t epoch 31 end: recall    0.74625 \n",
      "\t Checkpoint saved after epoch 31!\n",
      "epoch 32 global step 10: train batch loss 0.5449704527854919, test loss 18.46283721923828 \n",
      "epoch 32 global step 20: train batch loss 0.4669094681739807, test loss 18.32770347595215 \n",
      "epoch 32 global step 30: train batch loss 0.4559970200061798, test loss 18.986486434936523 \n",
      "\t epoch 32 end: test loss 18.885135650634766 \n",
      "\t epoch 32 end: precision 0.6267087276550999 \n",
      "\t epoch 32 end: recall    0.745 \n",
      "\t Checkpoint saved after epoch 32!\n",
      "epoch 33 global step 10: train batch loss 0.5442572236061096, test loss 18.445945739746094 \n",
      "epoch 33 global step 20: train batch loss 0.46521663665771484, test loss 18.344594955444336 \n",
      "epoch 33 global step 30: train batch loss 0.4567081332206726, test loss 19.00337791442871 \n",
      "\t epoch 33 end: test loss 18.935810089111328 \n",
      "\t epoch 33 end: precision 0.6257217847769029 \n",
      "\t epoch 33 end: recall    0.745 \n",
      "\t Checkpoint saved after epoch 33!\n",
      "epoch 34 global step 10: train batch loss 0.5437654256820679, test loss 18.445945739746094 \n",
      "epoch 34 global step 20: train batch loss 0.4637845754623413, test loss 18.243244171142578 \n",
      "epoch 34 global step 30: train batch loss 0.45633623003959656, test loss 19.08783721923828 \n",
      "\t epoch 34 end: test loss 19.00337791442871 \n",
      "\t epoch 34 end: precision 0.6244106862231534 \n",
      "\t epoch 34 end: recall    0.745 \n",
      "\t Checkpoint saved after epoch 34!\n",
      "epoch 35 global step 10: train batch loss 0.5435084700584412, test loss 18.429054260253906 \n",
      "epoch 35 global step 20: train batch loss 0.46220025420188904, test loss 18.243244171142578 \n",
      "epoch 35 global step 30: train batch loss 0.45585525035858154, test loss 19.03716278076172 \n",
      "\t epoch 35 end: test loss 19.0202693939209 \n",
      "\t epoch 35 end: precision 0.6242138364779874 \n",
      "\t epoch 35 end: recall    0.744375 \n",
      "\t Checkpoint saved after epoch 35!\n",
      "epoch 36 global step 10: train batch loss 0.5402163863182068, test loss 18.631755828857422 \n",
      "epoch 36 global step 20: train batch loss 0.461781769990921, test loss 18.37837791442871 \n",
      "epoch 36 global step 30: train batch loss 0.44687050580978394, test loss 18.75 \n",
      "\t epoch 36 end: test loss 18.564189910888672 \n",
      "\t epoch 36 end: precision 0.6333155934007451 \n",
      "\t epoch 36 end: recall    0.74375 \n",
      "\t Checkpoint saved after epoch 36!\n",
      "epoch 37 global step 10: train batch loss 0.5400106310844421, test loss 18.4797306060791 \n",
      "epoch 37 global step 20: train batch loss 0.46030113101005554, test loss 18.32770347595215 \n",
      "epoch 37 global step 30: train batch loss 0.44828400015830994, test loss 18.682432174682617 \n",
      "\t epoch 37 end: test loss 18.58108139038086 \n",
      "\t epoch 37 end: precision 0.6331203407880724 \n",
      "\t epoch 37 end: recall    0.743125 \n",
      "\t Checkpoint saved after epoch 37!\n",
      "epoch 38 global step 10: train batch loss 0.5394029021263123, test loss 18.46283721923828 \n",
      "epoch 38 global step 20: train batch loss 0.4593745768070221, test loss 18.260135650634766 \n",
      "epoch 38 global step 30: train batch loss 0.4486415982246399, test loss 18.733108520507812 \n",
      "\t epoch 38 end: test loss 18.597972869873047 \n",
      "\t epoch 38 end: precision 0.6329248801278636 \n",
      "\t epoch 38 end: recall    0.7425 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Checkpoint saved after epoch 38!\n",
      "epoch 39 global step 10: train batch loss 0.5390455722808838, test loss 18.429054260253906 \n",
      "epoch 39 global step 20: train batch loss 0.45853179693222046, test loss 18.20945930480957 \n",
      "epoch 39 global step 30: train batch loss 0.44844359159469604, test loss 18.716217041015625 \n",
      "\t epoch 39 end: test loss 18.648649215698242 \n",
      "\t epoch 39 end: precision 0.6319148936170212 \n",
      "\t epoch 39 end: recall    0.7425 \n",
      "\t Checkpoint saved after epoch 39!\n",
      "epoch 40 global step 10: train batch loss 0.5369148254394531, test loss 18.597972869873047 \n",
      "epoch 40 global step 20: train batch loss 0.45839521288871765, test loss 18.445945739746094 \n",
      "epoch 40 global step 30: train batch loss 0.4429212510585785, test loss 18.58108139038086 \n",
      "\t epoch 40 end: test loss 18.530405044555664 \n",
      "\t epoch 40 end: precision 0.6344200962052379 \n",
      "\t epoch 40 end: recall    0.741875 \n",
      "\t Checkpoint saved after epoch 40!\n",
      "epoch 41 global step 10: train batch loss 0.5370559692382812, test loss 18.530405044555664 \n",
      "epoch 41 global step 20: train batch loss 0.45747479796409607, test loss 18.32770347595215 \n",
      "epoch 41 global step 30: train batch loss 0.4441290497779846, test loss 18.54729652404785 \n",
      "\t epoch 41 end: test loss 18.530405044555664 \n",
      "\t epoch 41 end: precision 0.6344200962052379 \n",
      "\t epoch 41 end: recall    0.741875 \n",
      "\t Checkpoint saved after epoch 41!\n",
      "epoch 42 global step 10: train batch loss 0.5368055701255798, test loss 18.530405044555664 \n",
      "epoch 42 global step 20: train batch loss 0.45686519145965576, test loss 18.29391860961914 \n",
      "epoch 42 global step 30: train batch loss 0.4444788694381714, test loss 18.530405044555664 \n",
      "\t epoch 42 end: test loss 18.49662208557129 \n",
      "\t epoch 42 end: precision 0.6350989834135902 \n",
      "\t epoch 42 end: recall    0.741875 \n",
      "\t Checkpoint saved after epoch 42!\n",
      "epoch 43 global step 10: train batch loss 0.5365469455718994, test loss 18.530405044555664 \n",
      "epoch 43 global step 20: train batch loss 0.4562249183654785, test loss 18.243244171142578 \n",
      "epoch 43 global step 30: train batch loss 0.44445425271987915, test loss 18.54729652404785 \n",
      "\t epoch 43 end: test loss 18.49662208557129 \n",
      "\t epoch 43 end: precision 0.6350989834135902 \n",
      "\t epoch 43 end: recall    0.741875 \n",
      "\t Checkpoint saved after epoch 43!\n",
      "epoch 44 global step 10: train batch loss 0.5362784266471863, test loss 18.530405044555664 \n",
      "epoch 44 global step 20: train batch loss 0.45568224787712097, test loss 18.277027130126953 \n",
      "epoch 44 global step 30: train batch loss 0.4442756474018097, test loss 18.530405044555664 \n",
      "\t epoch 44 end: test loss 18.513513565063477 \n",
      "\t epoch 44 end: precision 0.6347593582887701 \n",
      "\t epoch 44 end: recall    0.741875 \n",
      "\t Checkpoint saved after epoch 44!\n",
      "epoch 45 global step 10: train batch loss 0.5360444188117981, test loss 18.54729652404785 \n",
      "epoch 45 global step 20: train batch loss 0.45494914054870605, test loss 18.277027130126953 \n",
      "epoch 45 global step 30: train batch loss 0.44402506947517395, test loss 18.564189910888672 \n",
      "\t epoch 45 end: test loss 18.54729652404785 \n",
      "\t epoch 45 end: precision 0.6343683083511777 \n",
      "\t epoch 45 end: recall    0.740625 \n",
      "\t Checkpoint saved after epoch 45!\n",
      "epoch 46 global step 10: train batch loss 0.5350245237350464, test loss 18.597972869873047 \n",
      "epoch 46 global step 20: train batch loss 0.4548528790473938, test loss 18.41216278076172 \n",
      "epoch 46 global step 30: train batch loss 0.43993401527404785, test loss 18.54729652404785 \n",
      "\t epoch 46 end: test loss 18.49662208557129 \n",
      "\t epoch 46 end: precision 0.6352437064809855 \n",
      "\t epoch 46 end: recall    0.74125 \n",
      "\t Checkpoint saved after epoch 46!\n",
      "epoch 47 global step 10: train batch loss 0.534938395023346, test loss 18.54729652404785 \n",
      "epoch 47 global step 20: train batch loss 0.45429667830467224, test loss 18.37837791442871 \n",
      "epoch 47 global step 30: train batch loss 0.44074153900146484, test loss 18.4797306060791 \n",
      "\t epoch 47 end: test loss 18.4797306060791 \n",
      "\t epoch 47 end: precision 0.6355841371918542 \n",
      "\t epoch 47 end: recall    0.74125 \n",
      "\t Checkpoint saved after epoch 47!\n",
      "epoch 48 global step 10: train batch loss 0.5348401665687561, test loss 18.54729652404785 \n",
      "epoch 48 global step 20: train batch loss 0.45377466082572937, test loss 18.344594955444336 \n",
      "epoch 48 global step 30: train batch loss 0.4410637617111206, test loss 18.49662208557129 \n",
      "\t epoch 48 end: test loss 18.513513565063477 \n",
      "\t epoch 48 end: precision 0.6351931330472103 \n",
      "\t epoch 48 end: recall    0.74 \n",
      "\t Checkpoint saved after epoch 48!\n",
      "epoch 49 global step 10: train batch loss 0.5346392393112183, test loss 18.530405044555664 \n",
      "epoch 49 global step 20: train batch loss 0.4533590078353882, test loss 18.32770347595215 \n",
      "epoch 49 global step 30: train batch loss 0.44110408425331116, test loss 18.4797306060791 \n",
      "\t epoch 49 end: test loss 18.4797306060791 \n",
      "\t epoch 49 end: precision 0.635875402792696 \n",
      "\t epoch 49 end: recall    0.74 \n",
      "\t Checkpoint saved after epoch 49!\n",
      "epoch 50 global step 10: train batch loss 0.5344823598861694, test loss 18.54729652404785 \n",
      "epoch 50 global step 20: train batch loss 0.45305052399635315, test loss 18.344594955444336 \n",
      "epoch 50 global step 30: train batch loss 0.4410748779773712, test loss 18.4797306060791 \n",
      "\t epoch 50 end: test loss 18.49662208557129 \n",
      "\t epoch 50 end: precision 0.6355340848094472 \n",
      "\t epoch 50 end: recall    0.74 \n",
      "\t Checkpoint saved after epoch 50!\n",
      "epoch 51 global step 10: train batch loss 0.5339493155479431, test loss 18.58108139038086 \n",
      "epoch 51 global step 20: train batch loss 0.4530530273914337, test loss 18.37837791442871 \n",
      "epoch 51 global step 30: train batch loss 0.4384914040565491, test loss 18.513513565063477 \n",
      "\t epoch 51 end: test loss 18.49662208557129 \n",
      "\t epoch 51 end: precision 0.6356797420741537 \n",
      "\t epoch 51 end: recall    0.739375 \n",
      "\t Checkpoint saved after epoch 51!\n",
      "epoch 52 global step 10: train batch loss 0.5338171124458313, test loss 18.513513565063477 \n",
      "epoch 52 global step 20: train batch loss 0.4527774155139923, test loss 18.37837791442871 \n",
      "epoch 52 global step 30: train batch loss 0.43888428807258606, test loss 18.4797306060791 \n",
      "\t epoch 52 end: test loss 18.46283721923828 \n",
      "\t epoch 52 end: precision 0.6363636363636364 \n",
      "\t epoch 52 end: recall    0.739375 \n",
      "\t Checkpoint saved after epoch 52!\n",
      "epoch 53 global step 10: train batch loss 0.5337581634521484, test loss 18.513513565063477 \n",
      "epoch 53 global step 20: train batch loss 0.4525502026081085, test loss 18.361486434936523 \n",
      "epoch 53 global step 30: train batch loss 0.4391027092933655, test loss 18.4797306060791 \n",
      "\t epoch 53 end: test loss 18.4797306060791 \n",
      "\t epoch 53 end: precision 0.636021505376344 \n",
      "\t epoch 53 end: recall    0.739375 \n",
      "\t Checkpoint saved after epoch 53!\n",
      "epoch 54 global step 10: train batch loss 0.5336741805076599, test loss 18.513513565063477 \n",
      "epoch 54 global step 20: train batch loss 0.4523255228996277, test loss 18.32770347595215 \n",
      "epoch 54 global step 30: train batch loss 0.43921327590942383, test loss 18.4797306060791 \n",
      "\t epoch 54 end: test loss 18.49662208557129 \n",
      "\t epoch 54 end: precision 0.6356797420741537 \n",
      "\t epoch 54 end: recall    0.739375 \n",
      "\t Checkpoint saved after epoch 54!\n",
      "epoch 55 global step 10: train batch loss 0.5335677862167358, test loss 18.513513565063477 \n",
      "epoch 55 global step 20: train batch loss 0.45210161805152893, test loss 18.32770347595215 \n",
      "epoch 55 global step 30: train batch loss 0.43926939368247986, test loss 18.4797306060791 \n",
      "\t epoch 55 end: test loss 18.4797306060791 \n",
      "\t epoch 55 end: precision 0.636021505376344 \n",
      "\t epoch 55 end: recall    0.739375 \n",
      "\t Checkpoint saved after epoch 55!\n",
      "epoch 56 global step 10: train batch loss 0.5332819223403931, test loss 18.530405044555664 \n",
      "epoch 56 global step 20: train batch loss 0.45216357707977295, test loss 18.361486434936523 \n",
      "epoch 56 global step 30: train batch loss 0.4377818703651428, test loss 18.46283721923828 \n",
      "\t epoch 56 end: test loss 18.49662208557129 \n",
      "\t epoch 56 end: precision 0.6356797420741537 \n",
      "\t epoch 56 end: recall    0.739375 \n",
      "\t Checkpoint saved after epoch 56!\n",
      "epoch 57 global step 10: train batch loss 0.5332525968551636, test loss 18.513513565063477 \n",
      "epoch 57 global step 20: train batch loss 0.4520396888256073, test loss 18.361486434936523 \n",
      "epoch 57 global step 30: train batch loss 0.4379381239414215, test loss 18.4797306060791 \n",
      "\t epoch 57 end: test loss 18.49662208557129 \n",
      "\t epoch 57 end: precision 0.6356797420741537 \n",
      "\t epoch 57 end: recall    0.739375 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Checkpoint saved after epoch 57!\n",
      "epoch 58 global step 10: train batch loss 0.5332120060920715, test loss 18.513513565063477 \n",
      "epoch 58 global step 20: train batch loss 0.451916366815567, test loss 18.344594955444336 \n",
      "epoch 58 global step 30: train batch loss 0.43805569410324097, test loss 18.46283721923828 \n",
      "\t epoch 58 end: test loss 18.4797306060791 \n",
      "\t epoch 58 end: precision 0.636021505376344 \n",
      "\t epoch 58 end: recall    0.739375 \n",
      "\t Checkpoint saved after epoch 58!\n",
      "epoch 59 global step 10: train batch loss 0.5330859422683716, test loss 18.513513565063477 \n",
      "epoch 59 global step 20: train batch loss 0.4519413411617279, test loss 18.3952693939209 \n",
      "epoch 59 global step 30: train batch loss 0.4373060166835785, test loss 18.46283721923828 \n",
      "\t epoch 59 end: test loss 18.4797306060791 \n",
      "\t epoch 59 end: precision 0.636021505376344 \n",
      "\t epoch 59 end: recall    0.739375 \n",
      "\t Checkpoint saved after epoch 59!\n",
      "epoch 60 global step 10: train batch loss 0.5330671072006226, test loss 18.513513565063477 \n",
      "epoch 60 global step 20: train batch loss 0.45187732577323914, test loss 18.37837791442871 \n",
      "epoch 60 global step 30: train batch loss 0.43739113211631775, test loss 18.46283721923828 \n",
      "\t epoch 60 end: test loss 18.4797306060791 \n",
      "\t epoch 60 end: precision 0.636021505376344 \n",
      "\t epoch 60 end: recall    0.739375 \n",
      "\t Checkpoint saved after epoch 60!\n",
      "epoch 61 global step 10: train batch loss 0.5330445766448975, test loss 18.513513565063477 \n",
      "epoch 61 global step 20: train batch loss 0.45181381702423096, test loss 18.37837791442871 \n",
      "epoch 61 global step 30: train batch loss 0.4374644160270691, test loss 18.46283721923828 \n",
      "\t epoch 61 end: test loss 18.4797306060791 \n",
      "\t epoch 61 end: precision 0.636021505376344 \n",
      "\t epoch 61 end: recall    0.739375 \n",
      "\t Checkpoint saved after epoch 61!\n",
      "epoch 62 global step 10: train batch loss 0.532987117767334, test loss 18.513513565063477 \n",
      "epoch 62 global step 20: train batch loss 0.45181867480278015, test loss 18.41216278076172 \n",
      "epoch 62 global step 30: train batch loss 0.4370853304862976, test loss 18.46283721923828 \n",
      "\t epoch 62 end: test loss 18.46283721923828 \n",
      "\t epoch 62 end: precision 0.6363636363636364 \n",
      "\t epoch 62 end: recall    0.739375 \n",
      "\t Checkpoint saved after epoch 62!\n",
      "epoch 63 global step 10: train batch loss 0.5329558253288269, test loss 18.513513565063477 \n",
      "epoch 63 global step 20: train batch loss 0.4517878293991089, test loss 18.41216278076172 \n",
      "epoch 63 global step 30: train batch loss 0.43712809681892395, test loss 18.445945739746094 \n",
      "\t epoch 63 end: test loss 18.46283721923828 \n",
      "\t epoch 63 end: precision 0.6363636363636364 \n",
      "\t epoch 63 end: recall    0.739375 \n",
      "\t Checkpoint saved after epoch 63!\n",
      "epoch 64 global step 10: train batch loss 0.532947301864624, test loss 18.530405044555664 \n",
      "epoch 64 global step 20: train batch loss 0.45175623893737793, test loss 18.41216278076172 \n",
      "epoch 64 global step 30: train batch loss 0.4371670186519623, test loss 18.429054260253906 \n",
      "\t epoch 64 end: test loss 18.46283721923828 \n",
      "\t epoch 64 end: precision 0.6363636363636364 \n",
      "\t epoch 64 end: recall    0.739375 \n",
      "\t Checkpoint saved after epoch 64!\n",
      "epoch 65 global step 10: train batch loss 0.5329142212867737, test loss 18.530405044555664 \n",
      "epoch 65 global step 20: train batch loss 0.451761394739151, test loss 18.41216278076172 \n",
      "epoch 65 global step 30: train batch loss 0.4369746148586273, test loss 18.429054260253906 \n",
      "\t epoch 65 end: test loss 18.46283721923828 \n",
      "\t epoch 65 end: precision 0.6363636363636364 \n",
      "\t epoch 65 end: recall    0.739375 \n",
      "\t Checkpoint saved after epoch 65!\n",
      "epoch 66 global step 10: train batch loss 0.5329087376594543, test loss 18.530405044555664 \n",
      "epoch 66 global step 20: train batch loss 0.45174482464790344, test loss 18.41216278076172 \n",
      "epoch 66 global step 30: train batch loss 0.4369964301586151, test loss 18.429054260253906 \n",
      "\t epoch 66 end: test loss 18.46283721923828 \n",
      "\t epoch 66 end: precision 0.6363636363636364 \n",
      "\t epoch 66 end: recall    0.739375 \n",
      "\t Checkpoint saved after epoch 66!\n",
      "epoch 67 global step 10: train batch loss 0.5329034328460693, test loss 18.530405044555664 \n",
      "epoch 67 global step 20: train batch loss 0.45172831416130066, test loss 18.41216278076172 \n",
      "epoch 67 global step 30: train batch loss 0.4370170533657074, test loss 18.429054260253906 \n",
      "\t epoch 67 end: test loss 18.46283721923828 \n",
      "\t epoch 67 end: precision 0.6363636363636364 \n",
      "\t epoch 67 end: recall    0.739375 \n",
      "\t Checkpoint saved after epoch 67!\n",
      "epoch 68 global step 10: train batch loss 0.5328869223594666, test loss 18.530405044555664 \n",
      "epoch 68 global step 20: train batch loss 0.4517316222190857, test loss 18.429054260253906 \n",
      "epoch 68 global step 30: train batch loss 0.4369204640388489, test loss 18.429054260253906 \n",
      "\t epoch 68 end: test loss 18.46283721923828 \n",
      "\t epoch 68 end: precision 0.6363636363636364 \n",
      "\t epoch 68 end: recall    0.739375 \n",
      "\t Checkpoint saved after epoch 68!\n",
      "epoch 69 global step 10: train batch loss 0.5328841209411621, test loss 18.530405044555664 \n",
      "epoch 69 global step 20: train batch loss 0.4517229199409485, test loss 18.41216278076172 \n",
      "epoch 69 global step 30: train batch loss 0.4369310140609741, test loss 18.429054260253906 \n",
      "\t epoch 69 end: test loss 18.46283721923828 \n",
      "\t epoch 69 end: precision 0.6363636363636364 \n",
      "\t epoch 69 end: recall    0.739375 \n",
      "\t Checkpoint saved after epoch 69!\n",
      "epoch 70 global step 10: train batch loss 0.5328811407089233, test loss 18.530405044555664 \n",
      "epoch 70 global step 20: train batch loss 0.4517144560813904, test loss 18.41216278076172 \n",
      "epoch 70 global step 30: train batch loss 0.4369414448738098, test loss 18.429054260253906 \n",
      "\t epoch 70 end: test loss 18.46283721923828 \n",
      "\t epoch 70 end: precision 0.6363636363636364 \n",
      "\t epoch 70 end: recall    0.739375 \n",
      "\t Checkpoint saved after epoch 70!\n",
      "epoch 71 global step 10: train batch loss 0.53287273645401, test loss 18.530405044555664 \n",
      "epoch 71 global step 20: train batch loss 0.45171603560447693, test loss 18.41216278076172 \n",
      "epoch 71 global step 30: train batch loss 0.4368932247161865, test loss 18.429054260253906 \n",
      "\t epoch 71 end: test loss 18.46283721923828 \n",
      "\t epoch 71 end: precision 0.6363636363636364 \n",
      "\t epoch 71 end: recall    0.739375 \n",
      "\t Checkpoint saved after epoch 71!\n",
      "epoch 72 global step 10: train batch loss 0.5328714847564697, test loss 18.530405044555664 \n",
      "epoch 72 global step 20: train batch loss 0.45171186327934265, test loss 18.41216278076172 \n",
      "epoch 72 global step 30: train batch loss 0.43689846992492676, test loss 18.429054260253906 \n",
      "\t epoch 72 end: test loss 18.4797306060791 \n",
      "\t epoch 72 end: precision 0.6361679224973089 \n",
      "\t epoch 72 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 72!\n",
      "epoch 73 global step 10: train batch loss 0.5328697562217712, test loss 18.530405044555664 \n",
      "epoch 73 global step 20: train batch loss 0.4517075717449188, test loss 18.41216278076172 \n",
      "epoch 73 global step 30: train batch loss 0.436903715133667, test loss 18.429054260253906 \n",
      "\t epoch 73 end: test loss 18.4797306060791 \n",
      "\t epoch 73 end: precision 0.6361679224973089 \n",
      "\t epoch 73 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 73!\n",
      "epoch 74 global step 10: train batch loss 0.5328680872917175, test loss 18.530405044555664 \n",
      "epoch 74 global step 20: train batch loss 0.451703280210495, test loss 18.41216278076172 \n",
      "epoch 74 global step 30: train batch loss 0.43690887093544006, test loss 18.429054260253906 \n",
      "\t epoch 74 end: test loss 18.4797306060791 \n",
      "\t epoch 74 end: precision 0.6361679224973089 \n",
      "\t epoch 74 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 74!\n",
      "epoch 75 global step 10: train batch loss 0.5328665375709534, test loss 18.530405044555664 \n",
      "epoch 75 global step 20: train batch loss 0.45169925689697266, test loss 18.41216278076172 \n",
      "epoch 75 global step 30: train batch loss 0.4369140565395355, test loss 18.429054260253906 \n",
      "\t epoch 75 end: test loss 18.4797306060791 \n",
      "\t epoch 75 end: precision 0.6361679224973089 \n",
      "\t epoch 75 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 75!\n",
      "epoch 76 global step 10: train batch loss 0.5328647494316101, test loss 18.530405044555664 \n",
      "epoch 76 global step 20: train batch loss 0.45169493556022644, test loss 18.41216278076172 \n",
      "epoch 76 global step 30: train batch loss 0.43691933155059814, test loss 18.429054260253906 \n",
      "\t epoch 76 end: test loss 18.4797306060791 \n",
      "\t epoch 76 end: precision 0.6361679224973089 \n",
      "\t epoch 76 end: recall    0.73875 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Checkpoint saved after epoch 76!\n",
      "epoch 77 global step 10: train batch loss 0.5328631401062012, test loss 18.530405044555664 \n",
      "epoch 77 global step 20: train batch loss 0.45169079303741455, test loss 18.41216278076172 \n",
      "epoch 77 global step 30: train batch loss 0.43692412972450256, test loss 18.41216278076172 \n",
      "\t epoch 77 end: test loss 18.4797306060791 \n",
      "\t epoch 77 end: precision 0.6361679224973089 \n",
      "\t epoch 77 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 77!\n",
      "epoch 78 global step 10: train batch loss 0.5328613519668579, test loss 18.530405044555664 \n",
      "epoch 78 global step 20: train batch loss 0.4516866207122803, test loss 18.41216278076172 \n",
      "epoch 78 global step 30: train batch loss 0.4369288980960846, test loss 18.41216278076172 \n",
      "\t epoch 78 end: test loss 18.4797306060791 \n",
      "\t epoch 78 end: precision 0.6361679224973089 \n",
      "\t epoch 78 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 78!\n",
      "epoch 79 global step 10: train batch loss 0.532859742641449, test loss 18.530405044555664 \n",
      "epoch 79 global step 20: train batch loss 0.45168250799179077, test loss 18.41216278076172 \n",
      "epoch 79 global step 30: train batch loss 0.4369340240955353, test loss 18.41216278076172 \n",
      "\t epoch 79 end: test loss 18.4797306060791 \n",
      "\t epoch 79 end: precision 0.6361679224973089 \n",
      "\t epoch 79 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 79!\n",
      "epoch 80 global step 10: train batch loss 0.53285813331604, test loss 18.530405044555664 \n",
      "epoch 80 global step 20: train batch loss 0.4516783058643341, test loss 18.41216278076172 \n",
      "epoch 80 global step 30: train batch loss 0.43693873286247253, test loss 18.41216278076172 \n",
      "\t epoch 80 end: test loss 18.4797306060791 \n",
      "\t epoch 80 end: precision 0.6361679224973089 \n",
      "\t epoch 80 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 80!\n",
      "epoch 81 global step 10: train batch loss 0.5328566431999207, test loss 18.530405044555664 \n",
      "epoch 81 global step 20: train batch loss 0.451674222946167, test loss 18.41216278076172 \n",
      "epoch 81 global step 30: train batch loss 0.43694356083869934, test loss 18.41216278076172 \n",
      "\t epoch 81 end: test loss 18.4797306060791 \n",
      "\t epoch 81 end: precision 0.6361679224973089 \n",
      "\t epoch 81 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 81!\n",
      "epoch 82 global step 10: train batch loss 0.5328547954559326, test loss 18.530405044555664 \n",
      "epoch 82 global step 20: train batch loss 0.4516700208187103, test loss 18.41216278076172 \n",
      "epoch 82 global step 30: train batch loss 0.43694832921028137, test loss 18.41216278076172 \n",
      "\t epoch 82 end: test loss 18.4797306060791 \n",
      "\t epoch 82 end: precision 0.6361679224973089 \n",
      "\t epoch 82 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 82!\n",
      "epoch 83 global step 10: train batch loss 0.5328530669212341, test loss 18.530405044555664 \n",
      "epoch 83 global step 20: train batch loss 0.4516659379005432, test loss 18.41216278076172 \n",
      "epoch 83 global step 30: train batch loss 0.43695294857025146, test loss 18.41216278076172 \n",
      "\t epoch 83 end: test loss 18.4797306060791 \n",
      "\t epoch 83 end: precision 0.6361679224973089 \n",
      "\t epoch 83 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 83!\n",
      "epoch 84 global step 10: train batch loss 0.5328514575958252, test loss 18.530405044555664 \n",
      "epoch 84 global step 20: train batch loss 0.45166173577308655, test loss 18.41216278076172 \n",
      "epoch 84 global step 30: train batch loss 0.43695753812789917, test loss 18.41216278076172 \n",
      "\t epoch 84 end: test loss 18.4797306060791 \n",
      "\t epoch 84 end: precision 0.6361679224973089 \n",
      "\t epoch 84 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 84!\n",
      "epoch 85 global step 10: train batch loss 0.5328497290611267, test loss 18.530405044555664 \n",
      "epoch 85 global step 20: train batch loss 0.45165759325027466, test loss 18.41216278076172 \n",
      "epoch 85 global step 30: train batch loss 0.43696218729019165, test loss 18.41216278076172 \n",
      "\t epoch 85 end: test loss 18.4797306060791 \n",
      "\t epoch 85 end: precision 0.6361679224973089 \n",
      "\t epoch 85 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 85!\n",
      "epoch 86 global step 10: train batch loss 0.532848060131073, test loss 18.530405044555664 \n",
      "epoch 86 global step 20: train batch loss 0.45165327191352844, test loss 18.41216278076172 \n",
      "epoch 86 global step 30: train batch loss 0.4369667172431946, test loss 18.41216278076172 \n",
      "\t epoch 86 end: test loss 18.4797306060791 \n",
      "\t epoch 86 end: precision 0.6361679224973089 \n",
      "\t epoch 86 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 86!\n",
      "epoch 87 global step 10: train batch loss 0.532846212387085, test loss 18.530405044555664 \n",
      "epoch 87 global step 20: train batch loss 0.4516489505767822, test loss 18.41216278076172 \n",
      "epoch 87 global step 30: train batch loss 0.4369713068008423, test loss 18.41216278076172 \n",
      "\t epoch 87 end: test loss 18.4797306060791 \n",
      "\t epoch 87 end: precision 0.6361679224973089 \n",
      "\t epoch 87 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 87!\n",
      "epoch 88 global step 10: train batch loss 0.5328447818756104, test loss 18.530405044555664 \n",
      "epoch 88 global step 20: train batch loss 0.45164480805397034, test loss 18.41216278076172 \n",
      "epoch 88 global step 30: train batch loss 0.4369756281375885, test loss 18.41216278076172 \n",
      "\t epoch 88 end: test loss 18.4797306060791 \n",
      "\t epoch 88 end: precision 0.6361679224973089 \n",
      "\t epoch 88 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 88!\n",
      "epoch 89 global step 10: train batch loss 0.5328428149223328, test loss 18.530405044555664 \n",
      "epoch 89 global step 20: train batch loss 0.45164060592651367, test loss 18.41216278076172 \n",
      "epoch 89 global step 30: train batch loss 0.4369802176952362, test loss 18.41216278076172 \n",
      "\t epoch 89 end: test loss 18.4797306060791 \n",
      "\t epoch 89 end: precision 0.6361679224973089 \n",
      "\t epoch 89 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 89!\n",
      "epoch 90 global step 10: train batch loss 0.5328410863876343, test loss 18.54729652404785 \n",
      "epoch 90 global step 20: train batch loss 0.45163649320602417, test loss 18.41216278076172 \n",
      "epoch 90 global step 30: train batch loss 0.4369843900203705, test loss 18.41216278076172 \n",
      "\t epoch 90 end: test loss 18.4797306060791 \n",
      "\t epoch 90 end: precision 0.6361679224973089 \n",
      "\t epoch 90 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 90!\n",
      "epoch 91 global step 10: train batch loss 0.5328394174575806, test loss 18.54729652404785 \n",
      "epoch 91 global step 20: train batch loss 0.45163220167160034, test loss 18.41216278076172 \n",
      "epoch 91 global step 30: train batch loss 0.4369885325431824, test loss 18.41216278076172 \n",
      "\t epoch 91 end: test loss 18.4797306060791 \n",
      "\t epoch 91 end: precision 0.6361679224973089 \n",
      "\t epoch 91 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 91!\n",
      "epoch 92 global step 10: train batch loss 0.5328377485275269, test loss 18.54729652404785 \n",
      "epoch 92 global step 20: train batch loss 0.45162802934646606, test loss 18.41216278076172 \n",
      "epoch 92 global step 30: train batch loss 0.43699294328689575, test loss 18.41216278076172 \n",
      "\t epoch 92 end: test loss 18.4797306060791 \n",
      "\t epoch 92 end: precision 0.6361679224973089 \n",
      "\t epoch 92 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 92!\n",
      "epoch 93 global step 10: train batch loss 0.5328359007835388, test loss 18.54729652404785 \n",
      "epoch 93 global step 20: train batch loss 0.45162370800971985, test loss 18.41216278076172 \n",
      "epoch 93 global step 30: train batch loss 0.43699711561203003, test loss 18.41216278076172 \n",
      "\t epoch 93 end: test loss 18.4797306060791 \n",
      "\t epoch 93 end: precision 0.6361679224973089 \n",
      "\t epoch 93 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 93!\n",
      "epoch 94 global step 10: train batch loss 0.5328342318534851, test loss 18.54729652404785 \n",
      "epoch 94 global step 20: train batch loss 0.45161962509155273, test loss 18.41216278076172 \n",
      "epoch 94 global step 30: train batch loss 0.4370013177394867, test loss 18.41216278076172 \n",
      "\t epoch 94 end: test loss 18.4797306060791 \n",
      "\t epoch 94 end: precision 0.6361679224973089 \n",
      "\t epoch 94 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 94!\n",
      "epoch 95 global step 10: train batch loss 0.5328323245048523, test loss 18.54729652404785 \n",
      "epoch 95 global step 20: train batch loss 0.4516153633594513, test loss 18.41216278076172 \n",
      "epoch 95 global step 30: train batch loss 0.4370054006576538, test loss 18.41216278076172 \n",
      "\t epoch 95 end: test loss 18.4797306060791 \n",
      "\t epoch 95 end: precision 0.6361679224973089 \n",
      "\t epoch 95 end: recall    0.73875 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Checkpoint saved after epoch 95!\n",
      "epoch 96 global step 10: train batch loss 0.5328307151794434, test loss 18.54729652404785 \n",
      "epoch 96 global step 20: train batch loss 0.4516112804412842, test loss 18.41216278076172 \n",
      "epoch 96 global step 30: train batch loss 0.43700966238975525, test loss 18.41216278076172 \n",
      "\t epoch 96 end: test loss 18.4797306060791 \n",
      "\t epoch 96 end: precision 0.6361679224973089 \n",
      "\t epoch 96 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 96!\n",
      "epoch 97 global step 10: train batch loss 0.5328290462493896, test loss 18.54729652404785 \n",
      "epoch 97 global step 20: train batch loss 0.4516071677207947, test loss 18.41216278076172 \n",
      "epoch 97 global step 30: train batch loss 0.4370136260986328, test loss 18.41216278076172 \n",
      "\t epoch 97 end: test loss 18.4797306060791 \n",
      "\t epoch 97 end: precision 0.6361679224973089 \n",
      "\t epoch 97 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 97!\n",
      "epoch 98 global step 10: train batch loss 0.5328272581100464, test loss 18.54729652404785 \n",
      "epoch 98 global step 20: train batch loss 0.45160290598869324, test loss 18.41216278076172 \n",
      "epoch 98 global step 30: train batch loss 0.43701761960983276, test loss 18.41216278076172 \n",
      "\t epoch 98 end: test loss 18.4797306060791 \n",
      "\t epoch 98 end: precision 0.6361679224973089 \n",
      "\t epoch 98 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 98!\n",
      "epoch 99 global step 10: train batch loss 0.5328253507614136, test loss 18.54729652404785 \n",
      "epoch 99 global step 20: train batch loss 0.45159879326820374, test loss 18.41216278076172 \n",
      "epoch 99 global step 30: train batch loss 0.43702152371406555, test loss 18.41216278076172 \n",
      "\t epoch 99 end: test loss 18.4797306060791 \n",
      "\t epoch 99 end: precision 0.6361679224973089 \n",
      "\t epoch 99 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 99!\n",
      "epoch 100 global step 10: train batch loss 0.5328237414360046, test loss 18.54729652404785 \n",
      "epoch 100 global step 20: train batch loss 0.45159462094306946, test loss 18.41216278076172 \n",
      "epoch 100 global step 30: train batch loss 0.43702539801597595, test loss 18.41216278076172 \n",
      "\t epoch 100 end: test loss 18.4797306060791 \n",
      "\t epoch 100 end: precision 0.6361679224973089 \n",
      "\t epoch 100 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 100!\n",
      "epoch 101 global step 10: train batch loss 0.5328218936920166, test loss 18.54729652404785 \n",
      "epoch 101 global step 20: train batch loss 0.4515904486179352, test loss 18.41216278076172 \n",
      "epoch 101 global step 30: train batch loss 0.4370291233062744, test loss 18.41216278076172 \n",
      "\t epoch 101 end: test loss 18.4797306060791 \n",
      "\t epoch 101 end: precision 0.6361679224973089 \n",
      "\t epoch 101 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 101!\n",
      "epoch 102 global step 10: train batch loss 0.5328202247619629, test loss 18.54729652404785 \n",
      "epoch 102 global step 20: train batch loss 0.45158651471138, test loss 18.41216278076172 \n",
      "epoch 102 global step 30: train batch loss 0.4370330274105072, test loss 18.41216278076172 \n",
      "\t epoch 102 end: test loss 18.4797306060791 \n",
      "\t epoch 102 end: precision 0.6361679224973089 \n",
      "\t epoch 102 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 102!\n",
      "epoch 103 global step 10: train batch loss 0.5328182578086853, test loss 18.54729652404785 \n",
      "epoch 103 global step 20: train batch loss 0.4515821635723114, test loss 18.3952693939209 \n",
      "epoch 103 global step 30: train batch loss 0.4370368421077728, test loss 18.41216278076172 \n",
      "\t epoch 103 end: test loss 18.4797306060791 \n",
      "\t epoch 103 end: precision 0.6361679224973089 \n",
      "\t epoch 103 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 103!\n",
      "epoch 104 global step 10: train batch loss 0.5328166484832764, test loss 18.54729652404785 \n",
      "epoch 104 global step 20: train batch loss 0.45157817006111145, test loss 18.3952693939209 \n",
      "epoch 104 global step 30: train batch loss 0.43704062700271606, test loss 18.41216278076172 \n",
      "\t epoch 104 end: test loss 18.4797306060791 \n",
      "\t epoch 104 end: precision 0.6361679224973089 \n",
      "\t epoch 104 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 104!\n",
      "epoch 105 global step 10: train batch loss 0.532814621925354, test loss 18.54729652404785 \n",
      "epoch 105 global step 20: train batch loss 0.45157405734062195, test loss 18.3952693939209 \n",
      "epoch 105 global step 30: train batch loss 0.43704429268836975, test loss 18.41216278076172 \n",
      "\t epoch 105 end: test loss 18.4797306060791 \n",
      "\t epoch 105 end: precision 0.6361679224973089 \n",
      "\t epoch 105 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 105!\n",
      "epoch 106 global step 10: train batch loss 0.5328128933906555, test loss 18.54729652404785 \n",
      "epoch 106 global step 20: train batch loss 0.4515700042247772, test loss 18.3952693939209 \n",
      "epoch 106 global step 30: train batch loss 0.4370478093624115, test loss 18.41216278076172 \n",
      "\t epoch 106 end: test loss 18.4797306060791 \n",
      "\t epoch 106 end: precision 0.6361679224973089 \n",
      "\t epoch 106 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 106!\n",
      "epoch 107 global step 10: train batch loss 0.532811164855957, test loss 18.54729652404785 \n",
      "epoch 107 global step 20: train batch loss 0.4515658915042877, test loss 18.3952693939209 \n",
      "epoch 107 global step 30: train batch loss 0.43705156445503235, test loss 18.41216278076172 \n",
      "\t epoch 107 end: test loss 18.4797306060791 \n",
      "\t epoch 107 end: precision 0.6361679224973089 \n",
      "\t epoch 107 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 107!\n",
      "epoch 108 global step 10: train batch loss 0.5328092575073242, test loss 18.54729652404785 \n",
      "epoch 108 global step 20: train batch loss 0.4515618681907654, test loss 18.3952693939209 \n",
      "epoch 108 global step 30: train batch loss 0.4370551109313965, test loss 18.41216278076172 \n",
      "\t epoch 108 end: test loss 18.4797306060791 \n",
      "\t epoch 108 end: precision 0.6361679224973089 \n",
      "\t epoch 108 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 108!\n",
      "epoch 109 global step 10: train batch loss 0.5328075885772705, test loss 18.54729652404785 \n",
      "epoch 109 global step 20: train batch loss 0.45155778527259827, test loss 18.37837791442871 \n",
      "epoch 109 global step 30: train batch loss 0.43705856800079346, test loss 18.429054260253906 \n",
      "\t epoch 109 end: test loss 18.4797306060791 \n",
      "\t epoch 109 end: precision 0.6361679224973089 \n",
      "\t epoch 109 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 109!\n",
      "epoch 110 global step 10: train batch loss 0.5328058004379272, test loss 18.54729652404785 \n",
      "epoch 110 global step 20: train batch loss 0.4515537619590759, test loss 18.37837791442871 \n",
      "epoch 110 global step 30: train batch loss 0.4370620846748352, test loss 18.429054260253906 \n",
      "\t epoch 110 end: test loss 18.4797306060791 \n",
      "\t epoch 110 end: precision 0.6361679224973089 \n",
      "\t epoch 110 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 110!\n",
      "epoch 111 global step 10: train batch loss 0.5328039526939392, test loss 18.54729652404785 \n",
      "epoch 111 global step 20: train batch loss 0.4515497088432312, test loss 18.37837791442871 \n",
      "epoch 111 global step 30: train batch loss 0.4370654225349426, test loss 18.429054260253906 \n",
      "\t epoch 111 end: test loss 18.4797306060791 \n",
      "\t epoch 111 end: precision 0.6361679224973089 \n",
      "\t epoch 111 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 111!\n",
      "epoch 112 global step 10: train batch loss 0.532802164554596, test loss 18.54729652404785 \n",
      "epoch 112 global step 20: train batch loss 0.4515458047389984, test loss 18.37837791442871 \n",
      "epoch 112 global step 30: train batch loss 0.43706899881362915, test loss 18.429054260253906 \n",
      "\t epoch 112 end: test loss 18.4797306060791 \n",
      "\t epoch 112 end: precision 0.6361679224973089 \n",
      "\t epoch 112 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 112!\n",
      "epoch 113 global step 10: train batch loss 0.5328003764152527, test loss 18.54729652404785 \n",
      "epoch 113 global step 20: train batch loss 0.4515415132045746, test loss 18.37837791442871 \n",
      "epoch 113 global step 30: train batch loss 0.43707239627838135, test loss 18.429054260253906 \n",
      "\t epoch 113 end: test loss 18.4797306060791 \n",
      "\t epoch 113 end: precision 0.6361679224973089 \n",
      "\t epoch 113 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 113!\n",
      "epoch 114 global step 10: train batch loss 0.5327984690666199, test loss 18.54729652404785 \n",
      "epoch 114 global step 20: train batch loss 0.4515376687049866, test loss 18.37837791442871 \n",
      "epoch 114 global step 30: train batch loss 0.4370756447315216, test loss 18.429054260253906 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t epoch 114 end: test loss 18.4797306060791 \n",
      "\t epoch 114 end: precision 0.6361679224973089 \n",
      "\t epoch 114 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 114!\n",
      "epoch 115 global step 10: train batch loss 0.5327966809272766, test loss 18.54729652404785 \n",
      "epoch 115 global step 20: train batch loss 0.4515336751937866, test loss 18.37837791442871 \n",
      "epoch 115 global step 30: train batch loss 0.4370791018009186, test loss 18.429054260253906 \n",
      "\t epoch 115 end: test loss 18.4797306060791 \n",
      "\t epoch 115 end: precision 0.6361679224973089 \n",
      "\t epoch 115 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 115!\n",
      "epoch 116 global step 10: train batch loss 0.5327948927879333, test loss 18.54729652404785 \n",
      "epoch 116 global step 20: train batch loss 0.4515294134616852, test loss 18.37837791442871 \n",
      "epoch 116 global step 30: train batch loss 0.4370822012424469, test loss 18.429054260253906 \n",
      "\t epoch 116 end: test loss 18.4797306060791 \n",
      "\t epoch 116 end: precision 0.6361679224973089 \n",
      "\t epoch 116 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 116!\n",
      "epoch 117 global step 10: train batch loss 0.5327932238578796, test loss 18.54729652404785 \n",
      "epoch 117 global step 20: train batch loss 0.45152565836906433, test loss 18.37837791442871 \n",
      "epoch 117 global step 30: train batch loss 0.4370856285095215, test loss 18.429054260253906 \n",
      "\t epoch 117 end: test loss 18.4797306060791 \n",
      "\t epoch 117 end: precision 0.6361679224973089 \n",
      "\t epoch 117 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 117!\n",
      "epoch 118 global step 10: train batch loss 0.5327913165092468, test loss 18.54729652404785 \n",
      "epoch 118 global step 20: train batch loss 0.45152154564857483, test loss 18.37837791442871 \n",
      "epoch 118 global step 30: train batch loss 0.43708890676498413, test loss 18.429054260253906 \n",
      "\t epoch 118 end: test loss 18.4797306060791 \n",
      "\t epoch 118 end: precision 0.6361679224973089 \n",
      "\t epoch 118 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 118!\n",
      "epoch 119 global step 10: train batch loss 0.532789409160614, test loss 18.54729652404785 \n",
      "epoch 119 global step 20: train batch loss 0.4515174925327301, test loss 18.37837791442871 \n",
      "epoch 119 global step 30: train batch loss 0.4370919167995453, test loss 18.429054260253906 \n",
      "\t epoch 119 end: test loss 18.4797306060791 \n",
      "\t epoch 119 end: precision 0.6361679224973089 \n",
      "\t epoch 119 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 119!\n",
      "epoch 120 global step 10: train batch loss 0.5327874422073364, test loss 18.54729652404785 \n",
      "epoch 120 global step 20: train batch loss 0.4515133798122406, test loss 18.37837791442871 \n",
      "epoch 120 global step 30: train batch loss 0.43709510564804077, test loss 18.429054260253906 \n",
      "\t epoch 120 end: test loss 18.4797306060791 \n",
      "\t epoch 120 end: precision 0.6361679224973089 \n",
      "\t epoch 120 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 120!\n",
      "epoch 121 global step 10: train batch loss 0.5327857732772827, test loss 18.54729652404785 \n",
      "epoch 121 global step 20: train batch loss 0.4515095353126526, test loss 18.37837791442871 \n",
      "epoch 121 global step 30: train batch loss 0.4370982348918915, test loss 18.429054260253906 \n",
      "\t epoch 121 end: test loss 18.4797306060791 \n",
      "\t epoch 121 end: precision 0.6361679224973089 \n",
      "\t epoch 121 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 121!\n",
      "epoch 122 global step 10: train batch loss 0.532783567905426, test loss 18.54729652404785 \n",
      "epoch 122 global step 20: train batch loss 0.4515054523944855, test loss 18.37837791442871 \n",
      "epoch 122 global step 30: train batch loss 0.4371013045310974, test loss 18.429054260253906 \n",
      "\t epoch 122 end: test loss 18.4797306060791 \n",
      "\t epoch 122 end: precision 0.6361679224973089 \n",
      "\t epoch 122 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 122!\n",
      "epoch 123 global step 10: train batch loss 0.5327820777893066, test loss 18.54729652404785 \n",
      "epoch 123 global step 20: train batch loss 0.45150142908096313, test loss 18.37837791442871 \n",
      "epoch 123 global step 30: train batch loss 0.43710431456565857, test loss 18.429054260253906 \n",
      "\t epoch 123 end: test loss 18.4797306060791 \n",
      "\t epoch 123 end: precision 0.6361679224973089 \n",
      "\t epoch 123 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 123!\n",
      "epoch 124 global step 10: train batch loss 0.5327801704406738, test loss 18.54729652404785 \n",
      "epoch 124 global step 20: train batch loss 0.45149746537208557, test loss 18.37837791442871 \n",
      "epoch 124 global step 30: train batch loss 0.4371073842048645, test loss 18.429054260253906 \n",
      "\t epoch 124 end: test loss 18.4797306060791 \n",
      "\t epoch 124 end: precision 0.6361679224973089 \n",
      "\t epoch 124 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 124!\n",
      "epoch 125 global step 10: train batch loss 0.5327780842781067, test loss 18.54729652404785 \n",
      "epoch 125 global step 20: train batch loss 0.45149335265159607, test loss 18.37837791442871 \n",
      "epoch 125 global step 30: train batch loss 0.43711045384407043, test loss 18.429054260253906 \n",
      "\t epoch 125 end: test loss 18.4797306060791 \n",
      "\t epoch 125 end: precision 0.6361679224973089 \n",
      "\t epoch 125 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 125!\n",
      "epoch 126 global step 10: train batch loss 0.5327765345573425, test loss 18.54729652404785 \n",
      "epoch 126 global step 20: train batch loss 0.4514895975589752, test loss 18.37837791442871 \n",
      "epoch 126 global step 30: train batch loss 0.43711337447166443, test loss 18.429054260253906 \n",
      "\t epoch 126 end: test loss 18.4797306060791 \n",
      "\t epoch 126 end: precision 0.6361679224973089 \n",
      "\t epoch 126 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 126!\n",
      "epoch 127 global step 10: train batch loss 0.5327743887901306, test loss 18.54729652404785 \n",
      "epoch 127 global step 20: train batch loss 0.45148545503616333, test loss 18.37837791442871 \n",
      "epoch 127 global step 30: train batch loss 0.4371163845062256, test loss 18.429054260253906 \n",
      "\t epoch 127 end: test loss 18.4797306060791 \n",
      "\t epoch 127 end: precision 0.6361679224973089 \n",
      "\t epoch 127 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 127!\n",
      "epoch 128 global step 10: train batch loss 0.5327724814414978, test loss 18.54729652404785 \n",
      "epoch 128 global step 20: train batch loss 0.45148155093193054, test loss 18.37837791442871 \n",
      "epoch 128 global step 30: train batch loss 0.43711936473846436, test loss 18.429054260253906 \n",
      "\t epoch 128 end: test loss 18.4797306060791 \n",
      "\t epoch 128 end: precision 0.6361679224973089 \n",
      "\t epoch 128 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 128!\n",
      "epoch 129 global step 10: train batch loss 0.5327706933021545, test loss 18.54729652404785 \n",
      "epoch 129 global step 20: train batch loss 0.451477587223053, test loss 18.37837791442871 \n",
      "epoch 129 global step 30: train batch loss 0.43712207674980164, test loss 18.429054260253906 \n",
      "\t epoch 129 end: test loss 18.4797306060791 \n",
      "\t epoch 129 end: precision 0.6361679224973089 \n",
      "\t epoch 129 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 129!\n",
      "epoch 130 global step 10: train batch loss 0.5327688455581665, test loss 18.54729652404785 \n",
      "epoch 130 global step 20: train batch loss 0.45147374272346497, test loss 18.37837791442871 \n",
      "epoch 130 global step 30: train batch loss 0.4371250867843628, test loss 18.429054260253906 \n",
      "\t epoch 130 end: test loss 18.4797306060791 \n",
      "\t epoch 130 end: precision 0.6361679224973089 \n",
      "\t epoch 130 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 130!\n",
      "epoch 131 global step 10: train batch loss 0.5327666997909546, test loss 18.54729652404785 \n",
      "epoch 131 global step 20: train batch loss 0.45146963000297546, test loss 18.37837791442871 \n",
      "epoch 131 global step 30: train batch loss 0.437127947807312, test loss 18.429054260253906 \n",
      "\t epoch 131 end: test loss 18.4797306060791 \n",
      "\t epoch 131 end: precision 0.6361679224973089 \n",
      "\t epoch 131 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 131!\n",
      "epoch 132 global step 10: train batch loss 0.5327651500701904, test loss 18.54729652404785 \n",
      "epoch 132 global step 20: train batch loss 0.4514658451080322, test loss 18.37837791442871 \n",
      "epoch 132 global step 30: train batch loss 0.43713048100471497, test loss 18.429054260253906 \n",
      "\t epoch 132 end: test loss 18.4797306060791 \n",
      "\t epoch 132 end: precision 0.6361679224973089 \n",
      "\t epoch 132 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 132!\n",
      "epoch 133 global step 10: train batch loss 0.5327629446983337, test loss 18.54729652404785 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 133 global step 20: train batch loss 0.4514618217945099, test loss 18.37837791442871 \n",
      "epoch 133 global step 30: train batch loss 0.4371333420276642, test loss 18.429054260253906 \n",
      "\t epoch 133 end: test loss 18.4797306060791 \n",
      "\t epoch 133 end: precision 0.6361679224973089 \n",
      "\t epoch 133 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 133!\n",
      "epoch 134 global step 10: train batch loss 0.5327611565589905, test loss 18.54729652404785 \n",
      "epoch 134 global step 20: train batch loss 0.45145782828330994, test loss 18.37837791442871 \n",
      "epoch 134 global step 30: train batch loss 0.43713614344596863, test loss 18.429054260253906 \n",
      "\t epoch 134 end: test loss 18.4797306060791 \n",
      "\t epoch 134 end: precision 0.6361679224973089 \n",
      "\t epoch 134 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 134!\n",
      "epoch 135 global step 10: train batch loss 0.5327591300010681, test loss 18.54729652404785 \n",
      "epoch 135 global step 20: train batch loss 0.45145383477211, test loss 18.37837791442871 \n",
      "epoch 135 global step 30: train batch loss 0.43713876605033875, test loss 18.429054260253906 \n",
      "\t epoch 135 end: test loss 18.4797306060791 \n",
      "\t epoch 135 end: precision 0.6361679224973089 \n",
      "\t epoch 135 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 135!\n",
      "epoch 136 global step 10: train batch loss 0.5327571034431458, test loss 18.54729652404785 \n",
      "epoch 136 global step 20: train batch loss 0.45144984126091003, test loss 18.37837791442871 \n",
      "epoch 136 global step 30: train batch loss 0.43714141845703125, test loss 18.429054260253906 \n",
      "\t epoch 136 end: test loss 18.4797306060791 \n",
      "\t epoch 136 end: precision 0.6361679224973089 \n",
      "\t epoch 136 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 136!\n",
      "epoch 137 global step 10: train batch loss 0.5327555537223816, test loss 18.54729652404785 \n",
      "epoch 137 global step 20: train batch loss 0.4514460265636444, test loss 18.37837791442871 \n",
      "epoch 137 global step 30: train batch loss 0.4371441900730133, test loss 18.429054260253906 \n",
      "\t epoch 137 end: test loss 18.4797306060791 \n",
      "\t epoch 137 end: precision 0.6361679224973089 \n",
      "\t epoch 137 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 137!\n",
      "epoch 138 global step 10: train batch loss 0.5327533483505249, test loss 18.54729652404785 \n",
      "epoch 138 global step 20: train batch loss 0.4514419138431549, test loss 18.37837791442871 \n",
      "epoch 138 global step 30: train batch loss 0.4371468722820282, test loss 18.429054260253906 \n",
      "\t epoch 138 end: test loss 18.4797306060791 \n",
      "\t epoch 138 end: precision 0.6361679224973089 \n",
      "\t epoch 138 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 138!\n",
      "epoch 139 global step 10: train batch loss 0.5327515006065369, test loss 18.54729652404785 \n",
      "epoch 139 global step 20: train batch loss 0.4514380097389221, test loss 18.37837791442871 \n",
      "epoch 139 global step 30: train batch loss 0.43714937567710876, test loss 18.429054260253906 \n",
      "\t epoch 139 end: test loss 18.4797306060791 \n",
      "\t epoch 139 end: precision 0.6361679224973089 \n",
      "\t epoch 139 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 139!\n",
      "epoch 140 global step 10: train batch loss 0.5327495336532593, test loss 18.54729652404785 \n",
      "epoch 140 global step 20: train batch loss 0.451433926820755, test loss 18.37837791442871 \n",
      "epoch 140 global step 30: train batch loss 0.43715184926986694, test loss 18.429054260253906 \n",
      "\t epoch 140 end: test loss 18.4797306060791 \n",
      "\t epoch 140 end: precision 0.6361679224973089 \n",
      "\t epoch 140 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 140!\n",
      "epoch 141 global step 10: train batch loss 0.5327475666999817, test loss 18.54729652404785 \n",
      "epoch 141 global step 20: train batch loss 0.4514300227165222, test loss 18.37837791442871 \n",
      "epoch 141 global step 30: train batch loss 0.437154620885849, test loss 18.429054260253906 \n",
      "\t epoch 141 end: test loss 18.4797306060791 \n",
      "\t epoch 141 end: precision 0.6361679224973089 \n",
      "\t epoch 141 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 141!\n",
      "epoch 142 global step 10: train batch loss 0.5327455401420593, test loss 18.54729652404785 \n",
      "epoch 142 global step 20: train batch loss 0.4514261484146118, test loss 18.37837791442871 \n",
      "epoch 142 global step 30: train batch loss 0.43715712428092957, test loss 18.429054260253906 \n",
      "\t epoch 142 end: test loss 18.4797306060791 \n",
      "\t epoch 142 end: precision 0.6361679224973089 \n",
      "\t epoch 142 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 142!\n",
      "epoch 143 global step 10: train batch loss 0.5327436327934265, test loss 18.54729652404785 \n",
      "epoch 143 global step 20: train batch loss 0.4514221251010895, test loss 18.37837791442871 \n",
      "epoch 143 global step 30: train batch loss 0.4371595084667206, test loss 18.429054260253906 \n",
      "\t epoch 143 end: test loss 18.4797306060791 \n",
      "\t epoch 143 end: precision 0.6361679224973089 \n",
      "\t epoch 143 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 143!\n",
      "epoch 144 global step 10: train batch loss 0.5327417254447937, test loss 18.54729652404785 \n",
      "epoch 144 global step 20: train batch loss 0.4514182507991791, test loss 18.37837791442871 \n",
      "epoch 144 global step 30: train batch loss 0.43716225028038025, test loss 18.429054260253906 \n",
      "\t epoch 144 end: test loss 18.4797306060791 \n",
      "\t epoch 144 end: precision 0.6361679224973089 \n",
      "\t epoch 144 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 144!\n",
      "epoch 145 global step 10: train batch loss 0.5327398180961609, test loss 18.54729652404785 \n",
      "epoch 145 global step 20: train batch loss 0.4514142870903015, test loss 18.37837791442871 \n",
      "epoch 145 global step 30: train batch loss 0.4371644854545593, test loss 18.429054260253906 \n",
      "\t epoch 145 end: test loss 18.4797306060791 \n",
      "\t epoch 145 end: precision 0.6361679224973089 \n",
      "\t epoch 145 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 145!\n",
      "epoch 146 global step 10: train batch loss 0.5327378511428833, test loss 18.54729652404785 \n",
      "epoch 146 global step 20: train batch loss 0.4514102339744568, test loss 18.37837791442871 \n",
      "epoch 146 global step 30: train batch loss 0.4371671974658966, test loss 18.429054260253906 \n",
      "\t epoch 146 end: test loss 18.4797306060791 \n",
      "\t epoch 146 end: precision 0.6361679224973089 \n",
      "\t epoch 146 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 146!\n",
      "epoch 147 global step 10: train batch loss 0.53273606300354, test loss 18.54729652404785 \n",
      "epoch 147 global step 20: train batch loss 0.45140641927719116, test loss 18.37837791442871 \n",
      "epoch 147 global step 30: train batch loss 0.43716946244239807, test loss 18.429054260253906 \n",
      "\t epoch 147 end: test loss 18.4797306060791 \n",
      "\t epoch 147 end: precision 0.6361679224973089 \n",
      "\t epoch 147 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 147!\n",
      "epoch 148 global step 10: train batch loss 0.5327337384223938, test loss 18.54729652404785 \n",
      "epoch 148 global step 20: train batch loss 0.4514024555683136, test loss 18.37837791442871 \n",
      "epoch 148 global step 30: train batch loss 0.4371718168258667, test loss 18.429054260253906 \n",
      "\t epoch 148 end: test loss 18.4797306060791 \n",
      "\t epoch 148 end: precision 0.6361679224973089 \n",
      "\t epoch 148 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 148!\n",
      "epoch 149 global step 10: train batch loss 0.5327317714691162, test loss 18.54729652404785 \n",
      "epoch 149 global step 20: train batch loss 0.45139840245246887, test loss 18.37837791442871 \n",
      "epoch 149 global step 30: train batch loss 0.4371740520000458, test loss 18.429054260253906 \n",
      "\t epoch 149 end: test loss 18.4797306060791 \n",
      "\t epoch 149 end: precision 0.6361679224973089 \n",
      "\t epoch 149 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 149!\n",
      "epoch 150 global step 10: train batch loss 0.532729983329773, test loss 18.54729652404785 \n",
      "epoch 150 global step 20: train batch loss 0.4513944983482361, test loss 18.37837791442871 \n",
      "epoch 150 global step 30: train batch loss 0.4371764361858368, test loss 18.429054260253906 \n",
      "\t epoch 150 end: test loss 18.4797306060791 \n",
      "\t epoch 150 end: precision 0.6361679224973089 \n",
      "\t epoch 150 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 150!\n",
      "epoch 151 global step 10: train batch loss 0.5327280759811401, test loss 18.54729652404785 \n",
      "epoch 151 global step 20: train batch loss 0.4513907730579376, test loss 18.37837791442871 \n",
      "epoch 151 global step 30: train batch loss 0.43717870116233826, test loss 18.429054260253906 \n",
      "\t epoch 151 end: test loss 18.4797306060791 \n",
      "\t epoch 151 end: precision 0.6361679224973089 \n",
      "\t epoch 151 end: recall    0.73875 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Checkpoint saved after epoch 151!\n",
      "epoch 152 global step 10: train batch loss 0.5327259302139282, test loss 18.54729652404785 \n",
      "epoch 152 global step 20: train batch loss 0.4513866603374481, test loss 18.37837791442871 \n",
      "epoch 152 global step 30: train batch loss 0.43718093633651733, test loss 18.429054260253906 \n",
      "\t epoch 152 end: test loss 18.4797306060791 \n",
      "\t epoch 152 end: precision 0.6361679224973089 \n",
      "\t epoch 152 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 152!\n",
      "epoch 153 global step 10: train batch loss 0.532724142074585, test loss 18.54729652404785 \n",
      "epoch 153 global step 20: train batch loss 0.45138299465179443, test loss 18.37837791442871 \n",
      "epoch 153 global step 30: train batch loss 0.437183141708374, test loss 18.429054260253906 \n",
      "\t epoch 153 end: test loss 18.4797306060791 \n",
      "\t epoch 153 end: precision 0.6361679224973089 \n",
      "\t epoch 153 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 153!\n",
      "epoch 154 global step 10: train batch loss 0.5327220559120178, test loss 18.54729652404785 \n",
      "epoch 154 global step 20: train batch loss 0.4513789117336273, test loss 18.37837791442871 \n",
      "epoch 154 global step 30: train batch loss 0.4371854066848755, test loss 18.429054260253906 \n",
      "\t epoch 154 end: test loss 18.4797306060791 \n",
      "\t epoch 154 end: precision 0.6361679224973089 \n",
      "\t epoch 154 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 154!\n",
      "epoch 155 global step 10: train batch loss 0.532720148563385, test loss 18.54729652404785 \n",
      "epoch 155 global step 20: train batch loss 0.4513750970363617, test loss 18.37837791442871 \n",
      "epoch 155 global step 30: train batch loss 0.43718740344047546, test loss 18.429054260253906 \n",
      "\t epoch 155 end: test loss 18.4797306060791 \n",
      "\t epoch 155 end: precision 0.6361679224973089 \n",
      "\t epoch 155 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 155!\n",
      "epoch 156 global step 10: train batch loss 0.5327181220054626, test loss 18.54729652404785 \n",
      "epoch 156 global step 20: train batch loss 0.45137110352516174, test loss 18.37837791442871 \n",
      "epoch 156 global step 30: train batch loss 0.43718963861465454, test loss 18.429054260253906 \n",
      "\t epoch 156 end: test loss 18.4797306060791 \n",
      "\t epoch 156 end: precision 0.6361679224973089 \n",
      "\t epoch 156 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 156!\n",
      "epoch 157 global step 10: train batch loss 0.5327162742614746, test loss 18.54729652404785 \n",
      "epoch 157 global step 20: train batch loss 0.45136746764183044, test loss 18.361486434936523 \n",
      "epoch 157 global step 30: train batch loss 0.4371916949748993, test loss 18.429054260253906 \n",
      "\t epoch 157 end: test loss 18.4797306060791 \n",
      "\t epoch 157 end: precision 0.6361679224973089 \n",
      "\t epoch 157 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 157!\n",
      "epoch 158 global step 10: train batch loss 0.5327140688896179, test loss 18.54729652404785 \n",
      "epoch 158 global step 20: train batch loss 0.4513632357120514, test loss 18.361486434936523 \n",
      "epoch 158 global step 30: train batch loss 0.4371938407421112, test loss 18.429054260253906 \n",
      "\t epoch 158 end: test loss 18.4797306060791 \n",
      "\t epoch 158 end: precision 0.6361679224973089 \n",
      "\t epoch 158 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 158!\n",
      "epoch 159 global step 10: train batch loss 0.5327123999595642, test loss 18.54729652404785 \n",
      "epoch 159 global step 20: train batch loss 0.4513595998287201, test loss 18.361486434936523 \n",
      "epoch 159 global step 30: train batch loss 0.4371958076953888, test loss 18.429054260253906 \n",
      "\t epoch 159 end: test loss 18.46283721923828 \n",
      "\t epoch 159 end: precision 0.6365105008077544 \n",
      "\t epoch 159 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 159!\n",
      "epoch 160 global step 10: train batch loss 0.5327102541923523, test loss 18.54729652404785 \n",
      "epoch 160 global step 20: train batch loss 0.45135560631752014, test loss 18.361486434936523 \n",
      "epoch 160 global step 30: train batch loss 0.4371979236602783, test loss 18.429054260253906 \n",
      "\t epoch 160 end: test loss 18.46283721923828 \n",
      "\t epoch 160 end: precision 0.6365105008077544 \n",
      "\t epoch 160 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 160!\n",
      "epoch 161 global step 10: train batch loss 0.5327082276344299, test loss 18.54729652404785 \n",
      "epoch 161 global step 20: train batch loss 0.45135167241096497, test loss 18.361486434936523 \n",
      "epoch 161 global step 30: train batch loss 0.4371998608112335, test loss 18.429054260253906 \n",
      "\t epoch 161 end: test loss 18.46283721923828 \n",
      "\t epoch 161 end: precision 0.6365105008077544 \n",
      "\t epoch 161 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 161!\n",
      "epoch 162 global step 10: train batch loss 0.5327063202857971, test loss 18.54729652404785 \n",
      "epoch 162 global step 20: train batch loss 0.4513479769229889, test loss 18.361486434936523 \n",
      "epoch 162 global step 30: train batch loss 0.43720200657844543, test loss 18.429054260253906 \n",
      "\t epoch 162 end: test loss 18.46283721923828 \n",
      "\t epoch 162 end: precision 0.6365105008077544 \n",
      "\t epoch 162 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 162!\n",
      "epoch 163 global step 10: train batch loss 0.5327043533325195, test loss 18.54729652404785 \n",
      "epoch 163 global step 20: train batch loss 0.45134398341178894, test loss 18.361486434936523 \n",
      "epoch 163 global step 30: train batch loss 0.4372038245201111, test loss 18.429054260253906 \n",
      "\t epoch 163 end: test loss 18.46283721923828 \n",
      "\t epoch 163 end: precision 0.6365105008077544 \n",
      "\t epoch 163 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 163!\n",
      "epoch 164 global step 10: train batch loss 0.5327023267745972, test loss 18.54729652404785 \n",
      "epoch 164 global step 20: train batch loss 0.45134007930755615, test loss 18.361486434936523 \n",
      "epoch 164 global step 30: train batch loss 0.4372057020664215, test loss 18.429054260253906 \n",
      "\t epoch 164 end: test loss 18.46283721923828 \n",
      "\t epoch 164 end: precision 0.6365105008077544 \n",
      "\t epoch 164 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 164!\n",
      "epoch 165 global step 10: train batch loss 0.5327003002166748, test loss 18.54729652404785 \n",
      "epoch 165 global step 20: train batch loss 0.45133641362190247, test loss 18.361486434936523 \n",
      "epoch 165 global step 30: train batch loss 0.43720757961273193, test loss 18.429054260253906 \n",
      "\t epoch 165 end: test loss 18.46283721923828 \n",
      "\t epoch 165 end: precision 0.6365105008077544 \n",
      "\t epoch 165 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 165!\n",
      "epoch 166 global step 10: train batch loss 0.5326984524726868, test loss 18.54729652404785 \n",
      "epoch 166 global step 20: train batch loss 0.45133253931999207, test loss 18.361486434936523 \n",
      "epoch 166 global step 30: train batch loss 0.43720942735671997, test loss 18.429054260253906 \n",
      "\t epoch 166 end: test loss 18.46283721923828 \n",
      "\t epoch 166 end: precision 0.6365105008077544 \n",
      "\t epoch 166 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 166!\n",
      "epoch 167 global step 10: train batch loss 0.5326964259147644, test loss 18.54729652404785 \n",
      "epoch 167 global step 20: train batch loss 0.4513288736343384, test loss 18.361486434936523 \n",
      "epoch 167 global step 30: train batch loss 0.4372112452983856, test loss 18.429054260253906 \n",
      "\t epoch 167 end: test loss 18.46283721923828 \n",
      "\t epoch 167 end: precision 0.6365105008077544 \n",
      "\t epoch 167 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 167!\n",
      "epoch 168 global step 10: train batch loss 0.5326942205429077, test loss 18.54729652404785 \n",
      "epoch 168 global step 20: train batch loss 0.4513249099254608, test loss 18.361486434936523 \n",
      "epoch 168 global step 30: train batch loss 0.4372131824493408, test loss 18.429054260253906 \n",
      "\t epoch 168 end: test loss 18.46283721923828 \n",
      "\t epoch 168 end: precision 0.6365105008077544 \n",
      "\t epoch 168 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 168!\n",
      "epoch 169 global step 10: train batch loss 0.532692551612854, test loss 18.54729652404785 \n",
      "epoch 169 global step 20: train batch loss 0.4513213634490967, test loss 18.361486434936523 \n",
      "epoch 169 global step 30: train batch loss 0.4372148811817169, test loss 18.41216278076172 \n",
      "\t epoch 169 end: test loss 18.46283721923828 \n",
      "\t epoch 169 end: precision 0.6365105008077544 \n",
      "\t epoch 169 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 169!\n",
      "epoch 170 global step 10: train batch loss 0.5326903462409973, test loss 18.54729652404785 \n",
      "epoch 170 global step 20: train batch loss 0.4513174593448639, test loss 18.361486434936523 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 170 global step 30: train batch loss 0.4372166395187378, test loss 18.41216278076172 \n",
      "\t epoch 170 end: test loss 18.46283721923828 \n",
      "\t epoch 170 end: precision 0.6365105008077544 \n",
      "\t epoch 170 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 170!\n",
      "epoch 171 global step 10: train batch loss 0.5326883792877197, test loss 18.54729652404785 \n",
      "epoch 171 global step 20: train batch loss 0.45131364464759827, test loss 18.361486434936523 \n",
      "epoch 171 global step 30: train batch loss 0.43721824884414673, test loss 18.41216278076172 \n",
      "\t epoch 171 end: test loss 18.46283721923828 \n",
      "\t epoch 171 end: precision 0.6365105008077544 \n",
      "\t epoch 171 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 171!\n",
      "epoch 172 global step 10: train batch loss 0.5326865315437317, test loss 18.54729652404785 \n",
      "epoch 172 global step 20: train batch loss 0.4513099491596222, test loss 18.361486434936523 \n",
      "epoch 172 global step 30: train batch loss 0.4372200667858124, test loss 18.41216278076172 \n",
      "\t epoch 172 end: test loss 18.46283721923828 \n",
      "\t epoch 172 end: precision 0.6365105008077544 \n",
      "\t epoch 172 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 172!\n",
      "epoch 173 global step 10: train batch loss 0.5326843857765198, test loss 18.54729652404785 \n",
      "epoch 173 global step 20: train batch loss 0.45130619406700134, test loss 18.361486434936523 \n",
      "epoch 173 global step 30: train batch loss 0.43722182512283325, test loss 18.41216278076172 \n",
      "\t epoch 173 end: test loss 18.46283721923828 \n",
      "\t epoch 173 end: precision 0.6365105008077544 \n",
      "\t epoch 173 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 173!\n",
      "epoch 174 global step 10: train batch loss 0.5326826572418213, test loss 18.54729652404785 \n",
      "epoch 174 global step 20: train batch loss 0.4513024687767029, test loss 18.361486434936523 \n",
      "epoch 174 global step 30: train batch loss 0.4372233748435974, test loss 18.41216278076172 \n",
      "\t epoch 174 end: test loss 18.46283721923828 \n",
      "\t epoch 174 end: precision 0.6365105008077544 \n",
      "\t epoch 174 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 174!\n",
      "epoch 175 global step 10: train batch loss 0.5326802730560303, test loss 18.54729652404785 \n",
      "epoch 175 global step 20: train batch loss 0.45129868388175964, test loss 18.361486434936523 \n",
      "epoch 175 global step 30: train batch loss 0.4372251629829407, test loss 18.41216278076172 \n",
      "\t epoch 175 end: test loss 18.46283721923828 \n",
      "\t epoch 175 end: precision 0.6365105008077544 \n",
      "\t epoch 175 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 175!\n",
      "epoch 176 global step 10: train batch loss 0.5326786041259766, test loss 18.54729652404785 \n",
      "epoch 176 global step 20: train batch loss 0.45129483938217163, test loss 18.361486434936523 \n",
      "epoch 176 global step 30: train batch loss 0.4372265040874481, test loss 18.41216278076172 \n",
      "\t epoch 176 end: test loss 18.46283721923828 \n",
      "\t epoch 176 end: precision 0.6365105008077544 \n",
      "\t epoch 176 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 176!\n",
      "epoch 177 global step 10: train batch loss 0.5326762795448303, test loss 18.54729652404785 \n",
      "epoch 177 global step 20: train batch loss 0.45129114389419556, test loss 18.344594955444336 \n",
      "epoch 177 global step 30: train batch loss 0.43722838163375854, test loss 18.41216278076172 \n",
      "\t epoch 177 end: test loss 18.46283721923828 \n",
      "\t epoch 177 end: precision 0.6365105008077544 \n",
      "\t epoch 177 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 177!\n",
      "epoch 178 global step 10: train batch loss 0.5326744914054871, test loss 18.54729652404785 \n",
      "epoch 178 global step 20: train batch loss 0.45128726959228516, test loss 18.344594955444336 \n",
      "epoch 178 global step 30: train batch loss 0.43722984194755554, test loss 18.41216278076172 \n",
      "\t epoch 178 end: test loss 18.46283721923828 \n",
      "\t epoch 178 end: precision 0.6365105008077544 \n",
      "\t epoch 178 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 178!\n",
      "epoch 179 global step 10: train batch loss 0.5326724648475647, test loss 18.54729652404785 \n",
      "epoch 179 global step 20: train batch loss 0.45128366351127625, test loss 18.344594955444336 \n",
      "epoch 179 global step 30: train batch loss 0.43723154067993164, test loss 18.41216278076172 \n",
      "\t epoch 179 end: test loss 18.46283721923828 \n",
      "\t epoch 179 end: precision 0.6365105008077544 \n",
      "\t epoch 179 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 179!\n",
      "epoch 180 global step 10: train batch loss 0.5326703190803528, test loss 18.54729652404785 \n",
      "epoch 180 global step 20: train batch loss 0.4512799382209778, test loss 18.344594955444336 \n",
      "epoch 180 global step 30: train batch loss 0.43723300099372864, test loss 18.41216278076172 \n",
      "\t epoch 180 end: test loss 18.46283721923828 \n",
      "\t epoch 180 end: precision 0.6365105008077544 \n",
      "\t epoch 180 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 180!\n",
      "epoch 181 global step 10: train batch loss 0.5326681733131409, test loss 18.54729652404785 \n",
      "epoch 181 global step 20: train batch loss 0.4512759745121002, test loss 18.344594955444336 \n",
      "epoch 181 global step 30: train batch loss 0.43723446130752563, test loss 18.41216278076172 \n",
      "\t epoch 181 end: test loss 18.46283721923828 \n",
      "\t epoch 181 end: precision 0.6365105008077544 \n",
      "\t epoch 181 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 181!\n",
      "epoch 182 global step 10: train batch loss 0.5326663851737976, test loss 18.54729652404785 \n",
      "epoch 182 global step 20: train batch loss 0.45127248764038086, test loss 18.344594955444336 \n",
      "epoch 182 global step 30: train batch loss 0.43723589181900024, test loss 18.41216278076172 \n",
      "\t epoch 182 end: test loss 18.46283721923828 \n",
      "\t epoch 182 end: precision 0.6365105008077544 \n",
      "\t epoch 182 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 182!\n",
      "epoch 183 global step 10: train batch loss 0.5326642990112305, test loss 18.54729652404785 \n",
      "epoch 183 global step 20: train batch loss 0.4512685537338257, test loss 18.344594955444336 \n",
      "epoch 183 global step 30: train batch loss 0.43723759055137634, test loss 18.41216278076172 \n",
      "\t epoch 183 end: test loss 18.46283721923828 \n",
      "\t epoch 183 end: precision 0.6365105008077544 \n",
      "\t epoch 183 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 183!\n",
      "epoch 184 global step 10: train batch loss 0.5326621532440186, test loss 18.54729652404785 \n",
      "epoch 184 global step 20: train batch loss 0.4512649476528168, test loss 18.344594955444336 \n",
      "epoch 184 global step 30: train batch loss 0.437238872051239, test loss 18.41216278076172 \n",
      "\t epoch 184 end: test loss 18.46283721923828 \n",
      "\t epoch 184 end: precision 0.6365105008077544 \n",
      "\t epoch 184 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 184!\n",
      "epoch 185 global step 10: train batch loss 0.5326604247093201, test loss 18.54729652404785 \n",
      "epoch 185 global step 20: train batch loss 0.45126113295555115, test loss 18.344594955444336 \n",
      "epoch 185 global step 30: train batch loss 0.4372403919696808, test loss 18.41216278076172 \n",
      "\t epoch 185 end: test loss 18.46283721923828 \n",
      "\t epoch 185 end: precision 0.6365105008077544 \n",
      "\t epoch 185 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 185!\n",
      "epoch 186 global step 10: train batch loss 0.5326582789421082, test loss 18.54729652404785 \n",
      "epoch 186 global step 20: train batch loss 0.4512574374675751, test loss 18.344594955444336 \n",
      "epoch 186 global step 30: train batch loss 0.4372418522834778, test loss 18.41216278076172 \n",
      "\t epoch 186 end: test loss 18.46283721923828 \n",
      "\t epoch 186 end: precision 0.6365105008077544 \n",
      "\t epoch 186 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 186!\n",
      "epoch 187 global step 10: train batch loss 0.5326561331748962, test loss 18.54729652404785 \n",
      "epoch 187 global step 20: train batch loss 0.4512537717819214, test loss 18.344594955444336 \n",
      "epoch 187 global step 30: train batch loss 0.43724325299263, test loss 18.41216278076172 \n",
      "\t epoch 187 end: test loss 18.46283721923828 \n",
      "\t epoch 187 end: precision 0.6365105008077544 \n",
      "\t epoch 187 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 187!\n",
      "epoch 188 global step 10: train batch loss 0.5326541662216187, test loss 18.54729652404785 \n",
      "epoch 188 global step 20: train batch loss 0.4512501060962677, test loss 18.344594955444336 \n",
      "epoch 188 global step 30: train batch loss 0.4372447729110718, test loss 18.41216278076172 \n",
      "\t epoch 188 end: test loss 18.46283721923828 \n",
      "\t epoch 188 end: precision 0.6365105008077544 \n",
      "\t epoch 188 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 188!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 189 global step 10: train batch loss 0.5326519012451172, test loss 18.54729652404785 \n",
      "epoch 189 global step 20: train batch loss 0.45124632120132446, test loss 18.344594955444336 \n",
      "epoch 189 global step 30: train batch loss 0.4372458755970001, test loss 18.41216278076172 \n",
      "\t epoch 189 end: test loss 18.46283721923828 \n",
      "\t epoch 189 end: precision 0.6365105008077544 \n",
      "\t epoch 189 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 189!\n",
      "epoch 190 global step 10: train batch loss 0.5326499938964844, test loss 18.54729652404785 \n",
      "epoch 190 global step 20: train batch loss 0.451242595911026, test loss 18.344594955444336 \n",
      "epoch 190 global step 30: train batch loss 0.4372476041316986, test loss 18.41216278076172 \n",
      "\t epoch 190 end: test loss 18.46283721923828 \n",
      "\t epoch 190 end: precision 0.6365105008077544 \n",
      "\t epoch 190 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 190!\n",
      "epoch 191 global step 10: train batch loss 0.5326481461524963, test loss 18.54729652404785 \n",
      "epoch 191 global step 20: train batch loss 0.4512389302253723, test loss 18.344594955444336 \n",
      "epoch 191 global step 30: train batch loss 0.43724873661994934, test loss 18.41216278076172 \n",
      "\t epoch 191 end: test loss 18.46283721923828 \n",
      "\t epoch 191 end: precision 0.6365105008077544 \n",
      "\t epoch 191 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 191!\n",
      "epoch 192 global step 10: train batch loss 0.5326459407806396, test loss 18.54729652404785 \n",
      "epoch 192 global step 20: train batch loss 0.4512351453304291, test loss 18.344594955444336 \n",
      "epoch 192 global step 30: train batch loss 0.43725013732910156, test loss 18.41216278076172 \n",
      "\t epoch 192 end: test loss 18.46283721923828 \n",
      "\t epoch 192 end: precision 0.6365105008077544 \n",
      "\t epoch 192 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 192!\n",
      "epoch 193 global step 10: train batch loss 0.5326439738273621, test loss 18.54729652404785 \n",
      "epoch 193 global step 20: train batch loss 0.4512315094470978, test loss 18.344594955444336 \n",
      "epoch 193 global step 30: train batch loss 0.4372512698173523, test loss 18.41216278076172 \n",
      "\t epoch 193 end: test loss 18.46283721923828 \n",
      "\t epoch 193 end: precision 0.6365105008077544 \n",
      "\t epoch 193 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 193!\n",
      "epoch 194 global step 10: train batch loss 0.5326418876647949, test loss 18.54729652404785 \n",
      "epoch 194 global step 20: train batch loss 0.45122772455215454, test loss 18.344594955444336 \n",
      "epoch 194 global step 30: train batch loss 0.4372527301311493, test loss 18.41216278076172 \n",
      "\t epoch 194 end: test loss 18.46283721923828 \n",
      "\t epoch 194 end: precision 0.6365105008077544 \n",
      "\t epoch 194 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 194!\n",
      "epoch 195 global step 10: train batch loss 0.5326396822929382, test loss 18.54729652404785 \n",
      "epoch 195 global step 20: train batch loss 0.45122411847114563, test loss 18.344594955444336 \n",
      "epoch 195 global step 30: train batch loss 0.43725404143333435, test loss 18.41216278076172 \n",
      "\t epoch 195 end: test loss 18.46283721923828 \n",
      "\t epoch 195 end: precision 0.6365105008077544 \n",
      "\t epoch 195 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 195!\n",
      "epoch 196 global step 10: train batch loss 0.5326377749443054, test loss 18.54729652404785 \n",
      "epoch 196 global step 20: train batch loss 0.45122048258781433, test loss 18.344594955444336 \n",
      "epoch 196 global step 30: train batch loss 0.4372550845146179, test loss 18.41216278076172 \n",
      "\t epoch 196 end: test loss 18.46283721923828 \n",
      "\t epoch 196 end: precision 0.6365105008077544 \n",
      "\t epoch 196 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 196!\n",
      "epoch 197 global step 10: train batch loss 0.5326358079910278, test loss 18.54729652404785 \n",
      "epoch 197 global step 20: train batch loss 0.45121678709983826, test loss 18.344594955444336 \n",
      "epoch 197 global step 30: train batch loss 0.4372563660144806, test loss 18.41216278076172 \n",
      "\t epoch 197 end: test loss 18.46283721923828 \n",
      "\t epoch 197 end: precision 0.6365105008077544 \n",
      "\t epoch 197 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 197!\n",
      "epoch 198 global step 10: train batch loss 0.5326334238052368, test loss 18.54729652404785 \n",
      "epoch 198 global step 20: train batch loss 0.4512130320072174, test loss 18.344594955444336 \n",
      "epoch 198 global step 30: train batch loss 0.4372575879096985, test loss 18.429054260253906 \n",
      "\t epoch 198 end: test loss 18.46283721923828 \n",
      "\t epoch 198 end: precision 0.6365105008077544 \n",
      "\t epoch 198 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 198!\n",
      "epoch 199 global step 10: train batch loss 0.5326315760612488, test loss 18.54729652404785 \n",
      "epoch 199 global step 20: train batch loss 0.4512094557285309, test loss 18.344594955444336 \n",
      "epoch 199 global step 30: train batch loss 0.4372585713863373, test loss 18.429054260253906 \n",
      "\t epoch 199 end: test loss 18.46283721923828 \n",
      "\t epoch 199 end: precision 0.6365105008077544 \n",
      "\t epoch 199 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 199!\n",
      "epoch 200 global step 10: train batch loss 0.532629668712616, test loss 18.54729652404785 \n",
      "epoch 200 global step 20: train batch loss 0.45120564103126526, test loss 18.344594955444336 \n",
      "epoch 200 global step 30: train batch loss 0.43725982308387756, test loss 18.429054260253906 \n",
      "\t epoch 200 end: test loss 18.46283721923828 \n",
      "\t epoch 200 end: precision 0.6365105008077544 \n",
      "\t epoch 200 end: recall    0.73875 \n",
      "\t Checkpoint saved after epoch 200!\n"
     ]
    }
   ],
   "source": [
    "# mnet = MNet(out_size=trainlabels.shape[1])\n",
    "# checkpoint = torch.load('/home/huangz78/mri/checkpoints/mnet.pth')\n",
    "# mnet.load_state_dict(checkpoint['model_state_dict'])\n",
    "# print('mnet loaded successfully from : ' + '/home/huangz78/mri/checkpoints/mnet.pth' )\n",
    "# mnet.train()\n",
    "# # print(mnet)\n",
    "trainMNet(traindata,trainlabels, valdata,vallabels,\n",
    "          epochs=60, batchsize=5, \\\n",
    "          lr=1e-3, lr_weight_decay=0,opt_momentum=0,positive_weight=3,\\\n",
    "          lr_s_stepsize=2,lr_s_gamma=0.5,\\\n",
    "          threshold=.5, beta=1,save_cp=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
