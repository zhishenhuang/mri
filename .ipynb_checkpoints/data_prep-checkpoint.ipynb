{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from matplotlib import image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.fft as F\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import gurobipy\n",
    "import h5py\n",
    "from importlib import reload\n",
    "from torch.nn.functional import relu\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Func\n",
    "import torch.optim as optim\n",
    "import utils\n",
    "from utils import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dir  = '/mnt/shared_a/data/fastMRI/knee_singlecoil_val.npz'\n",
    "test_dir = '/mnt/shared_a/data/fastMRI/knee_singlecoil_test.npz'\n",
    "valdata  = np.load(val_dir)['data']\n",
    "testdata = np.load(test_dir)['data']\n",
    "print(valdata.shape)\n",
    "print(testdata.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train test split prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir1 = '/home/huangz78/data/data_gt.npz'\n",
    "data1 = np.load(data_dir1)\n",
    "print('file1',data1.files)\n",
    "print(data1['imgdata'].shape)\n",
    "data_dir2 = '/mnt/shared_b/data/fastMRI/singlecoil_train/expanded_gt.npz'\n",
    "data2 = np.load(data_dir2)\n",
    "print('file2',data2.files)\n",
    "print(data2['imgdata'].shape)\n",
    "\n",
    "# data = np.concatenate((data1['imgdata'],data2['imgdata']),axis=2)\n",
    "data = np.concatenate((data2['imgdata'],data1['imgdata']),axis=2)\n",
    "del data1\n",
    "del data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "imgNum = 199+1014\n",
    "traininds, testinds = train_test_split(np.arange(imgNum),random_state=0,shuffle=True,train_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traininds = np.arange(0,1014,1)\n",
    "testinds  = np.arange(1014,199+1014,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainimgs = data['imgdata'][:,:,traininds]\n",
    "# testimgs = data['imgdata'][:,:,testinds]\n",
    "dtyp = torch.float\n",
    "Dtyp = torch.cfloat\n",
    "trainimgs = data[:,:,traininds]\n",
    "testimgs  = data[:,:,testinds]\n",
    "train_y = torch.zeros((trainimgs.shape[2],trainimgs.shape[0],trainimgs.shape[1]),dtype=Dtyp)\n",
    "train_yfull = torch.zeros((trainimgs.shape[2],trainimgs.shape[0],trainimgs.shape[1]),dtype=Dtyp)\n",
    "train_x = torch.zeros((trainimgs.shape[2],trainimgs.shape[0],trainimgs.shape[1]),dtype=dtyp)\n",
    "train_xfull = torch.zeros((trainimgs.shape[2],trainimgs.shape[0],trainimgs.shape[1]),dtype=dtyp)\n",
    "\n",
    "test_y  = torch.zeros((testimgs.shape[2],testimgs.shape[0],testimgs.shape[1]),dtype=Dtyp)\n",
    "test_yfull  = torch.zeros((testimgs.shape[2],testimgs.shape[0],testimgs.shape[1]),dtype=Dtyp)\n",
    "test_x  = torch.zeros((testimgs.shape[2],testimgs.shape[0],testimgs.shape[1]),dtype=dtyp)\n",
    "test_xfull  = torch.zeros((testimgs.shape[2],testimgs.shape[0],testimgs.shape[1]),dtype=dtyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load an image and make it into correct shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'file1000605.h5'\n",
    "filename = 'file1000568.h5'\n",
    "f = h5py.File(filename, 'r')\n",
    "print(f.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for ind in range(f['reconstruction_esc'].shape[0]):\n",
    "    plt.imshow(f['reconstruction_esc'].value[ind,:,:],origin='lower')\n",
    "    plt.title(f'{filename}, frame {ind}')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199\n"
     ]
    }
   ],
   "source": [
    "# prepare document list with all files end with .h5\n",
    "# fastMRI_path = '/mnt/shared_b/data/fastMRI/singlecoil_train'\n",
    "fastMRI_path = '/mnt/shared_b/data/fastMRI/singlecoil_val/'\n",
    "# fastMRI_path = '/mnt/shared_b/data/fastMRI/singlecoil_test_v2/' # ALL files in this directory is broken!\n",
    "os.chdir(fastMRI_path)\n",
    "doculist = list([])\n",
    "for file in os.listdir(fastMRI_path):\n",
    "    if (not file.startswith('.')) and (file.endswith('.h5')):\n",
    "#     if (not file.startswith('.')) and (not file.endswith('.npz')) and (not file.endswith('.txt')):\n",
    "        doculist.append(file)\n",
    "print(len(doculist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "13*973"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for file in doculist:\n",
    "    try:\n",
    "        f = h5py.File(file,'r')\n",
    "        print('filename: ', file, '\\t', 'Number of images: ', f['reconstruction_esc'].shape[0])\n",
    "        f.close()\n",
    "    except:\n",
    "        print(file,'failed to be opened')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what volumes should we pick? 'diff' below means the number of volumes we should select\n",
    "diff = 6\n",
    "indset = np.random.choice(range(len(doculist)),size=20,replace=False)\n",
    "for ind in indset:\n",
    "    try:\n",
    "        filename = doculist[ind]\n",
    "        f = h5py.File(filename, 'r')\n",
    "        total_frames = f['reconstruction_esc'].shape[0]\n",
    "        print(filename,'number of frames: ', total_frames)\n",
    "        \n",
    "        im = torch.tensor(f['reconstruction_esc'][total_frames//2-diff,:,:])\n",
    "        plt.title(filename+'  slice: '+str(total_frames//2-diff))\n",
    "        plt.imshow(im,origin='lower')\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        \n",
    "        im = torch.tensor(f['reconstruction_esc'][total_frames//2+diff,:,:])\n",
    "        plt.title(filename+'  slice: '+str(total_frames//2+diff))\n",
    "        plt.imshow(im,origin='lower')\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        \n",
    "        f.close()\n",
    "    except:\n",
    "#         doculist.remove(filename)\n",
    "        print('failed to open the file: ',filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file1000552.h5 36\n",
      "file1002007.h5 38\n",
      "file1002002.h5 35\n",
      "file1002412.h5 34\n",
      "file1002067.h5 37\n",
      "file1001759.h5 40\n",
      "file1001959.h5 36\n",
      "file1001533.h5 35\n",
      "file1002382.h5 40\n",
      "file1001703.h5 36\n",
      "file1000758.h5 30\n",
      "file1001850.h5 38\n",
      "file1001365.h5 35\n",
      "file1001143.h5 33\n",
      "file1001916.h5 35\n",
      "file1002417.h5 36\n",
      "file1001643.h5 42\n",
      "file1001687.h5 40\n",
      "file1001184.h5 37\n",
      "file1002280.h5 34\n",
      "file1001122.h5 36\n",
      "file1001650.h5 35\n",
      "file1000017.h5 34\n",
      "file1000628.h5 36\n",
      "file1001480.h5 30\n",
      "file1002274.h5 42\n",
      "file1001825.h5 32\n",
      "file1001170.h5 38\n",
      "file1001059.h5 30\n",
      "file1000769.h5 40\n",
      "file1001096.h5 34\n",
      "file1000555.h5 30\n",
      "file1001057.h5 37\n",
      "file1000273.h5 35\n",
      "file1001188.h5 34\n",
      "file1000631.h5 40\n",
      "file1001429.h5 42\n",
      "file1000323.h5 34\n",
      "file1000735.h5 38\n",
      "file1001289.h5 40\n",
      "file1001331.h5 36\n",
      "file1000182.h5 38\n",
      "file1001168.h5 30\n",
      "file1002389.h5 39\n",
      "file1000759.h5 41\n",
      "file1000926.h5 37\n",
      "file1000591.h5 38\n",
      "file1001275.h5 35\n",
      "file1001598.h5 30\n",
      "file1001163.h5 40\n",
      "file1000496.h5 32\n",
      "file1000280.h5 33\n",
      "file1001450.h5 34\n",
      "file1000817.h5 35\n",
      "file1001930.h5 40\n",
      "file1000976.h5 40\n",
      "file1000041.h5 38\n",
      "file1000201.h5 40\n",
      "file1001381.h5 37\n",
      "file1001984.h5 32\n",
      "file1002145.h5 38\n",
      "file1000990.h5 40\n",
      "file1001834.h5 40\n",
      "file1001140.h5 40\n",
      "file1000480.h5 33\n",
      "file1002538.h5 34\n",
      "file1000277.h5 42\n",
      "file1000748.h5 32\n",
      "file1001798.h5 35\n",
      "file1001221.h5 36\n",
      "file1000858.h5 33\n",
      "file1000108.h5 37\n",
      "file1001655.h5 37\n",
      "file1001159.h5 30\n",
      "file1000810.h5 36\n",
      "file1000350.h5 32\n",
      "file1000871.h5 34\n",
      "file1000254.h5 32\n",
      "file1001497.h5 37\n",
      "file1000264.h5 36\n",
      "file1000842.h5 38\n",
      "file1000942.h5 35\n",
      "file1001339.h5 33\n",
      "file1000196.h5 36\n",
      "file1000243.h5 33\n",
      "file1000903.h5 33\n",
      "file1002436.h5 37\n",
      "file1001818.h5 34\n",
      "file1000190.h5 36\n",
      "file1000267.h5 35\n",
      "file1000263.h5 35\n",
      "file1000328.h5 34\n",
      "file1000314.h5 32\n",
      "file1001668.h5 35\n",
      "file1001715.h5 33\n",
      "file1001219.h5 33\n",
      "file1000528.h5 38\n",
      "file1001862.h5 37\n",
      "file1000538.h5 40\n",
      "file1000537.h5 35\n",
      "1300 1300\n"
     ]
    }
   ],
   "source": [
    "# create training/validation/testing dataset\n",
    "diff = 6\n",
    "fileinds = np.random.permutation(len(doculist))\n",
    "doculist_val  = [doculist[i] for i in fileinds[0:99]] # validation\n",
    "doculist_test = [doculist[i] for i in fileinds[99:]]  # test\n",
    "# doculist_current = doculist[:99] # validation\n",
    "# doculist_current = doculist[99:] # test\n",
    "doculist_current = doculist_test\n",
    "imgdata = np.zeros((len(doculist_current)*(2*diff+1),320,320))\n",
    "imgind = 0\n",
    "for filename in doculist_current:\n",
    "    f = h5py.File(filename, 'r')    \n",
    "    fsize = f['reconstruction_esc'].shape[0]\n",
    "    print(filename, fsize)\n",
    "    for ind in np.arange(-diff,diff+1,1):\n",
    "        imgdata[imgind,:,:] = torch.tensor(f['reconstruction_esc'][fsize//2+ind,:,:])\n",
    "        imgind += 1\n",
    "print(len(doculist_current)*(2*diff+1),imgind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save training dataset\n",
    "# filename = '/mnt/shared_a/data/fastMRI/knee_singlecoil_train.npz'\n",
    "# filename = '/mnt/shared_a/fastMRI/knee_singlecoil_val_2.npz'\n",
    "filename = '/mnt/shared_a/fastMRI/knee_singlecoil_test_2.npz'\n",
    "np.savez(filename,data=imgdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = torch.tensor(f['reconstruction_rss'][22,:,:])\n",
    "plt.imshow(im,cmap='gray')\n",
    "plt.colorbar()\n",
    "y = F.fftn(im,dim=(0,1),norm='ortho')\n",
    "f.close()\n",
    "\n",
    "\n",
    "imgHeg = y.shape[0]\n",
    "imgWid = y.shape[1]\n",
    "if len(y.shape)<3:\n",
    "     y = y.view((y.shape[0],y.shape[1],1)) \n",
    "x_star  = im.view(imgHeg,imgWid,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### load Siddhant's greedy mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load greedy mask provided by Siddhant\n",
    "file1 = np.load('/mnt/shared_b/gautamsi/mri-sampling/simulation-results/greedy_fastmri_mp50.npz')\n",
    "file2 = np.load('/mnt/shared_b/gautamsi/mri-sampling/simulation-results/greedy_fastmri_mp100.npz')\n",
    "file3 = np.load('/mnt/shared_b/gautamsi/mri-sampling/simulation-results/greedy_fastmri_mp150.npz')\n",
    "file4 = np.load('/mnt/shared_b/gautamsi/mri-sampling/simulation-results/greedy_fastmri_mp198.npz')\n",
    "file5 = np.load('/mnt/shared_b/gautamsi/mri-sampling/simulation-results/greedy_fastmri_mp199.npz')\n",
    "mask1 = file1['arr_0']; mask2 = file2['arr_0']; mask3 = file3['arr_0']; mask4 = file4['arr_0']; mask5 = file5['arr_0']\n",
    "plt.imshow(mask1[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = [mask1,mask2,mask3,mask4,mask5]\n",
    "mask_label = np.zeros((320,199))\n",
    "ind = 0\n",
    "for maskfile in masks:\n",
    "    masknum = maskfile.shape[2]\n",
    "    for i in range(masknum):\n",
    "        mask_label[:,i+ind] = maskfile[0,:,i]\n",
    "    ind += masknum\n",
    "\n",
    "np.savez('/home/huangz78/data/data_gt_greedymask.npz',mask=mask_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At what sampling ratio does random sampling overtake low frequency sampling ?\n",
    "    - Conclusion : 30% base, 10% additional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At what sampling ratio does random sampling overtake low frequency sampling ?\n",
    "# Conclusion : 30% base, 10% additional\n",
    "# lamda = 5e-7\n",
    "np.random.seed(0)\n",
    "roll_flag = True\n",
    "mps = np.ones((1,imgHeg,imgWid))\n",
    "# x_recon = np.fft.fftshift(np.real(TotalVariationRecon(ksp, mps, lamda, weights=mask).run()))\n",
    "# x_recon = np.fft.fftshift(np.real(L1WaveletRecon(ksp, mps, lamda, weights=mask).run()))\n",
    "# l1wavelet: -9,-8.5; tv: -6.5,-6.3\n",
    "lamda = 10**(-6.31)\n",
    "\n",
    "base_r_grid = np.linspace(.25,.35,10)\n",
    "rand_r = 0.1\n",
    "error_rand = np.zeros(base_r_grid.size); error_freq = np.zeros(base_r_grid.size)\n",
    "\n",
    "Rep = 10\n",
    "\n",
    "ind = 0\n",
    "for base_r in base_r_grid:\n",
    "#     base_r = 0.3; \n",
    "    total_r   = base_r + rand_r    \n",
    "    mask_freq,_,_ = mask_naiveRand(imgHeg,fix=imgHeg*total_r,other=0,roll=roll_flag)\n",
    "    mask_freq = mask_freq.numpy()\n",
    "    y_freq = np.reshape(np.diag(mask_freq)@yraw,(-1,imgHeg,imgWid)) \n",
    "    \n",
    "    rep = 0\n",
    "    while rep < Rep:\n",
    "        mask_rand = mask_prob(img,fix=imgHeg*base_r,other=imgHeg*rand_r,roll=roll_flag,seed=int(time.strftime('%S')))\n",
    "        y_rand = np.reshape(np.diag(mask_rand)@yraw,(-1,imgHeg,imgWid)) \n",
    "        x_recon_rand = np.fft.fftshift( np.real(TotalVariationRecon(y_rand,mps,lamda,show_pbar=False,max_iter=50).run()) )\n",
    "        error_rand[ind] += np.sqrt( np.sum((x_recon_rand.flatten()-img.flatten())**2) )/np.sqrt( np.sum( (img.flatten())**2 ))\n",
    "        rep += 1\n",
    "    error_rand[ind] /= Rep\n",
    "    \n",
    "    x_recon_freq = np.fft.fftshift( np.real(TotalVariationRecon(y_freq,mps,lamda,show_pbar=False,max_iter=50).run()) )\n",
    "    error_freq[ind] = np.sqrt( np.sum((x_recon_freq.flatten()-img.flatten())**2) )/np.sqrt( np.sum( (img.flatten())**2 ))\n",
    "    # error_rand = np.mean(np.abs(x_recon_rand.flatten()-img.flatten())) \n",
    "    # error_freq = np.mean(np.abs(x_recon_freq.flatten()-img.flatten())) \n",
    "\n",
    "    print('rand.     mask recon. error = ' , error_rand[ind])\n",
    "    print('low.freq. mask recon. error = ' , error_freq[ind])\n",
    "    ind += 1\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(base_r_grid,error_rand,label='rand')\n",
    "plt.plot(base_r_grid,error_freq,label='freq')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sanity test\n",
    "  - shepp-logan phantom\n",
    "  - brain img from class material\n",
    "  - the following cells are mostly loading imgs into notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastMRI_path = '/Users/leonardohuang/Desktop/msu_research/code/data/singlecoil_val/'\n",
    "# sys.path.append(fastMRI_path)\n",
    "imgHeg   = 320\n",
    "imgWid   = 320\n",
    "DType    = torch.cfloat\n",
    "\n",
    "os.chdir(fastMRI_path)\n",
    "filename = 'file1001557.h5'\n",
    "f = h5py.File(filename, 'r')\n",
    "print(f.keys())\n",
    "\n",
    "im = torch.tensor(f['reconstruction_rss'][22,:,:])\n",
    "plt.imshow(im,cmap='gray')\n",
    "plt.colorbar()\n",
    "y = F.fftn(im,dim=(0,1),norm='ortho')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shepp-logan phantom image loading\n",
    "im = image.imread('/Users/leonardohuang/Desktop/msu_research/code/data/phantom.gif')\n",
    "im = torch.tensor(im[:,:,0]).to(torch.float)\n",
    "imgHeg = im.shape[0]\n",
    "imgWid = im.shape[1]\n",
    "y = F.fftn(im,dim=(0,1),norm='ortho')\n",
    "DType  = torch.cfloat\n",
    "plt.clf()\n",
    "plt.imshow(im)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_sp(x):\n",
    "    maxval = np.max(np.abs(x))\n",
    "    minval = np.min(np.abs(x))\n",
    "    K = 255./(maxval-minval)\n",
    "    B = - (minval*255.)/(maxval-minval)\n",
    "    x = K*x+B\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare useful FastMRI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgdata = np.load('/home/huangz78/data/imgdata.npz')\n",
    "data = imgdata['imgdata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = .3\n",
    "addi = .1\n",
    "imgHeg = 320\n",
    "labels = np.zeros((199,int(imgHeg*(1-base))))\n",
    "suby   = np.zeros((199,int(imgHeg*base),320,2))\n",
    "ifftimgs  = np.zeros((199,320,320))\n",
    "\n",
    "coreInds = np.arange(int(imgHeg/2)-int(imgHeg*base/2), int(imgHeg/2)+int(imgHeg*base/2))\n",
    "mask_low,_,_ = mask_naiveRand(imgHeg,fix=int(imgHeg*base),other=0,roll=True)\n",
    "mask_low = mask_low.numpy()\n",
    "\n",
    "np.random.seed(2021)\n",
    "for ind in range(199):\n",
    "    img = data[:,:,ind]\n",
    "    fullmask = mask_prob(img,fix=imgHeg*base,other=imgHeg*addi,roll=True)\n",
    "    labels[ind,:] = fullmask[np.setdiff1d(np.arange(imgHeg),coreInds)] # labels for high freq\n",
    "    \n",
    "    yraw = np.fft.fftshift(np.fft.fftn(img,norm='ortho'))\n",
    "    y = np.diag(mask_low)@yraw # subsampled y\n",
    "    ifftimgs[ind,:,:] = np.abs(np.fft.ifftn(np.fft.fftshift(y),norm='ortho')) # ifft imgs\n",
    "    suby[ind,:,:,0] = np.real(y[coreInds,:]) # subsampled y real\n",
    "    suby[ind,:,:,1] = np.imag(y[coreInds,:]) # subsampled y imag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/home/huangz78/data/datafornn.npz'\n",
    "np.savez(filepath,labels=labels,sub_y=suby,ifftimgs=ifftimgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img = data[:,:,3]\n",
    "fullmask = mask_prob(img,fix=imgHeg*base,other=imgHeg*addi,roll=True)\n",
    "labels[ind,:] = fullmask[np.setdiff1d(np.arange(imgHeg),coreInds)]\n",
    "\n",
    "mask_low,_,_ = mask_naiveRand(imgHeg,fix=int(imgHeg*base),other=0,roll=True)\n",
    "mask_low = mask_low.numpy()\n",
    "yraw = np.fft.fftshift(np.fft.fftn(img,norm='ortho'))\n",
    "y = np.diag(mask_low)@yraw\n",
    "xifft = np.abs(np.fft.ifftn(np.fft.fftshift(y),norm='ortho'))\n",
    "\n",
    "plt.figure(figsize=(5,10))\n",
    "plt.subplot(311)\n",
    "plt.title('orig img')\n",
    "plt.imshow(img)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.title('naive masked y')\n",
    "plt.imshow(np.log(np.abs(y)))\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.title('ifft img')\n",
    "plt.imshow(xifft)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "kplot(fullmask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### manually select images by printing all images for view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastMRI_path = '/Users/leonardohuang/Desktop/msu_research/code/data/singlecoil_val/'\n",
    "# sys.path.append(fastMRI_path)\n",
    "imgHeg   = 320\n",
    "imgWid   = 320\n",
    "DType    = torch.cfloat\n",
    "\n",
    "doculist = list([])\n",
    "for file in os.listdir(fastMRI_path):\n",
    "    if not file.startswith('.'):\n",
    "        doculist.append(file)\n",
    "# docutrain,docutest = train_test_split(doculist,train_size=int(len(doculist)*.8), random_state=1024)\n",
    "# print('Number of image documents for training: ', len(docutrain))\n",
    "# print('Number of image documents for testing : ', len(docutest) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "mainInd = int(26)\n",
    "fileind = 0\n",
    "for filename in doculist:\n",
    "    print(\"image {} out of {}\".format(fileind+1,len(doculist)))\n",
    "    f  = h5py.File(filename,'r')    \n",
    "    fileNum = f['reconstruction_rss'].shape[0]\n",
    "    if mainInd < fileNum:\n",
    "        plt.clf()\n",
    "        plt.imshow(f['reconstruction_rss'][mainInd,:,:])\n",
    "        plt.title('{0}: slice {1}'.format(filename,mainInd))\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        indicator = input()\n",
    "        goodind = mainInd\n",
    "        if int(indicator) != 1:\n",
    "            for ind in range(18,min(32,fileNum),1):\n",
    "                plt.clf()\n",
    "                plt.imshow(f['reconstruction_rss'][ind,:,:])\n",
    "                plt.title('{0}: slice {1}'.format(filename,ind))\n",
    "                plt.colorbar()\n",
    "                plt.show()\n",
    "            goodind = input()\n",
    "        data[filename] = int(goodind)\n",
    "    else:\n",
    "        print(filename,\" fails to load \", mainInd, \" slice!!!\")\n",
    "    f.close()\n",
    "    fileind += 1\n",
    "\n",
    "#     for pic in range(fileNum):   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(list(database.keys())[51],'r')\n",
    "for ind in range(f['reconstruction_rss'].shape[0]):\n",
    "                plt.clf()\n",
    "                plt.imshow(f['reconstruction_rss'][ind,:,:])\n",
    "                plt.title('{0}: slice {1}'.format(filename,ind))\n",
    "                plt.colorbar()\n",
    "                plt.show()\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"dict.txt\",\"w\")\n",
    "f.write( str(database) )\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('imgdata',imgdata=imgdata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
