{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f9b85e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.fft as F\n",
    "from importlib import reload\n",
    "from torch.nn.functional import relu\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Func\n",
    "import torch.optim as optim\n",
    "import utils\n",
    "import mnet\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from utils import kplot,mask_naiveRand,mask_filter\n",
    "from mnet import MNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40c6b223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 2, 320, 320])\n",
      "torch.Size([199, 320, 320])\n"
     ]
    }
   ],
   "source": [
    "imgs = torch.tensor( np.load('/home/huangz78/data/data_gt.npz')['imgdata'] ).permute(2,0,1)\n",
    "base = 24\n",
    "mask_lf,_,_ = mask_naiveRand(imgs.shape[1],fix=base,other=0,roll=True)\n",
    "\n",
    "yfulls = torch.zeros((imgs.shape[0],2,imgs.shape[1],imgs.shape[2]),dtype=torch.float)\n",
    "ys     = torch.zeros((imgs.shape[0],2,imgs.shape[1],imgs.shape[2]),dtype=torch.float)\n",
    "xs     = torch.zeros(imgs.shape)\n",
    "for ind in range(imgs.shape[0]):\n",
    "    imgs[ind,:,:] = imgs[ind,:,:]/torch.max(torch.abs(imgs[ind,:,:]))\n",
    "    y = torch.fft.fftshift(F.fftn(imgs[ind,:,:],dim=(0,1),norm='ortho'))\n",
    "    ysub = torch.zeros(y.shape,dtype=y.dtype)\n",
    "    ysub[mask_lf==1,:] = y[mask_lf==1,:]\n",
    "    xs[ind,:,:] = torch.abs(F.ifftn(torch.fft.ifftshift(ysub),dim=(0,1),norm='ortho')) \n",
    "    \n",
    "    yfulls[ind,0,:,:] = torch.real(y)\n",
    "    yfulls[ind,1,:,:] = torch.imag(y)\n",
    "    ys[ind,:,mask_lf==1,:] = yfulls[ind,:,mask_lf==1,:]\n",
    "      \n",
    "labels = torch.tensor( np.load('/home/huangz78/data/data_gt_greedymask.npz')['mask'].T ) \n",
    "print(ys.shape)\n",
    "print(xs.shape)\n",
    "# labels are already rolled\n",
    "\n",
    "imgNum = imgs.shape[0]\n",
    "traininds, testinds = train_test_split(np.arange(imgNum),random_state=0,shuffle=True,train_size=round(imgNum*0.8))\n",
    "test_total  = testinds.size\n",
    "\n",
    "traindata   = ys[traininds,:,:,:]\n",
    "valdata     = ys[testinds[0:test_total//2],:,:,:]\n",
    "testdata    = ys[testinds[test_total//2:],:,:,:]\n",
    "\n",
    "trainlabels = mask_filter(labels[traininds,:],base=base)\n",
    "vallabels   = mask_filter(labels[testinds[0:test_total//2],:],base=base)\n",
    "testlabels  = mask_filter(labels[testinds[test_total//2:],:],base=base)\n",
    "\n",
    "# traindata   = xs[traininds,:,:]\n",
    "# valdata     = xs[testinds[0:test_total//2],:,:]\n",
    "# traindata = traindata.view(traindata.shape[0],1,traindata.shape[1],traindata.shape[2])\n",
    "# valdata = valdata.view(valdata.shape[0],1,valdata.shape[1],valdata.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00887338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(xs[0,:,:])\n",
    "# plt.colorbar()\n",
    "# plt.show()\n",
    "# kplot(mask_lf)\n",
    "# kplot(xs[0,:,:],log=False,roll=False)\n",
    "kplot(torch.abs(ys[0,0,:,:]),log=True,roll=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4d3a949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_binarize(M,threshold=0.5):\n",
    "    sigmoid = nn.Sigmoid()\n",
    "    mask = sigmoid(M)\n",
    "    mask_pred = torch.ones_like(mask)\n",
    "    for ind in range(M.shape[0]):\n",
    "        mask_pred[ind,mask[ind,:]<=threshold] = 0\n",
    "    return mask_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b525632f",
   "metadata": {},
   "source": [
    "# mnet test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6179fbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mnet.mnet.MNet'; 'mnet.mnet' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f7a847114bf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMNet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mMnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m net = MNet(beta=1,in_channels=traindata.shape[1],out_size=trainlabels.shape[1],\\\n\u001b[1;32m      3\u001b[0m                    imgsize=(traindata.shape[2],traindata.shape[3]),poolk=3)\n\u001b[1;32m      4\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mnet.mnet.MNet'; 'mnet.mnet' is not a package"
     ]
    }
   ],
   "source": [
    "import mnet.mnet.MNet as Mnet\n",
    "net = MNet(beta=1,in_channels=traindata.shape[1],out_size=trainlabels.shape[1],\\\n",
    "                   imgsize=(traindata.shape[2],traindata.shape[3]),poolk=3)\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccf7c4b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnet is successfully loaded from the file: /home/huangz78/checkpoints/mnet.pth\n"
     ]
    }
   ],
   "source": [
    "dir_checkpoint = '/home/huangz78/checkpoints/'\n",
    "net = MNet(beta=1,in_channels=traindata.shape[1],out_size=trainlabels.shape[1],\\\n",
    "                   imgsize=(traindata.shape[2],traindata.shape[3]),poolk=3)\n",
    "dictpath   = '/home/huangz78/checkpoints/mnet.pth'\n",
    "checkpoint = torch.load(dictpath)\n",
    "net.load_state_dict(checkpoint['model_state_dict'])\n",
    "net.eval()\n",
    "print('mnet is successfully loaded from the file: ' + dictpath )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a269e873",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testdata shape is:  torch.Size([20, 2, 320, 320])\n",
      "tensor(2.)\n",
      "tensor([[ 0.6231,  0.6122,  0.6316,  0.6154,  0.6485,  0.5887,  0.6250,  0.6208,\n",
      "          0.6336,  0.6580,  0.6527,  0.6270,  0.6393,  0.6440,  0.6191,  0.6347,\n",
      "          0.6297,  0.6451,  0.6119,  0.6353,  0.6322,  0.4998,  0.6105,  0.5882,\n",
      "          0.5943,  0.5781,  0.5539,  0.5334,  0.6113,  0.5184,  0.5878,  0.6283,\n",
      "          0.6422,  0.6336,  0.6206,  0.6137,  0.6268,  0.6206,  0.6453,  0.6267,\n",
      "          0.6459,  0.5872,  0.5977,  0.6293,  0.6430,  0.5786,  0.6196,  0.5396,\n",
      "          0.6329,  0.6537,  0.6125,  0.5996,  0.6314,  0.5678,  0.5547,  0.5703,\n",
      "          0.6109,  0.6372,  0.6478,  0.6370,  0.5674,  0.5867,  0.5104,  0.5721,\n",
      "          0.5470,  0.6463,  0.5999,  0.5212,  0.3986,  0.6455,  0.5654,  0.5221,\n",
      "          0.5351,  0.5637,  0.5313,  0.5198,  0.5549,  0.5106,  0.7370,  0.5516,\n",
      "          0.4768,  0.5394,  0.6553,  0.6186,  0.6124,  0.5719,  0.6316,  0.6315,\n",
      "          0.5556,  0.5929,  0.6142,  0.5811,  0.5822,  0.2788,  0.2247,  0.2205,\n",
      "          0.1870,  0.4664,  0.1479,  0.6003,  0.3853, -0.7722,  0.2842,  0.4098,\n",
      "          0.1258,  0.0068, -0.7302,  0.2521,  0.0313, -0.2100, -0.1250,  0.0841,\n",
      "          0.1933, -0.1212, -0.2090, -0.5579, -0.2627, -0.3884, -0.2613, -0.1853,\n",
      "         -0.2192, -0.1478, -0.3593, -0.2505, -0.2062, -0.3120, -0.2169, -0.6176,\n",
      "         -0.4810, -0.4733, -0.5722, -0.5656, -0.6030, -0.6134, -0.5511, -0.5945,\n",
      "         -0.6115, -0.6290, -0.6293, -0.6327, -0.6485, -0.5988, -0.6336, -0.6389,\n",
      "         -0.6178, -0.6335, -0.6261, -0.6101, -0.6292, -0.6290, -0.6121, -0.6283,\n",
      "         -0.6287, -0.6324, -0.6322, -0.6421, -0.6171, -0.6435, -0.6614, -0.6294,\n",
      "         -0.6027, -0.5955, -0.5742, -0.6176, -0.5340, -0.5581, -0.5953, -0.4696,\n",
      "         -0.4674, -0.5325, -0.2269, -0.3548, -0.1636, -0.2108, -0.3138, -0.1612,\n",
      "         -0.2484, -0.2363, -0.2052, -0.4271, -0.0967, -0.5185, -0.2355, -0.1590,\n",
      "          0.1416, -0.0085, -0.2013, -0.1467, -0.1101,  0.2249, -0.7075, -0.0659,\n",
      "          0.1496,  0.3807,  0.5504,  0.1658,  0.5093,  0.6148,  0.1039,  0.5626,\n",
      "          0.6305,  0.2340,  0.2029,  0.2718,  0.5708,  0.6422,  0.5875,  0.6081,\n",
      "          0.4051,  0.6579,  0.6305,  0.5864,  0.6212,  0.6129,  0.6477,  0.5432,\n",
      "          0.5796,  0.5122,  0.6071,  0.4954,  0.5492,  0.5280,  0.5082,  0.5663,\n",
      "          0.4736,  0.5416,  0.5806,  0.6182,  0.4777,  0.5748,  0.5858,  0.6400,\n",
      "          0.5833,  0.5420,  0.4892,  0.5780,  0.5449,  0.6150,  0.6166,  0.6109,\n",
      "          0.6039,  0.5960,  0.5596,  0.5639,  0.6211,  0.6031,  0.5908,  0.6321,\n",
      "          0.6453,  0.5360,  0.6488,  0.5577,  0.6720,  0.6493,  0.6385,  0.5893,\n",
      "          0.6269,  0.6121,  0.6573,  0.6196,  0.5977,  0.6512,  0.6206,  0.6371,\n",
      "          0.6234,  0.6201,  0.6381,  0.6271,  0.6200,  0.6188,  0.5682,  0.5532,\n",
      "          0.6215,  0.5696,  0.5855,  0.5556,  0.6506,  0.6141,  0.6105,  0.6092,\n",
      "          0.6365,  0.6524,  0.6418,  0.6451,  0.6259,  0.6175,  0.6596,  0.6322,\n",
      "          0.6202,  0.5559,  0.6369,  0.5644,  0.6156,  0.6181,  0.6498,  0.6246]],\n",
      "       grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "print('testdata shape is: ', testdata.shape)\n",
    "\n",
    "imgind = 18\n",
    "testimg = testdata[imgind,:,:,:]\n",
    "output_1 = net(testimg.view(-1,2,320,320))\n",
    "binary_1 = sigmoid_binarize(output_1)[0,:]\n",
    "\n",
    "imgind = 10\n",
    "testimg = testdata[imgind,:,:,:]\n",
    "output_2 = net(testimg.view(-1,2,320,320))\n",
    "binary_2 = sigmoid_binarize(output_2)[0,:]\n",
    "\n",
    "print(torch.sum(torch.abs(binary_1-binary_2)))\n",
    "print(output_1 - output_2)\n",
    "# sigmoid_binarize(output)[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc84399c",
   "metadata": {},
   "source": [
    "# mnet train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c966fd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainMNet(trainimgs,trainlabels,testimgs,testlabels,\\\n",
    "              epochs=20,batchsize=5,positive_weight=1,\\\n",
    "              lr=0.01,lr_weight_decay=1e-8,opt_momentum=0,\\\n",
    "              lr_s_stepsize=5,lr_s_gamma=0.5,\\\n",
    "              model=None,save_cp=True,threshold=0.5,\\\n",
    "              beta=1,poolk=3,datatype=torch.float,print_every=10):\n",
    "    '''\n",
    "    trainimgs    : train data, with dimension (num. of imgs,layer, height, width)\n",
    "    '''\n",
    "    \n",
    "    train_shape  = trainimgs.shape; test_shape = testimgs.shape \n",
    "    trainimgs    = torch.tensor(trainimgs,dtype=datatype)\n",
    "    trainlabels  = torch.tensor(trainlabels,dtype=datatype)\n",
    "    testimgs     = torch.tensor(testimgs,dtype=datatype)\n",
    "    testlabels   = torch.tensor(testlabels,dtype=datatype)    \n",
    "    dir_checkpoint = '/home/huangz78/checkpoints/'\n",
    "    # input images are assumed to be normalized\n",
    "    \n",
    "    if model is None:\n",
    "        net = MNet(beta=beta,in_channels=train_shape[1],out_size=trainlabels.shape[1],\\\n",
    "                   imgsize=(train_shape[2],train_shape[3]),poolk=poolk)\n",
    "    else:\n",
    "        net = model\n",
    "#     optimizer = optim.RMSprop(net.parameters(), lr=lr, weight_decay=lr_weight_decay, momentum=opt_momentum)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=lr_weight_decay, amsgrad=False)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=lr_s_stepsize,factor=lr_s_gamma)\n",
    "    pos_weight = torch.ones([trainlabels.shape[1]]) * positive_weight # weight assigned to positive labels \n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    test_criterion = nn.BCELoss()\n",
    "    \n",
    "    epoch_loss        = np.full((epochs),np.nan)\n",
    "    precision_train   = list([]); recall_train = list([])\n",
    "    precision_history = np.full((epochs),np.nan); recall_history = np.full((epochs),np.nan)\n",
    "    net.train()\n",
    "    for epoch in range(epochs):\n",
    "        batch_init = 0; step_count = 0\n",
    "        while batch_init < train_shape[0]:\n",
    "            batch = np.arange(batch_init,min(batch_init+batchsize,train_shape[0]))\n",
    "            imgbatch = trainimgs[batch,:,:,:] # maybe shuffling?\n",
    "            batchlabels = trainlabels[batch,:]\n",
    "            mask_pred   = net(imgbatch)\n",
    "            train_loss  = criterion(mask_pred,batchlabels)\n",
    "            batch_init += batchsize; step_count += 1\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            if (step_count%print_every)==0:\n",
    "                print('[{}/{}][{}] train batch loss {}'.format(epoch+1,epochs,step_count,train_loss.item()))\n",
    "                precision_train.append(\\\n",
    "                    precision_score(torch.flatten(batchlabels),\\\n",
    "                                    torch.flatten(sigmoid_binarize(mask_pred,threshold=threshold))) ) \n",
    "                recall_train.append(\\\n",
    "                    recall_score(torch.flatten(batchlabels),\\\n",
    "                                 torch.flatten(sigmoid_binarize(mask_pred,threshold=threshold))) )\n",
    "                print('[{}/{}][{}] precision {}, recall {}'.format(epoch+1,epochs,step_count,precision_train[-1],recall_train[-1]))\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            mask_test = sigmoid_binarize(net(testimgs),threshold=threshold)\n",
    "            test_loss = test_criterion(mask_test,testlabels)\n",
    "            net.train()\n",
    "            scheduler.step(test_loss)\n",
    "            epoch_loss[epoch] = test_loss.item()\n",
    "            precision_history[epoch] = precision_score(torch.flatten(testlabels),torch.flatten(mask_test))\n",
    "            recall_history[epoch] = recall_score(torch.flatten(testlabels),torch.flatten(mask_test))\n",
    "            print('\\t [{}/{}] test loss {} '.format(epoch+1,epochs,test_loss.item()))\n",
    "            print('\\t [{}/{}] precision {} '.format(epoch+1,epochs,precision_history[epoch]))\n",
    "            print('\\t [{}/{}] recall    {} '.format(epoch+1,epochs,recall_history[epoch]))\n",
    "        if save_cp:\n",
    "            try:\n",
    "                os.mkdir(dir_checkpoint)\n",
    "                print('Created checkpoint directory')\n",
    "            except OSError:\n",
    "                pass\n",
    "            torch.save({'model_state_dict': net.state_dict()}, dir_checkpoint + 'mnet.pth')\n",
    "#                         'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                         'epoch': epoch,\n",
    "#                         'threshold':threshold\n",
    "#                         }, dir_checkpoint + 'mnet.pth')\n",
    "#                         }, dir_checkpoint + f'CP_epoch{epoch + 1}.pth')\n",
    "            print(f'\\t Checkpoint saved after epoch {epoch + 1}!')\n",
    "            np.savez(dir_checkpoint+'epoch_loss.npz', loss=epoch_loss,\\\n",
    "                     precision_train=precision_train,recall_train=recall_train,\\\n",
    "                     precision_test=precision_history,recall_test=recall_history)\n",
    "        \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68505a97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/pyenv/lib/python3.7/site-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if sys.path[0] == '':\n",
      "/opt/anaconda/envs/pyenv/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  del sys.path[0]\n",
      "/opt/anaconda/envs/pyenv/lib/python3.7/site-packages/ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/opt/anaconda/envs/pyenv/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/150][1] train batch loss 0.6922899484634399\n",
      "[1/150][1] precision 0.29280397022332505, recall 0.49166666666666664\n",
      "\t [1/150] test loss 44.36936950683594 \n",
      "\t [1/150] precision 0.30256410256410254 \n",
      "\t [1/150] recall    0.49166666666666664 \n",
      "\t Checkpoint saved after epoch 1!\n",
      "[2/150][1] train batch loss 0.6755446195602417\n",
      "[2/150][1] precision 0.4479768786127168, recall 0.6458333333333334\n",
      "\t [2/150] test loss 41.44144058227539 \n",
      "\t [2/150] precision 0.3241758241758242 \n",
      "\t [2/150] recall    0.49166666666666664 \n",
      "\t Checkpoint saved after epoch 2!\n",
      "[3/150][1] train batch loss 0.6425778269767761\n",
      "[3/150][1] precision 0.540785498489426, recall 0.7458333333333333\n",
      "\t [3/150] test loss 30.630630493164062 \n",
      "\t [3/150] precision 0.4470198675496689 \n",
      "\t [3/150] recall    0.5625 \n",
      "\t Checkpoint saved after epoch 3!\n",
      "[4/150][1] train batch loss 0.5931060314178467\n",
      "[4/150][1] precision 0.6190476190476191, recall 0.7583333333333333\n",
      "\t [4/150] test loss 22.635135650634766 \n",
      "\t [4/150] precision 0.568904593639576 \n",
      "\t [4/150] recall    0.6708333333333333 \n",
      "\t Checkpoint saved after epoch 4!\n",
      "[5/150][1] train batch loss 0.5238432884216309\n",
      "[5/150][1] precision 0.6814814814814815, recall 0.7666666666666667\n",
      "\t [5/150] test loss 20.83333396911621 \n",
      "\t [5/150] precision 0.6061776061776062 \n",
      "\t [5/150] recall    0.6541666666666667 \n",
      "\t Checkpoint saved after epoch 5!\n",
      "[6/150][1] train batch loss 0.4396411180496216\n",
      "[6/150][1] precision 0.7126436781609196, recall 0.775\n",
      "\t [6/150] test loss 18.58108139038086 \n",
      "\t [6/150] precision 0.6506024096385542 \n",
      "\t [6/150] recall    0.675 \n",
      "\t Checkpoint saved after epoch 6!\n",
      "[7/150][1] train batch loss 0.3572869300842285\n",
      "[7/150][1] precision 0.753968253968254, recall 0.7916666666666666\n",
      "\t [7/150] test loss 16.779279708862305 \n",
      "\t [7/150] precision 0.6872427983539094 \n",
      "\t [7/150] recall    0.6958333333333333 \n",
      "\t Checkpoint saved after epoch 7!\n",
      "[8/150][1] train batch loss 0.29536038637161255\n",
      "[8/150][1] precision 0.7860082304526749, recall 0.7958333333333333\n",
      "\t [8/150] test loss 15.090089797973633 \n",
      "\t [8/150] precision 0.7245762711864406 \n",
      "\t [8/150] recall    0.7125 \n",
      "\t Checkpoint saved after epoch 8!\n",
      "[9/150][1] train batch loss 0.2564138174057007\n",
      "[9/150][1] precision 0.8227848101265823, recall 0.8125\n",
      "\t [9/150] test loss 14.86486530303955 \n",
      "\t [9/150] precision 0.7307692307692307 \n",
      "\t [9/150] recall    0.7125 \n",
      "\t Checkpoint saved after epoch 9!\n",
      "[10/150][1] train batch loss 0.22969499230384827\n",
      "[10/150][1] precision 0.8333333333333334, recall 0.8125\n",
      "\t [10/150] test loss 14.86486530303955 \n",
      "\t [10/150] precision 0.7307692307692307 \n",
      "\t [10/150] recall    0.7125 \n",
      "\t Checkpoint saved after epoch 10!\n",
      "[11/150][1] train batch loss 0.21358144283294678\n",
      "[11/150][1] precision 0.8333333333333334, recall 0.8125\n",
      "\t [11/150] test loss 14.752252578735352 \n",
      "\t [11/150] precision 0.729957805907173 \n",
      "\t [11/150] recall    0.7208333333333333 \n",
      "\t Checkpoint saved after epoch 11!\n",
      "[12/150][1] train batch loss 0.19352759420871735\n",
      "[12/150][1] precision 0.8270042194092827, recall 0.8166666666666667\n",
      "\t [12/150] test loss 15.878377914428711 \n",
      "\t [12/150] precision 0.7037037037037037 \n",
      "\t [12/150] recall    0.7125 \n",
      "\t Checkpoint saved after epoch 12!\n",
      "[13/150][1] train batch loss 0.18244825303554535\n",
      "[13/150][1] precision 0.8106995884773662, recall 0.8208333333333333\n",
      "\t [13/150] test loss 17.117116928100586 \n",
      "\t [13/150] precision 0.6833333333333333 \n",
      "\t [13/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 13!\n",
      "[14/150][1] train batch loss 0.1731671243906021\n",
      "[14/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [14/150] test loss 16.66666603088379 \n",
      "\t [14/150] precision 0.6916666666666667 \n",
      "\t [14/150] recall    0.6916666666666667 \n",
      "\t Checkpoint saved after epoch 14!\n",
      "[15/150][1] train batch loss 0.16773968935012817\n",
      "[15/150][1] precision 0.7966804979253111, recall 0.8\n",
      "\t [15/150] test loss 15.878377914428711 \n",
      "\t [15/150] precision 0.7037037037037037 \n",
      "\t [15/150] recall    0.7125 \n",
      "\t Checkpoint saved after epoch 15!\n",
      "[16/150][1] train batch loss 0.1651437133550644\n",
      "[16/150][1] precision 0.7901234567901234, recall 0.8\n",
      "\t [16/150] test loss 16.554054260253906 \n",
      "\t [16/150] precision 0.691358024691358 \n",
      "\t [16/150] recall    0.7 \n",
      "\t Checkpoint saved after epoch 16!\n",
      "[17/150][1] train batch loss 0.16045281291007996\n",
      "[17/150][1] precision 0.799163179916318, recall 0.7958333333333333\n",
      "\t [17/150] test loss 16.891891479492188 \n",
      "\t [17/150] precision 0.6829268292682927 \n",
      "\t [17/150] recall    0.7 \n",
      "\t Checkpoint saved after epoch 17!\n",
      "[18/150][1] train batch loss 0.15761259198188782\n",
      "[18/150][1] precision 0.7926829268292683, recall 0.8125\n",
      "\t [18/150] test loss 17.004505157470703 \n",
      "\t [18/150] precision 0.6831275720164609 \n",
      "\t [18/150] recall    0.6916666666666667 \n",
      "\t Checkpoint saved after epoch 18!\n",
      "[19/150][1] train batch loss 0.1556941419839859\n",
      "[19/150][1] precision 0.7983539094650206, recall 0.8083333333333333\n",
      "\t [19/150] test loss 17.004505157470703 \n",
      "\t [19/150] precision 0.6846473029045643 \n",
      "\t [19/150] recall    0.6875 \n",
      "\t Checkpoint saved after epoch 19!\n",
      "[20/150][1] train batch loss 0.15339697897434235\n",
      "[20/150][1] precision 0.8008298755186722, recall 0.8041666666666667\n",
      "\t [20/150] test loss 17.117116928100586 \n",
      "\t [20/150] precision 0.6833333333333333 \n",
      "\t [20/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 20!\n",
      "[21/150][1] train batch loss 0.15182001888751984\n",
      "[21/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [21/150] test loss 17.117116928100586 \n",
      "\t [21/150] precision 0.6833333333333333 \n",
      "\t [21/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 21!\n",
      "[22/150][1] train batch loss 0.1511756330728531\n",
      "[22/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [22/150] test loss 17.117116928100586 \n",
      "\t [22/150] precision 0.6833333333333333 \n",
      "\t [22/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 22!\n",
      "[23/150][1] train batch loss 0.15032383799552917\n",
      "[23/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [23/150] test loss 17.117116928100586 \n",
      "\t [23/150] precision 0.6833333333333333 \n",
      "\t [23/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 23!\n",
      "[24/150][1] train batch loss 0.1493341028690338\n",
      "[24/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [24/150] test loss 17.117116928100586 \n",
      "\t [24/150] precision 0.6833333333333333 \n",
      "\t [24/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 24!\n",
      "[25/150][1] train batch loss 0.1488388478755951\n",
      "[25/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [25/150] test loss 17.117116928100586 \n",
      "\t [25/150] precision 0.6833333333333333 \n",
      "\t [25/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 25!\n",
      "[26/150][1] train batch loss 0.14836306869983673\n",
      "[26/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [26/150] test loss 17.117116928100586 \n",
      "\t [26/150] precision 0.6833333333333333 \n",
      "\t [26/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 26!\n",
      "[27/150][1] train batch loss 0.14792652428150177\n",
      "[27/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [27/150] test loss 17.117116928100586 \n",
      "\t [27/150] precision 0.6833333333333333 \n",
      "\t [27/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 27!\n",
      "[28/150][1] train batch loss 0.14771611988544464\n",
      "[28/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [28/150] test loss 17.117116928100586 \n",
      "\t [28/150] precision 0.6833333333333333 \n",
      "\t [28/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 28!\n",
      "[29/150][1] train batch loss 0.14750051498413086\n",
      "[29/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [29/150] test loss 17.117116928100586 \n",
      "\t [29/150] precision 0.6833333333333333 \n",
      "\t [29/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 29!\n",
      "[30/150][1] train batch loss 0.14727720618247986\n",
      "[30/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [30/150] test loss 17.117116928100586 \n",
      "\t [30/150] precision 0.6833333333333333 \n",
      "\t [30/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 30!\n",
      "[31/150][1] train batch loss 0.14716027677059174\n",
      "[31/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [31/150] test loss 17.117116928100586 \n",
      "\t [31/150] precision 0.6833333333333333 \n",
      "\t [31/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 31!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32/150][1] train batch loss 0.14703765511512756\n",
      "[32/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [32/150] test loss 17.117116928100586 \n",
      "\t [32/150] precision 0.6833333333333333 \n",
      "\t [32/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 32!\n",
      "[33/150][1] train batch loss 0.14691057801246643\n",
      "[33/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [33/150] test loss 17.117116928100586 \n",
      "\t [33/150] precision 0.6833333333333333 \n",
      "\t [33/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 33!\n",
      "[34/150][1] train batch loss 0.14684513211250305\n",
      "[34/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [34/150] test loss 17.117116928100586 \n",
      "\t [34/150] precision 0.6833333333333333 \n",
      "\t [34/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 34!\n",
      "[35/150][1] train batch loss 0.14677806198596954\n",
      "[35/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [35/150] test loss 17.117116928100586 \n",
      "\t [35/150] precision 0.6833333333333333 \n",
      "\t [35/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 35!\n",
      "[36/150][1] train batch loss 0.14670981466770172\n",
      "[36/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [36/150] test loss 17.117116928100586 \n",
      "\t [36/150] precision 0.6833333333333333 \n",
      "\t [36/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 36!\n",
      "[37/150][1] train batch loss 0.1466752290725708\n",
      "[37/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [37/150] test loss 17.117116928100586 \n",
      "\t [37/150] precision 0.6833333333333333 \n",
      "\t [37/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 37!\n",
      "[38/150][1] train batch loss 0.14664019644260406\n",
      "[38/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [38/150] test loss 17.117116928100586 \n",
      "\t [38/150] precision 0.6833333333333333 \n",
      "\t [38/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 38!\n",
      "[39/150][1] train batch loss 0.14660483598709106\n",
      "[39/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [39/150] test loss 17.117116928100586 \n",
      "\t [39/150] precision 0.6833333333333333 \n",
      "\t [39/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 39!\n",
      "[40/150][1] train batch loss 0.14658699929714203\n",
      "[40/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [40/150] test loss 17.117116928100586 \n",
      "\t [40/150] precision 0.6833333333333333 \n",
      "\t [40/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 40!\n",
      "[41/150][1] train batch loss 0.14656898379325867\n",
      "[41/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [41/150] test loss 17.117116928100586 \n",
      "\t [41/150] precision 0.6833333333333333 \n",
      "\t [41/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 41!\n",
      "[42/150][1] train batch loss 0.14655083417892456\n",
      "[42/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [42/150] test loss 17.117116928100586 \n",
      "\t [42/150] precision 0.6833333333333333 \n",
      "\t [42/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 42!\n",
      "[43/150][1] train batch loss 0.14654171466827393\n",
      "[43/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [43/150] test loss 17.117116928100586 \n",
      "\t [43/150] precision 0.6833333333333333 \n",
      "\t [43/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 43!\n",
      "[44/150][1] train batch loss 0.14653250575065613\n",
      "[44/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [44/150] test loss 17.117116928100586 \n",
      "\t [44/150] precision 0.6833333333333333 \n",
      "\t [44/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 44!\n",
      "[45/150][1] train batch loss 0.14652326703071594\n",
      "[45/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [45/150] test loss 17.117116928100586 \n",
      "\t [45/150] precision 0.6833333333333333 \n",
      "\t [45/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 45!\n",
      "[46/150][1] train batch loss 0.14651860296726227\n",
      "[46/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [46/150] test loss 17.117116928100586 \n",
      "\t [46/150] precision 0.6833333333333333 \n",
      "\t [46/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 46!\n",
      "[47/150][1] train batch loss 0.1465139091014862\n",
      "[47/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [47/150] test loss 17.117116928100586 \n",
      "\t [47/150] precision 0.6833333333333333 \n",
      "\t [47/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 47!\n",
      "[48/150][1] train batch loss 0.14650920033454895\n",
      "[48/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [48/150] test loss 17.117116928100586 \n",
      "\t [48/150] precision 0.6833333333333333 \n",
      "\t [48/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 48!\n",
      "[49/150][1] train batch loss 0.14650683104991913\n",
      "[49/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [49/150] test loss 17.117116928100586 \n",
      "\t [49/150] precision 0.6833333333333333 \n",
      "\t [49/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 49!\n",
      "[50/150][1] train batch loss 0.14650443196296692\n",
      "[50/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [50/150] test loss 17.117116928100586 \n",
      "\t [50/150] precision 0.6833333333333333 \n",
      "\t [50/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 50!\n",
      "[51/150][1] train batch loss 0.14650201797485352\n",
      "[51/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [51/150] test loss 17.117116928100586 \n",
      "\t [51/150] precision 0.6833333333333333 \n",
      "\t [51/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 51!\n",
      "[52/150][1] train batch loss 0.1465008407831192\n",
      "[52/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [52/150] test loss 17.117116928100586 \n",
      "\t [52/150] precision 0.6833333333333333 \n",
      "\t [52/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 52!\n",
      "[53/150][1] train batch loss 0.1464996337890625\n",
      "[53/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [53/150] test loss 17.117116928100586 \n",
      "\t [53/150] precision 0.6833333333333333 \n",
      "\t [53/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 53!\n",
      "[54/150][1] train batch loss 0.1464984118938446\n",
      "[54/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [54/150] test loss 17.117116928100586 \n",
      "\t [54/150] precision 0.6833333333333333 \n",
      "\t [54/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 54!\n",
      "[55/150][1] train batch loss 0.14649780094623566\n",
      "[55/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [55/150] test loss 17.117116928100586 \n",
      "\t [55/150] precision 0.6833333333333333 \n",
      "\t [55/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 55!\n",
      "[56/150][1] train batch loss 0.14649717509746552\n",
      "[56/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [56/150] test loss 17.117116928100586 \n",
      "\t [56/150] precision 0.6833333333333333 \n",
      "\t [56/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 56!\n",
      "[57/150][1] train batch loss 0.14649657905101776\n",
      "[57/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [57/150] test loss 17.117116928100586 \n",
      "\t [57/150] precision 0.6833333333333333 \n",
      "\t [57/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 57!\n",
      "[58/150][1] train batch loss 0.1464962512254715\n",
      "[58/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [58/150] test loss 17.117116928100586 \n",
      "\t [58/150] precision 0.6833333333333333 \n",
      "\t [58/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 58!\n",
      "[59/150][1] train batch loss 0.14649593830108643\n",
      "[59/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [59/150] test loss 17.117116928100586 \n",
      "\t [59/150] precision 0.6833333333333333 \n",
      "\t [59/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 59!\n",
      "[60/150][1] train batch loss 0.14649562537670135\n",
      "[60/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [60/150] test loss 17.117116928100586 \n",
      "\t [60/150] precision 0.6833333333333333 \n",
      "\t [60/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 60!\n",
      "[61/150][1] train batch loss 0.14649532735347748\n",
      "[61/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [61/150] test loss 17.117116928100586 \n",
      "\t [61/150] precision 0.6833333333333333 \n",
      "\t [61/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 61!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62/150][1] train batch loss 0.1464949995279312\n",
      "[62/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [62/150] test loss 17.117116928100586 \n",
      "\t [62/150] precision 0.6833333333333333 \n",
      "\t [62/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 62!\n",
      "[63/150][1] train batch loss 0.14649468660354614\n",
      "[63/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [63/150] test loss 17.117116928100586 \n",
      "\t [63/150] precision 0.6833333333333333 \n",
      "\t [63/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 63!\n",
      "[64/150][1] train batch loss 0.14649435877799988\n",
      "[64/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [64/150] test loss 17.117116928100586 \n",
      "\t [64/150] precision 0.6833333333333333 \n",
      "\t [64/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 64!\n",
      "[65/150][1] train batch loss 0.1464940309524536\n",
      "[65/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [65/150] test loss 17.117116928100586 \n",
      "\t [65/150] precision 0.6833333333333333 \n",
      "\t [65/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 65!\n",
      "[66/150][1] train batch loss 0.14649371802806854\n",
      "[66/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [66/150] test loss 17.117116928100586 \n",
      "\t [66/150] precision 0.6833333333333333 \n",
      "\t [66/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 66!\n",
      "[67/150][1] train batch loss 0.14649342000484467\n",
      "[67/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [67/150] test loss 17.117116928100586 \n",
      "\t [67/150] precision 0.6833333333333333 \n",
      "\t [67/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 67!\n",
      "[68/150][1] train batch loss 0.1464930921792984\n",
      "[68/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [68/150] test loss 17.117116928100586 \n",
      "\t [68/150] precision 0.6833333333333333 \n",
      "\t [68/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 68!\n",
      "[69/150][1] train batch loss 0.14649274945259094\n",
      "[69/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [69/150] test loss 17.117116928100586 \n",
      "\t [69/150] precision 0.6833333333333333 \n",
      "\t [69/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 69!\n",
      "[70/150][1] train batch loss 0.14649243652820587\n",
      "[70/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [70/150] test loss 17.117116928100586 \n",
      "\t [70/150] precision 0.6833333333333333 \n",
      "\t [70/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 70!\n",
      "[71/150][1] train batch loss 0.1464921087026596\n",
      "[71/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [71/150] test loss 17.117116928100586 \n",
      "\t [71/150] precision 0.6833333333333333 \n",
      "\t [71/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 71!\n",
      "[72/150][1] train batch loss 0.14649178087711334\n",
      "[72/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [72/150] test loss 17.117116928100586 \n",
      "\t [72/150] precision 0.6833333333333333 \n",
      "\t [72/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 72!\n",
      "[73/150][1] train batch loss 0.14649143815040588\n",
      "[73/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [73/150] test loss 17.117116928100586 \n",
      "\t [73/150] precision 0.6833333333333333 \n",
      "\t [73/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 73!\n",
      "[74/150][1] train batch loss 0.14649111032485962\n",
      "[74/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [74/150] test loss 17.117116928100586 \n",
      "\t [74/150] precision 0.6833333333333333 \n",
      "\t [74/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 74!\n",
      "[75/150][1] train batch loss 0.14649076759815216\n",
      "[75/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [75/150] test loss 17.117116928100586 \n",
      "\t [75/150] precision 0.6833333333333333 \n",
      "\t [75/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 75!\n",
      "[76/150][1] train batch loss 0.1464904397726059\n",
      "[76/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [76/150] test loss 17.117116928100586 \n",
      "\t [76/150] precision 0.6833333333333333 \n",
      "\t [76/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 76!\n",
      "[77/150][1] train batch loss 0.14649012684822083\n",
      "[77/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [77/150] test loss 17.117116928100586 \n",
      "\t [77/150] precision 0.6833333333333333 \n",
      "\t [77/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 77!\n",
      "[78/150][1] train batch loss 0.14648976922035217\n",
      "[78/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [78/150] test loss 17.117116928100586 \n",
      "\t [78/150] precision 0.6833333333333333 \n",
      "\t [78/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 78!\n",
      "[79/150][1] train batch loss 0.1464894562959671\n",
      "[79/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [79/150] test loss 17.117116928100586 \n",
      "\t [79/150] precision 0.6833333333333333 \n",
      "\t [79/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 79!\n",
      "[80/150][1] train batch loss 0.14648911356925964\n",
      "[80/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [80/150] test loss 17.117116928100586 \n",
      "\t [80/150] precision 0.6833333333333333 \n",
      "\t [80/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 80!\n",
      "[81/150][1] train batch loss 0.14648878574371338\n",
      "[81/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [81/150] test loss 17.117116928100586 \n",
      "\t [81/150] precision 0.6833333333333333 \n",
      "\t [81/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 81!\n",
      "[82/150][1] train batch loss 0.14648845791816711\n",
      "[82/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [82/150] test loss 17.117116928100586 \n",
      "\t [82/150] precision 0.6833333333333333 \n",
      "\t [82/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 82!\n",
      "[83/150][1] train batch loss 0.14648810029029846\n",
      "[83/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [83/150] test loss 17.117116928100586 \n",
      "\t [83/150] precision 0.6833333333333333 \n",
      "\t [83/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 83!\n",
      "[84/150][1] train batch loss 0.146487757563591\n",
      "[84/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [84/150] test loss 17.117116928100586 \n",
      "\t [84/150] precision 0.6833333333333333 \n",
      "\t [84/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 84!\n",
      "[85/150][1] train batch loss 0.14648742973804474\n",
      "[85/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [85/150] test loss 17.117116928100586 \n",
      "\t [85/150] precision 0.6833333333333333 \n",
      "\t [85/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 85!\n",
      "[86/150][1] train batch loss 0.14648708701133728\n",
      "[86/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [86/150] test loss 17.117116928100586 \n",
      "\t [86/150] precision 0.6833333333333333 \n",
      "\t [86/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 86!\n",
      "[87/150][1] train batch loss 0.14648674428462982\n",
      "[87/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [87/150] test loss 17.117116928100586 \n",
      "\t [87/150] precision 0.6833333333333333 \n",
      "\t [87/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 87!\n",
      "[88/150][1] train batch loss 0.14648641645908356\n",
      "[88/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [88/150] test loss 17.117116928100586 \n",
      "\t [88/150] precision 0.6833333333333333 \n",
      "\t [88/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 88!\n",
      "[89/150][1] train batch loss 0.1464860588312149\n",
      "[89/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [89/150] test loss 17.117116928100586 \n",
      "\t [89/150] precision 0.6833333333333333 \n",
      "\t [89/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 89!\n",
      "[90/150][1] train batch loss 0.14648571610450745\n",
      "[90/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [90/150] test loss 17.117116928100586 \n",
      "\t [90/150] precision 0.6833333333333333 \n",
      "\t [90/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 90!\n",
      "[91/150][1] train batch loss 0.1464853733778\n",
      "[91/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [91/150] test loss 17.117116928100586 \n",
      "\t [91/150] precision 0.6833333333333333 \n",
      "\t [91/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 91!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[92/150][1] train batch loss 0.14648503065109253\n",
      "[92/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [92/150] test loss 17.117116928100586 \n",
      "\t [92/150] precision 0.6833333333333333 \n",
      "\t [92/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 92!\n",
      "[93/150][1] train batch loss 0.14648468792438507\n",
      "[93/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [93/150] test loss 17.117116928100586 \n",
      "\t [93/150] precision 0.6833333333333333 \n",
      "\t [93/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 93!\n",
      "[94/150][1] train batch loss 0.1464843451976776\n",
      "[94/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [94/150] test loss 17.117116928100586 \n",
      "\t [94/150] precision 0.6833333333333333 \n",
      "\t [94/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 94!\n",
      "[95/150][1] train batch loss 0.14648397266864777\n",
      "[95/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [95/150] test loss 17.117116928100586 \n",
      "\t [95/150] precision 0.6833333333333333 \n",
      "\t [95/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 95!\n",
      "[96/150][1] train batch loss 0.1464836299419403\n",
      "[96/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [96/150] test loss 17.117116928100586 \n",
      "\t [96/150] precision 0.6833333333333333 \n",
      "\t [96/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 96!\n",
      "[97/150][1] train batch loss 0.14648327231407166\n",
      "[97/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [97/150] test loss 17.117116928100586 \n",
      "\t [97/150] precision 0.6833333333333333 \n",
      "\t [97/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 97!\n",
      "[98/150][1] train batch loss 0.1464829295873642\n",
      "[98/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [98/150] test loss 17.117116928100586 \n",
      "\t [98/150] precision 0.6833333333333333 \n",
      "\t [98/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 98!\n",
      "[99/150][1] train batch loss 0.14648258686065674\n",
      "[99/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [99/150] test loss 17.117116928100586 \n",
      "\t [99/150] precision 0.6833333333333333 \n",
      "\t [99/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 99!\n",
      "[100/150][1] train batch loss 0.14648224413394928\n",
      "[100/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [100/150] test loss 17.117116928100586 \n",
      "\t [100/150] precision 0.6833333333333333 \n",
      "\t [100/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 100!\n",
      "[101/150][1] train batch loss 0.14648188650608063\n",
      "[101/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [101/150] test loss 17.117116928100586 \n",
      "\t [101/150] precision 0.6833333333333333 \n",
      "\t [101/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 101!\n",
      "[102/150][1] train batch loss 0.14648154377937317\n",
      "[102/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [102/150] test loss 17.117116928100586 \n",
      "\t [102/150] precision 0.6833333333333333 \n",
      "\t [102/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 102!\n",
      "[103/150][1] train batch loss 0.14648118615150452\n",
      "[103/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [103/150] test loss 17.117116928100586 \n",
      "\t [103/150] precision 0.6833333333333333 \n",
      "\t [103/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 103!\n",
      "[104/150][1] train batch loss 0.14648081362247467\n",
      "[104/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [104/150] test loss 17.117116928100586 \n",
      "\t [104/150] precision 0.6833333333333333 \n",
      "\t [104/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 104!\n",
      "[105/150][1] train batch loss 0.1464804708957672\n",
      "[105/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [105/150] test loss 17.117116928100586 \n",
      "\t [105/150] precision 0.6833333333333333 \n",
      "\t [105/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 105!\n",
      "[106/150][1] train batch loss 0.14648012816905975\n",
      "[106/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [106/150] test loss 17.117116928100586 \n",
      "\t [106/150] precision 0.6833333333333333 \n",
      "\t [106/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 106!\n",
      "[107/150][1] train batch loss 0.1464797705411911\n",
      "[107/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [107/150] test loss 17.117116928100586 \n",
      "\t [107/150] precision 0.6833333333333333 \n",
      "\t [107/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 107!\n",
      "[108/150][1] train batch loss 0.14647941291332245\n",
      "[108/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [108/150] test loss 17.117116928100586 \n",
      "\t [108/150] precision 0.6833333333333333 \n",
      "\t [108/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 108!\n",
      "[109/150][1] train batch loss 0.146479070186615\n",
      "[109/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [109/150] test loss 17.117116928100586 \n",
      "\t [109/150] precision 0.6833333333333333 \n",
      "\t [109/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 109!\n",
      "[110/150][1] train batch loss 0.14647868275642395\n",
      "[110/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [110/150] test loss 17.117116928100586 \n",
      "\t [110/150] precision 0.6833333333333333 \n",
      "\t [110/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 110!\n",
      "[111/150][1] train batch loss 0.1464783251285553\n",
      "[111/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [111/150] test loss 17.117116928100586 \n",
      "\t [111/150] precision 0.6833333333333333 \n",
      "\t [111/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 111!\n",
      "[112/150][1] train batch loss 0.14647796750068665\n",
      "[112/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [112/150] test loss 17.117116928100586 \n",
      "\t [112/150] precision 0.6833333333333333 \n",
      "\t [112/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 112!\n",
      "[113/150][1] train batch loss 0.1464776247739792\n",
      "[113/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [113/150] test loss 17.117116928100586 \n",
      "\t [113/150] precision 0.6833333333333333 \n",
      "\t [113/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 113!\n",
      "[114/150][1] train batch loss 0.14647726714611053\n",
      "[114/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [114/150] test loss 17.117116928100586 \n",
      "\t [114/150] precision 0.6833333333333333 \n",
      "\t [114/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 114!\n",
      "[115/150][1] train batch loss 0.1464768797159195\n",
      "[115/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [115/150] test loss 17.117116928100586 \n",
      "\t [115/150] precision 0.6833333333333333 \n",
      "\t [115/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 115!\n",
      "[116/150][1] train batch loss 0.14647653698921204\n",
      "[116/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [116/150] test loss 17.117116928100586 \n",
      "\t [116/150] precision 0.6833333333333333 \n",
      "\t [116/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 116!\n",
      "[117/150][1] train batch loss 0.1464761644601822\n",
      "[117/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [117/150] test loss 17.117116928100586 \n",
      "\t [117/150] precision 0.6833333333333333 \n",
      "\t [117/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 117!\n",
      "[118/150][1] train batch loss 0.14647580683231354\n",
      "[118/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [118/150] test loss 17.117116928100586 \n",
      "\t [118/150] precision 0.6833333333333333 \n",
      "\t [118/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 118!\n",
      "[119/150][1] train batch loss 0.1464754343032837\n",
      "[119/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [119/150] test loss 17.117116928100586 \n",
      "\t [119/150] precision 0.6833333333333333 \n",
      "\t [119/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 119!\n",
      "[120/150][1] train batch loss 0.14647507667541504\n",
      "[120/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [120/150] test loss 17.117116928100586 \n",
      "\t [120/150] precision 0.6833333333333333 \n",
      "\t [120/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 120!\n",
      "[121/150][1] train batch loss 0.1464747190475464\n",
      "[121/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [121/150] test loss 17.117116928100586 \n",
      "\t [121/150] precision 0.6833333333333333 \n",
      "\t [121/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 121!\n",
      "[122/150][1] train batch loss 0.14647434651851654\n",
      "[122/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [122/150] test loss 17.117116928100586 \n",
      "\t [122/150] precision 0.6833333333333333 \n",
      "\t [122/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 122!\n",
      "[123/150][1] train batch loss 0.1464739888906479\n",
      "[123/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [123/150] test loss 17.117116928100586 \n",
      "\t [123/150] precision 0.6833333333333333 \n",
      "\t [123/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 123!\n",
      "[124/150][1] train batch loss 0.14647361636161804\n",
      "[124/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [124/150] test loss 17.117116928100586 \n",
      "\t [124/150] precision 0.6833333333333333 \n",
      "\t [124/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 124!\n",
      "[125/150][1] train batch loss 0.1464732438325882\n",
      "[125/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [125/150] test loss 17.117116928100586 \n",
      "\t [125/150] precision 0.6833333333333333 \n",
      "\t [125/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 125!\n",
      "[126/150][1] train batch loss 0.14647291600704193\n",
      "[126/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [126/150] test loss 17.117116928100586 \n",
      "\t [126/150] precision 0.6833333333333333 \n",
      "\t [126/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 126!\n",
      "[127/150][1] train batch loss 0.1464725285768509\n",
      "[127/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [127/150] test loss 17.117116928100586 \n",
      "\t [127/150] precision 0.6833333333333333 \n",
      "\t [127/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 127!\n",
      "[128/150][1] train batch loss 0.14647215604782104\n",
      "[128/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [128/150] test loss 17.117116928100586 \n",
      "\t [128/150] precision 0.6833333333333333 \n",
      "\t [128/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 128!\n",
      "[129/150][1] train batch loss 0.1464717984199524\n",
      "[129/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [129/150] test loss 17.117116928100586 \n",
      "\t [129/150] precision 0.6833333333333333 \n",
      "\t [129/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 129!\n",
      "[130/150][1] train batch loss 0.14647142589092255\n",
      "[130/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [130/150] test loss 17.117116928100586 \n",
      "\t [130/150] precision 0.6833333333333333 \n",
      "\t [130/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 130!\n",
      "[131/150][1] train batch loss 0.1464710384607315\n",
      "[131/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [131/150] test loss 17.117116928100586 \n",
      "\t [131/150] precision 0.6833333333333333 \n",
      "\t [131/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 131!\n",
      "[132/150][1] train batch loss 0.14647068083286285\n",
      "[132/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [132/150] test loss 17.117116928100586 \n",
      "\t [132/150] precision 0.6833333333333333 \n",
      "\t [132/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 132!\n",
      "[133/150][1] train batch loss 0.1464703232049942\n",
      "[133/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [133/150] test loss 17.117116928100586 \n",
      "\t [133/150] precision 0.6833333333333333 \n",
      "\t [133/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 133!\n",
      "[134/150][1] train batch loss 0.14646993577480316\n",
      "[134/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [134/150] test loss 17.117116928100586 \n",
      "\t [134/150] precision 0.6833333333333333 \n",
      "\t [134/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 134!\n",
      "[135/150][1] train batch loss 0.14646956324577332\n",
      "[135/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [135/150] test loss 17.117116928100586 \n",
      "\t [135/150] precision 0.6833333333333333 \n",
      "\t [135/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 135!\n",
      "[136/150][1] train batch loss 0.14646919071674347\n",
      "[136/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [136/150] test loss 17.117116928100586 \n",
      "\t [136/150] precision 0.6833333333333333 \n",
      "\t [136/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 136!\n",
      "[137/150][1] train batch loss 0.14646881818771362\n",
      "[137/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [137/150] test loss 17.117116928100586 \n",
      "\t [137/150] precision 0.6833333333333333 \n",
      "\t [137/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 137!\n",
      "[138/150][1] train batch loss 0.14646844565868378\n",
      "[138/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [138/150] test loss 17.117116928100586 \n",
      "\t [138/150] precision 0.6833333333333333 \n",
      "\t [138/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 138!\n",
      "[139/150][1] train batch loss 0.14646807312965393\n",
      "[139/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [139/150] test loss 17.117116928100586 \n",
      "\t [139/150] precision 0.6833333333333333 \n",
      "\t [139/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 139!\n",
      "[140/150][1] train batch loss 0.14646770060062408\n",
      "[140/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [140/150] test loss 17.117116928100586 \n",
      "\t [140/150] precision 0.6833333333333333 \n",
      "\t [140/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 140!\n",
      "[141/150][1] train batch loss 0.14646732807159424\n",
      "[141/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [141/150] test loss 17.117116928100586 \n",
      "\t [141/150] precision 0.6833333333333333 \n",
      "\t [141/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 141!\n",
      "[142/150][1] train batch loss 0.1464669555425644\n",
      "[142/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [142/150] test loss 17.117116928100586 \n",
      "\t [142/150] precision 0.6833333333333333 \n",
      "\t [142/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 142!\n",
      "[143/150][1] train batch loss 0.14646656811237335\n",
      "[143/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [143/150] test loss 17.117116928100586 \n",
      "\t [143/150] precision 0.6833333333333333 \n",
      "\t [143/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 143!\n",
      "[144/150][1] train batch loss 0.1464661955833435\n",
      "[144/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [144/150] test loss 17.117116928100586 \n",
      "\t [144/150] precision 0.6833333333333333 \n",
      "\t [144/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 144!\n",
      "[145/150][1] train batch loss 0.14646582305431366\n",
      "[145/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [145/150] test loss 17.117116928100586 \n",
      "\t [145/150] precision 0.6833333333333333 \n",
      "\t [145/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 145!\n",
      "[146/150][1] train batch loss 0.14646543562412262\n",
      "[146/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [146/150] test loss 17.117116928100586 \n",
      "\t [146/150] precision 0.6833333333333333 \n",
      "\t [146/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 146!\n",
      "[147/150][1] train batch loss 0.14646506309509277\n",
      "[147/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [147/150] test loss 17.117116928100586 \n",
      "\t [147/150] precision 0.6833333333333333 \n",
      "\t [147/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 147!\n",
      "[148/150][1] train batch loss 0.14646470546722412\n",
      "[148/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [148/150] test loss 17.117116928100586 \n",
      "\t [148/150] precision 0.6833333333333333 \n",
      "\t [148/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 148!\n",
      "[149/150][1] train batch loss 0.14646431803703308\n",
      "[149/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [149/150] test loss 17.117116928100586 \n",
      "\t [149/150] precision 0.6833333333333333 \n",
      "\t [149/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 149!\n",
      "[150/150][1] train batch loss 0.14646394550800323\n",
      "[150/150][1] precision 0.8041666666666667, recall 0.8041666666666667\n",
      "\t [150/150] test loss 17.117116928100586 \n",
      "\t [150/150] precision 0.6833333333333333 \n",
      "\t [150/150] recall    0.6833333333333333 \n",
      "\t Checkpoint saved after epoch 150!\n"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "# model = net  \n",
    "oldshape = traindata[0,:,:,:].shape\n",
    "num = 3\n",
    "trainpt = traindata[0:num,:,:,:].view(num,oldshape[0],oldshape[1],oldshape[2])\n",
    "trainlb = trainlabels[0:num,:].view(num,-1)\n",
    "valpt = valdata[0:num,:,:,:].view(num,oldshape[0],oldshape[1],oldshape[2])\n",
    "vallb = vallabels[0:num,:].view(num,-1)\n",
    "# mnet  = trainMNet(traindata, trainlabels, valdata, vallabels,model=model, \\\n",
    "#               epochs=8, batchsize=5, \\\n",
    "#               positive_weight=1,\\\n",
    "#               lr=1e-1, lr_weight_decay=0, opt_momentum=0,\\\n",
    "#               lr_s_stepsize=2, lr_s_gamma=0.5,\\\n",
    "#               threshold=.5, beta=1, save_cp=True)\n",
    "mnet  = trainMNet(trainpt, trainlb, valpt, vallb,model=model, \\\n",
    "              epochs=150, batchsize=5, \\\n",
    "              positive_weight=1,\\\n",
    "              lr=5e-4, lr_weight_decay=0, opt_momentum=0,\\\n",
    "              lr_s_stepsize=2, lr_s_gamma=0.5,\\\n",
    "              threshold=.5, beta=1, save_cp=True,print_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97da505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b058c75",
   "metadata": {},
   "source": [
    "# data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7327de72",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = 24\n",
    "mask = torch.tensor( mask_naiveRand(320,fix=base,other=0,roll=False)[0] ,dtype=torch.float )\n",
    "data_under = np.zeros((datashape[2],datashape[0],datashape[1]))\n",
    "for ind in range(data.shape[2]):\n",
    "    img = data[:,:,ind]\n",
    "    img = img/np.max(np.abs(img))\n",
    "    yfull = F.fftn(torch.tensor(img,dtype=torch.float),dim=(0,1),norm='ortho')\n",
    "    ypart = torch.tensordot(torch.diag(mask).to(torch.cfloat) , yfull,dims=([1],[0]))\n",
    "    data_under[ind,:,:] = torch.abs(F.ifftn(ypart,dim=(0,1),norm='ortho'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff9fb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "imgNum = 199\n",
    "traininds, testinds = train_test_split(np.arange(imgNum),random_state=0,shuffle=True,train_size=round(imgNum*0.8))\n",
    "test_total = testinds.size\n",
    "traindata    = data_under[traininds,:,:]\n",
    "trainlabels  = mask_filter(labels[traininds,:],base=base)\n",
    "valdata      = data_under[testinds[0:test_total//2],:,:]\n",
    "vallabels    = mask_filter(labels[testinds[0:test_total//2],:],base=base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5b2d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(traindata.shape)\n",
    "print(trainlabels.shape)\n",
    "print(valdata.shape)\n",
    "print(vallabels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fedb9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sys.path.insert(0,'/home/huangz78/mri/mnet/')\n",
    "import mnet\n",
    "reload(mnet)\n",
    "from mnet import MNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d585f5",
   "metadata": {},
   "source": [
    "#### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec01a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnet = MNet(out_size=trainlabels.shape[1])\n",
    "# checkpoint = torch.load('/home/huangz78/mri/checkpoints/mnet.pth')\n",
    "# mnet.load_state_dict(checkpoint['model_state_dict'])\n",
    "# print('mnet loaded successfully from : ' + '/home/huangz78/mri/checkpoints/mnet.pth' )\n",
    "# mnet.train()\n",
    "# # print(mnet)\n",
    "trainMNet(traindata,trainlabels, valdata,vallabels,\n",
    "          epochs=60, batchsize=5, \\\n",
    "          lr=1e-4, lr_weight_decay=0,opt_momentum=0,positive_weight=1,\\\n",
    "          lr_s_stepsize=2,lr_s_gamma=0.5,\\\n",
    "          threshold=.5, beta=1,save_cp=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
