{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76416b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.fft as F\n",
    "from importlib import reload\n",
    "from torch.nn.functional import relu\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Func\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import pdb\n",
    "from utils import kplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45f091d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0,'/home/huangz78/mri/unet/')\n",
    "import unet_model\n",
    "reload(unet_model)\n",
    "from unet_model import UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc55eddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir1 = '/home/huangz78/data/data_gt.npz'\n",
    "data1 = np.load(data_dir1)\n",
    "print('file1',data1.files)\n",
    "print(data1['imgdata'].shape)\n",
    "data_dir2 = '/mnt/shared_b/data/fastMRI/singlecoil_train/expanded_gt.npz'\n",
    "data2 = np.load(data_dir2)\n",
    "print('file2',data2.files)\n",
    "print(data2['imgdata'].shape)\n",
    "\n",
    "data = np.concatenate((data1['imgdata'],data2['imgdata']),axis=2)\n",
    "del data1\n",
    "del data2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9d7c72",
   "metadata": {},
   "source": [
    "## train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80deb5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "imgNum = 199+1014\n",
    "traininds, testinds = train_test_split(np.arange(imgNum),random_state=0,shuffle=True,train_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672f6c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainimgs = data['imgdata'][:,:,traininds]\n",
    "# testimgs = data['imgdata'][:,:,testinds]\n",
    "trainimgs = data[:,:,traininds]\n",
    "testimgs  = data[:,:,testinds]\n",
    "train_y = torch.zeros((trainimgs.shape[2],trainimgs.shape[0],trainimgs.shape[1]),dtype=torch.cdouble)\n",
    "train_yfull = torch.zeros((trainimgs.shape[2],trainimgs.shape[0],trainimgs.shape[1]),dtype=torch.cdouble)\n",
    "train_x = torch.zeros((trainimgs.shape[2],trainimgs.shape[0],trainimgs.shape[1]),dtype=torch.double)\n",
    "train_xfull = torch.zeros((trainimgs.shape[2],trainimgs.shape[0],trainimgs.shape[1]),dtype=torch.double)\n",
    "test_y  = torch.zeros((testimgs.shape[2],testimgs.shape[0],testimgs.shape[1]),dtype=torch.cdouble)\n",
    "test_yfull  = torch.zeros((testimgs.shape[2],testimgs.shape[0],testimgs.shape[1]),dtype=torch.cdouble)\n",
    "test_x  = torch.zeros((testimgs.shape[2],testimgs.shape[0],testimgs.shape[1]),dtype=torch.double)\n",
    "test_xfull  = torch.zeros((testimgs.shape[2],testimgs.shape[0],testimgs.shape[1]),dtype=torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02ad12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from utils import mask_prob,mask_naiveRand\n",
    "base = 0.05; other=0.15\n",
    "torch.manual_seed(0)\n",
    "mask = torch.tensor( mask_naiveRand(320,fix=round(320*base),other=round(320*other),roll=False)[0] ,dtype=torch.double )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fdf49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = torch.fft.fftshift(mask)\n",
    "kplot(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237b3691",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in range(trainimgs.shape[2]):\n",
    "    img = trainimgs[:,:,ind]\n",
    "    img = img/np.max(np.abs(img))\n",
    "#     mask = torch.tensor( mask_prob(img,fix=round(320*base),other=round(320*other),roll=False) ,dtype=torch.double )\n",
    "    train_xfull[ind,:,:] = torch.tensor(img)\n",
    "    train_yfull[ind,:,:] = F.fftn(torch.tensor(img),dim=(0,1),norm='ortho')\n",
    "    train_y[ind,:,:] = torch.tensordot(torch.diag(mask).to(torch.cdouble) , train_yfull[ind,:,:],dims=([1],[0]))\n",
    "    train_x[ind,:,:] = torch.abs(F.ifftn(train_y[ind,:,:],dim=(0,1),norm='ortho'))\n",
    "    \n",
    "np.savez('/home/huangz78/data/traindata_y.npz',yfull=train_yfull,y=train_y,mask=mask)\n",
    "np.savez('/home/huangz78/data/traindata_x.npz',xfull=train_xfull,x=train_x,mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce467e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in range(testimgs.shape[2]):\n",
    "    img = testimgs[:,:,ind]\n",
    "    img = img/np.max(np.abs(img))\n",
    "#     mask = torch.tensor( mask_prob(img,fix=round(320*base),other=round(320*other),roll=False) ,dtype=torch.cdouble )\n",
    "    test_xfull[ind,:,:] = torch.tensor(img)\n",
    "    test_yfull[ind,:,:] = F.fftn(test_xfull[ind,:,:],dim=(0,1),norm='ortho')\n",
    "    test_y[ind,:,:] = torch.tensordot(torch.diag(mask).to(torch.cdouble) , test_yfull[ind,:,:],dims=([1],[0]))\n",
    "    test_x[ind,:,:] = torch.abs(F.ifftn(test_y[ind,:,:],dim=(0,1),norm='ortho'))\n",
    "np.savez('/home/huangz78/data/testdata_y.npz',yfull=test_yfull,y=test_y,mask=mask)\n",
    "np.savez('/home/huangz78/data/testdata_x.npz',xfull=test_xfull,x=test_x,mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f9fa92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check test data as an example\n",
    "test_x_dir = '/home/huangz78/data/testdata_x.npz'\n",
    "test_y_dir = '/home/huangz78/data/testdata_y.npz'\n",
    "test_xfull = np.load(test_x_dir)['xfull']\n",
    "test_yfull = np.load(test_y_dir)['yfull']\n",
    "test_x = np.load(test_x_dir)['x']\n",
    "test_y = np.load(test_y_dir)['y']\n",
    "kplot(test_xfull[0,:,:])\n",
    "kplot(test_x[0,:,:])\n",
    "kplot(test_yfull[0,:,:],roll=True,log=True)\n",
    "kplot(test_y[0,:,:],roll=True,log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848230e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = '/home/huangz78/data/traindata.npz'\n",
    "train_y = np.load(train_data_dir)['y'][:,:,0:2]\n",
    "train_yfull = np.load(train_data_dir)['yfull'][:,:,0:2]\n",
    "\n",
    "train = np.load(train_data_dir)\n",
    "print(train.files)\n",
    "\n",
    "train['y'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e7f2fb",
   "metadata": {},
   "source": [
    "## data view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a9f9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '/home/huangz78/data/traindata_x.npz'\n",
    "train_sub = np.load(train_dir)['x'][0:2,:,:]\n",
    "train_full = np.load(train_dir)['xfull'][0:2,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5337f093",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load(train_dir)['xfull'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ca0bc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_full = np.load(train_dir)['xfull']\n",
    "for ind in range(train_full.shape[0]):\n",
    "    plt.figure()\n",
    "    plt.imshow(train_full[ind,:,:])\n",
    "    plt.title('img '+str(ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e75817",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(train_full[0,:,:]))\n",
    "print(train_full[0,-1,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ad5807",
   "metadata": {},
   "source": [
    "## train net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccddc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(net,epochs=5,batch_size=5,\\\n",
    "              lr=0.001,lr_weight_decay=1e-8,lr_momentum=0.9,\\\n",
    "              lr_s_stepsize=10,lr_s_factor=0.5,\\\n",
    "              save_cp=False,datatype=torch.float):\n",
    "    if net.n_channels == 2:\n",
    "        train_dir = '/home/huangz78/data/traindata_y.npz'\n",
    "        train_sub = np.load(train_dir)['y']\n",
    "        train_full = np.load(train_dir)['yfull']\n",
    "    elif net.n_channels == 1:\n",
    "        train_dir = '/home/huangz78/data/traindata_x.npz'\n",
    "        train_sub = np.load(train_dir)['x']\n",
    "        train_full = np.load(train_dir)['xfull']\n",
    "    \n",
    "#     test_sub = np.copy(train_sub[0:2,:,:])\n",
    "#     test_full = np.copy(train_full[0:2,:,:])\n",
    "\n",
    "    test_dir = '/home/huangz78/data/testdata_x.npz'\n",
    "    test_sub  = torch.tensor(np.load(test_dir)['x'])     ; test_sub  = test_sub[0:test_sub.shape[0]//2,:,:]\n",
    "    test_full = torch.tensor(np.load(test_dir)['xfull']) ; test_full = test_full[0:test_full.shape[0]//2,:,:]      \n",
    "    Heg,Wid,n_train,n_test = train_sub.shape[1],train_sub.shape[2],train_sub.shape[0],test_sub.shape[0]\n",
    "    \n",
    "    print('n_train = {}, n_test = {}'.format(n_train,n_test))\n",
    "   \n",
    "    if net.n_channels == 2:\n",
    "        testsub  = torch.zeros((n_test,2,Heg,Wid),dtype=datatype)\n",
    "        testsub[:,0,:,:] = torch.real(test_sub)\n",
    "        testsub[:,1,:,:] = torch.imag(test_sub)\n",
    "\n",
    "        testfull  = torch.zeros((n_test,2,Heg,Wid),dtype=datatype)\n",
    "        testfull[:,0,:,:] = torch.real(test_full)\n",
    "        testfull[:,1,:,:] = torch.imag(test_full)\n",
    "    elif net.n_channels == 1:\n",
    "        testsub = torch.reshape(test_sub,(n_test,1,Heg,Wid)).to(datatype)\n",
    "        testfull = torch.reshape(test_full,(n_test,1,Heg,Wid)).to(datatype)\n",
    "\n",
    "    optimizer = optim.RMSprop(net.parameters(), lr=lr, weight_decay=lr_weight_decay, momentum=lr_momentum)\n",
    "    # optimizer = optim.Adam/SGD\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_s_stepsize, gamma=lr_s_gamma)\n",
    "#     scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min' if net.n_classes > 1 else 'max', patience=2)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    train_loss = list([]); test_loss = list([])\n",
    "    global_step = 1\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0        \n",
    "        batch_init = 0\n",
    "        while batch_init < n_train:        \n",
    "            batch = np.arange(batch_init,min(batch_init+batch_size,n_train))\n",
    "            \n",
    "            imgbatch_tmp = torch.tensor(train_sub[batch,:,:],dtype=datatype)\n",
    "            if net.n_channels == 2:\n",
    "                imgbatch = torch.zeros((len(batch),2,Heg,Wid),dtype=datatype)\n",
    "                imgbatch[:,0,:,:] = torch.real(imgbatch_tmp)\n",
    "                imgbatch[:,1,:,:] = torch.imag(imgbatch_tmp)\n",
    "            elif net.n_channels == 1:\n",
    "                imgbatch = torch.reshape(imgbatch_tmp,(len(batch),1,Heg,Wid))\n",
    "            \n",
    "            labelbatch_tmp = torch.tensor(train_full[batch,:,:],dtype=datatype)\n",
    "            if net.n_channels == 2:\n",
    "                labelbatch = torch.zeros((len(batch),2,Heg,Wid),dtype=datatype)\n",
    "                labelbatch[:,0,:,:] = torch.real(labelbatch_tmp)\n",
    "                labelbatch[:,1,:,:] = torch.imag(labelbatch_tmp)\n",
    "            elif net.n_channels == 1:\n",
    "                labelbatch = torch.reshape(labelbatch_tmp,(len(batch),1,Heg,Wid))\n",
    "            \n",
    "            batch_init += len(batch)\n",
    "            \n",
    "            pred = net(imgbatch)\n",
    "            loss = criterion(pred, labelbatch)\n",
    "            epoch_loss += loss.item()\n",
    "#             writer.add_scalar('Loss/train', loss.item(), global_step)\n",
    "            print('step:{}, loss/train: {}'.format(global_step,loss.item()))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_value_(net.parameters(), 0.1)\n",
    "            optimizer.step()\n",
    "\n",
    "            global_step += 1\n",
    "            if ( global_step % max(n_train//(10*batch_size),1) )==0:\n",
    "#                 samp = np.random.choice(n_test,20)\n",
    "                samp = 0\n",
    "                pred = net(testsub[samp,:,:,:].view(1,1,320,320))\n",
    "                testloss = criterion(pred, testfull[samp,:,:,:].view(pred.shape))\n",
    "                print('step:{}, loss/test/img {}: {}'.format(global_step,samp,testloss))\n",
    "                if net.n_channels == 2:\n",
    "                    fig, axs = plt.subplots(1, 2,figsize=(10,16))\n",
    "                    predimg = torch.fft.fftshift( torch.abs(pred[0,0,:,:] + 1j * pred[0,1,:,:]) )\n",
    "                    hd1 = axs[0].imshow(torch.log(predimg.detach()))\n",
    "                elif net.n_channels == 1:\n",
    "                    fig, axs = plt.subplots(1, 3,figsize=(8,10))\n",
    "                    predimg = pred[0,0,:,:]\n",
    "                    hd1 = axs[0].imshow(predimg.detach())\n",
    "                axs[0].set_title('pred.') # predicted image by Unet\n",
    "                divider = make_axes_locatable(axs[0]); cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "                fig.colorbar(hd1,cax=cax)\n",
    "                \n",
    "                if net.n_channels == 2:\n",
    "                    gtimg = torch.fft.fftshift( torch.abs(testfull[samp,0,:,:]+1j*testfull[samp,1,:,:]) )\n",
    "                    hd2 = axs[1].imshow(torch.log(gtimg))\n",
    "                elif net.n_channels == 1:\n",
    "                    gtimg = testfull[samp,0,:,:]\n",
    "                    hd2 = axs[1].imshow(gtimg)\n",
    "                    hd3 = axs[2].imshow(torch.squeeze(testsub[samp,:,:,:]))\n",
    "                    axs[2].set_title('ifft')\n",
    "                    divider = make_axes_locatable(axs[2]); cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "                    fig.colorbar(hd3,cax=cax)\n",
    "                axs[1].set_title('g.t.') # ground truth image\n",
    "                divider = make_axes_locatable(axs[1]); cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "                fig.colorbar(hd2,cax=cax)           \n",
    "                plt.show()\n",
    "#         print('step:{}, epoch loss/train: {}'.format(global_step,epoch_loss))\n",
    "        if save_cp:\n",
    "            dir_checkpoint = '/home/huangz78/mri/checkpoints/'\n",
    "            try:\n",
    "                os.mkdir(dir_checkpoint)\n",
    "                print('Created checkpoint directory')\n",
    "#                 logging.info('Created checkpoint directory')\n",
    "            except OSError:\n",
    "                pass\n",
    "\n",
    "            torch.save({'model_state_dict': net.state_dict()}, dir_checkpoint + 'unet_' + str(net.n_channels) +'.pth')\n",
    "#                         }, dir_checkpoint + f'CP_epoch{epoch + 1}.pth')\n",
    "            print(f'\\t Checkpoint saved after epoch {epoch + 1}!')\n",
    "        train_loss.append(epoch_loss)\n",
    "        pred = net(testsub)\n",
    "        testloss = criterion(pred, testfull)\n",
    "        test_loss.append(testloss.item())\n",
    "        scheduler.step()\n",
    "    \n",
    "    return train_loss, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412ca33d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sys.path.insert(0,'/home/huangz78/mri/unet/')\n",
    "import unet_model\n",
    "reload(unet_model)\n",
    "from unet_model import UNet\n",
    "\n",
    "unet = UNet(n_channels=1,n_classes=1,bilinear=True,skip=False)\n",
    "train_loss,test_loss = train_net(unet,epochs=50,batch_size=5,\\\n",
    "                                 lr=5e-4,lr_weight_decay=0,lr_momentum=0,\\\n",
    "                                 save_cp=True)\n",
    "fig, ax1 = plt.subplots()\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('iter')\n",
    "ax1.set_ylabel('train', color=color)\n",
    "ax1.plot(train_loss, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.set_yscale('log')\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('loss', color=color)\n",
    "ax2.plot(test_loss, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.set_yscale('log')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09355439",
   "metadata": {},
   "source": [
    "## scratches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f13c76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## re-find what mask is:\n",
    "# kplot(test_y[0,:,:],log=True,roll=False)\n",
    "mask = np.zeros((320))\n",
    "mask[np.abs(test_y[0,:,0])!=0]=1\n",
    "np.fft.fftshift(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3c1b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNET =  UNet(n_channels=2,n_classes=2,bilinear=False)\n",
    "checkpoint = torch.load('/home/huangz78/mri/checkpoints/unet.pth')\n",
    "UNET.load_state_dict(checkpoint['model_state_dict'])\n",
    "UNET.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
