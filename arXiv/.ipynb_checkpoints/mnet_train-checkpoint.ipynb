{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83f4b303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.fft as F\n",
    "from importlib import reload\n",
    "from torch.nn.functional import relu\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Func\n",
    "import torch.optim as optim\n",
    "import utils\n",
    "import mnet\n",
    "from mnet import MNet\n",
    "from matplotlib import pyplot as plt\n",
    "from utils import kplot,mask_naiveRand,mask_filter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41748306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 2, 320, 320])\n",
      "torch.Size([199, 320, 320])\n"
     ]
    }
   ],
   "source": [
    "imgs = torch.tensor( np.load('/home/huangz78/data/data_gt.npz')['imgdata'] ).permute(2,0,1)\n",
    "base = 24\n",
    "mask_lf,_,_ = mask_naiveRand(imgs.shape[1],fix=base,other=0,roll=True)\n",
    "\n",
    "yfulls = torch.zeros((imgs.shape[0],2,imgs.shape[1],imgs.shape[2]),dtype=torch.float)\n",
    "ys     = torch.zeros((imgs.shape[0],2,imgs.shape[1],imgs.shape[2]),dtype=torch.float)\n",
    "xs     = torch.zeros(imgs.shape)\n",
    "for ind in range(imgs.shape[0]):\n",
    "    imgs[ind,:,:] = imgs[ind,:,:]/torch.max(torch.abs(imgs[ind,:,:]))\n",
    "    y = torch.fft.fftshift(F.fftn(imgs[ind,:,:],dim=(0,1),norm='ortho'))\n",
    "    ysub = torch.zeros(y.shape,dtype=y.dtype)\n",
    "    ysub[mask_lf==1,:] = y[mask_lf==1,:]\n",
    "    xs[ind,:,:] = torch.abs(F.ifftn(torch.fft.ifftshift(ysub),dim=(0,1),norm='ortho')) \n",
    "    \n",
    "    yfulls[ind,0,:,:] = torch.real(y)\n",
    "    yfulls[ind,1,:,:] = torch.imag(y)\n",
    "    ys[ind,:,mask_lf==1,:] = yfulls[ind,:,mask_lf==1,:]\n",
    "      \n",
    "labels = torch.tensor( np.load('/home/huangz78/data/data_gt_greedymask.npz')['mask'].T ) \n",
    "print(ys.shape)\n",
    "print(xs.shape)\n",
    "# labels are already rolled\n",
    "\n",
    "imgNum = imgs.shape[0]\n",
    "traininds, testinds = train_test_split(np.arange(imgNum),random_state=0,shuffle=True,train_size=round(imgNum*0.8))\n",
    "test_total  = testinds.size\n",
    "\n",
    "traindata   = ys[traininds,:,:,:]\n",
    "valdata     = ys[testinds[0:test_total//2],:,:,:]\n",
    "testdata    = ys[testinds[test_total//2:],:,:,:]\n",
    "\n",
    "trainlabels = mask_filter(labels[traininds,:],base=base)\n",
    "vallabels   = mask_filter(labels[testinds[0:test_total//2],:],base=base)\n",
    "testlabels  = mask_filter(labels[testinds[test_total//2:],:],base=base)\n",
    "\n",
    "# traindata   = xs[traininds,:,:]\n",
    "# valdata     = xs[testinds[0:test_total//2],:,:]\n",
    "# traindata = traindata.view(traindata.shape[0],1,traindata.shape[1],traindata.shape[2])\n",
    "# valdata = valdata.view(valdata.shape[0],1,valdata.shape[1],valdata.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b95c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(xs[0,:,:])\n",
    "# plt.colorbar()\n",
    "# plt.show()\n",
    "# kplot(mask_lf)\n",
    "# kplot(xs[0,:,:],log=False,roll=False)\n",
    "kplot(torch.abs(ys[0,0,:,:]),log=True,roll=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46f35c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_binarize(M,threshold=0.5):\n",
    "    sigmoid = nn.Sigmoid()\n",
    "    mask = sigmoid(M)\n",
    "    mask_pred = torch.ones_like(mask)\n",
    "    for ind in range(M.shape[0]):\n",
    "        mask_pred[ind,mask[ind,:]<=threshold] = 0\n",
    "    return mask_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7090ef7",
   "metadata": {},
   "source": [
    "# mnet test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86b8f803",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MNet(\n",
       "  (inc): DoubleConv(\n",
       "    (double_conv): Sequential(\n",
       "      (0): Conv2d(2, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (down1): Down(\n",
       "    (maxpool_conv): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down2): Down(\n",
       "    (maxpool_conv): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down3): Down(\n",
       "    (maxpool_conv): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down4): Down(\n",
       "    (maxpool_conv): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (outc): OutConv(\n",
       "    (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (fc1): Linear(in_features=2048, out_features=1420, bias=True)\n",
       "  (fc2): Linear(in_features=1420, out_features=792, bias=True)\n",
       "  (fc3): Linear(in_features=792, out_features=478, bias=True)\n",
       "  (fc4): Linear(in_features=478, out_features=296, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(mnet)\n",
    "from mnet import MNet\n",
    "net = MNet(beta=1,in_channels=traindata.shape[1],out_size=trainlabels.shape[1],\\\n",
    "                   imgsize=(traindata.shape[2],traindata.shape[3]),poolk=3)\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9667390",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnet is successfully loaded from the file: /home/huangz78/checkpoints/mnet.pth\n"
     ]
    }
   ],
   "source": [
    "dir_checkpoint = '/home/huangz78/checkpoints/'\n",
    "net = MNet(beta=1,in_channels=traindata.shape[1],out_size=trainlabels.shape[1],\\\n",
    "                   imgsize=(traindata.shape[2],traindata.shape[3]),poolk=3)\n",
    "dictpath   = '/home/huangz78/checkpoints/mnet.pth'\n",
    "checkpoint = torch.load(dictpath)\n",
    "net.load_state_dict(checkpoint['model_state_dict'])\n",
    "net.eval()\n",
    "print('mnet is successfully loaded from the file: ' + dictpath )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6381bef9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testdata shape is:  torch.Size([20, 2, 320, 320])\n",
      "tensor(10.)\n",
      "tensor([[ 82.4977,  62.1520,  93.7176,  68.0966,  94.3024,  97.3108,  84.8475,\n",
      "          89.7607,  66.7971,  93.3319,  87.9182,  68.0016,  94.7396,  84.5460,\n",
      "          74.8958,  84.4100,  27.3483,  75.4741,  86.1479,  70.1038,  89.7006,\n",
      "          94.9551,  92.7310,  81.2937, 100.6270,  96.2222,  84.2758,  20.6120,\n",
      "          84.1709,  82.3289,  79.9693,  76.8130,  93.9968,  89.7806,  90.6824,\n",
      "          76.4415,  89.1296,  81.1975,  94.2939,  82.8559,  86.6993,  92.6577,\n",
      "          82.2602,  83.8862,  83.2109,  94.9764,  84.9684,  27.0864,  86.0837,\n",
      "          85.2859,  20.8679,  24.3479,  86.4053,  71.3624,  92.1977,  68.8290,\n",
      "         103.2846,  79.7121,  95.3768,  77.6370,  88.4875,  85.6561,  79.9100,\n",
      "          68.3325,  29.2137,  18.2932,  83.2812, 102.5323,  82.4363,  79.2711,\n",
      "          85.2263,  78.9588,  91.9754,  -4.2725,  20.7160,  -5.1994,  79.1838,\n",
      "          25.7344,  -3.9354,  74.9997,  20.4797,  89.8759, 110.5640,  64.7741,\n",
      "          29.0969,  75.6442,  96.8913,  80.7550,  85.4196,  69.4755,  98.2787,\n",
      "          96.2849,  81.3275,  95.9736, 106.2653,  94.1134,  90.1527,  21.4299,\n",
      "          90.2501,   3.9464, -10.9244,  98.6425,  -3.1874, -27.3286,  -8.6181,\n",
      "         -11.0398,  87.6752,  -4.8868,  -5.8062, -16.3404, -30.5845, -91.4329,\n",
      "          10.8269,   6.4934, -26.9005,  89.1393, -84.7340,  11.8474,   5.2899,\n",
      "         -22.8004,   3.4993, -24.5013,   4.6631,   4.9910, -22.0938, -20.6215,\n",
      "         -17.5822, -20.2015, -23.3911, -21.4060, -21.9227, -86.0965, -94.2308,\n",
      "         -88.7769, -92.5790, -91.6145, -75.2772, -87.6677, -95.4221, -81.9574,\n",
      "         -66.7047, -84.3969, -80.6548, -72.9714, -85.6221, -75.9406, -82.6629,\n",
      "         -78.4879, -78.5850, -79.6139, -94.3695, -81.2052, -88.8262, -86.7667,\n",
      "         -76.8872, -88.2598, -95.0447, -83.0326, -80.4952, -97.8145, -80.9472,\n",
      "         -72.3009, -97.9147, -98.0651, -92.5827, -92.9617, -24.6938, -24.9242,\n",
      "         -20.6106, -22.9574, -24.5849, -18.1123, -29.2185,   5.1840,   5.9091,\n",
      "         -26.1137,   8.1500,   3.0772,   6.1587,   8.8414, -88.7259,  81.0422,\n",
      "         -20.2732,   6.3234,  11.9735, -82.2120, -25.5322, -19.3543,  -3.8188,\n",
      "          -4.9734,  79.3056,  -7.8531, -12.6987, -21.5304,  -7.7712,  78.6655,\n",
      "         -11.8322,  76.0493,  92.8670,  27.7156,  79.2282,  74.3476,  75.0367,\n",
      "          83.3365,  75.3252,  89.1421,  85.1491,  75.3869,  98.8167,  89.9636,\n",
      "          82.8873, 100.8819,  28.9969,  95.0261,  96.7193,  77.4817,  23.4362,\n",
      "          70.3041,  21.5088,  26.7724,  22.9500,  23.1197,  30.3990,  -4.0211,\n",
      "          90.0103,  77.5556,  72.0439,  79.3167,  69.5448,  85.3103,  83.2446,\n",
      "          21.8943,  31.1924,  89.5468,  91.6334,  81.2252,  90.4219,  98.1608,\n",
      "          95.4961,  73.2068,  90.6129,  83.7792,  94.8982,  73.1593,  65.2720,\n",
      "          21.8306,  19.1567,  80.6101,  75.2843,  19.9161, 102.0215,  98.8541,\n",
      "          71.8051,  93.2282,  71.0137,  92.8766,  94.7908,  92.3819,  86.3531,\n",
      "          97.5554,  73.7630,  64.8144,  87.2164,  75.2595,  78.2793, 102.1127,\n",
      "         111.7603,  72.5545,  92.7113,  29.5983,  88.2126,  95.7911,  82.8235,\n",
      "         105.3397,  74.1676,  84.4640,  97.7885,  85.4676,  65.6037,  79.3176,\n",
      "          25.0148,  84.4242,  60.0032, 100.2210,  83.6300,  97.7176, 102.6666,\n",
      "          77.5968,  85.4943,  88.9092,  75.0561,  83.1084,  78.9846,  92.8642,\n",
      "         104.3417,  77.9346]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "print('testdata shape is: ', testdata.shape)\n",
    "\n",
    "imgind = 18\n",
    "testimg = testdata[imgind,:,:,:]\n",
    "output_1 = net(testimg.view(-1,2,320,320))\n",
    "binary_1 = sigmoid_binarize(output_1)[0,:]\n",
    "\n",
    "imgind = 3\n",
    "testimg = testdata[imgind,:,:,:]\n",
    "output_2 = net(testimg.view(-1,2,320,320))\n",
    "binary_2 = sigmoid_binarize(output_2)[0,:]\n",
    "\n",
    "print(torch.sum(torch.abs(binary_1-binary_2)))\n",
    "print(output_1 - output_2)\n",
    "# sigmoid_binarize(output)[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab7cbe7",
   "metadata": {},
   "source": [
    "# mnet train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65a62ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainMNet(trainimgs,trainlabels,testimgs,testlabels,\\\n",
    "              epochs=20,batchsize=5,positive_weight=1,\\\n",
    "              lr=0.01,lr_weight_decay=1e-8,opt_momentum=0,\\\n",
    "              lr_s_stepsize=5,lr_s_gamma=0.5,\\\n",
    "              model=None,save_cp=True,threshold=0.5,\\\n",
    "              beta=1,poolk=3,datatype=torch.float,print_every=10):\n",
    "    '''\n",
    "    trainimgs    : train data, with dimension (num. of imgs,layer, height, width)\n",
    "    '''\n",
    "    \n",
    "    train_shape  = trainimgs.shape; test_shape = testimgs.shape \n",
    "    trainimgs    = torch.tensor(trainimgs,dtype=datatype)\n",
    "    trainlabels  = torch.tensor(trainlabels,dtype=datatype)\n",
    "    testimgs     = torch.tensor(testimgs,dtype=datatype)\n",
    "    testlabels   = torch.tensor(testlabels,dtype=datatype)    \n",
    "    dir_checkpoint = '/home/huangz78/checkpoints/'\n",
    "    # input images are assumed to be normalized\n",
    "    \n",
    "    if model is None:\n",
    "        net = MNet(beta=beta,in_channels=train_shape[1],out_size=trainlabels.shape[1],\\\n",
    "                   imgsize=(train_shape[2],train_shape[3]),poolk=poolk)\n",
    "    else:\n",
    "        net = model\n",
    "#     optimizer = optim.RMSprop(net.parameters(), lr=lr, weight_decay=lr_weight_decay, momentum=opt_momentum)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=lr_weight_decay, amsgrad=False)\n",
    "#     scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=lr_s_stepsize,factor=lr_s_gamma)\n",
    "    pos_weight = torch.ones([trainlabels.shape[1]]) * positive_weight # weight assigned to positive labels \n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    test_criterion = nn.BCELoss()\n",
    "    \n",
    "    epoch_loss        = np.full((epochs),np.nan)\n",
    "    precision_train   = list([]); recall_train = list([])\n",
    "    precision_history = np.full((epochs),np.nan); recall_history = np.full((epochs),np.nan)\n",
    "    net.train()\n",
    "    for epoch in range(epochs):\n",
    "        batch_init = 0; step_count = 0\n",
    "        while batch_init < train_shape[0]:\n",
    "            batch = np.arange(batch_init,min(batch_init+batchsize,train_shape[0]))\n",
    "            imgbatch = trainimgs[batch,:,:,:] # maybe shuffling?\n",
    "            batchlabels = trainlabels[batch,:]\n",
    "            mask_pred   = net(imgbatch)\n",
    "            train_loss  = criterion(mask_pred,batchlabels)\n",
    "            batch_init += batchsize; step_count += 1\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            if (step_count%print_every)==0:\n",
    "                print('[{}/{}][{}] train batch loss {}'.format(epoch+1,epochs,step_count,train_loss.item()))\n",
    "                precision_train.append(\\\n",
    "                    precision_score(torch.flatten(batchlabels),\\\n",
    "                                    torch.flatten(sigmoid_binarize(mask_pred,threshold=threshold))) ) \n",
    "                recall_train.append(\\\n",
    "                    recall_score(torch.flatten(batchlabels),\\\n",
    "                                 torch.flatten(sigmoid_binarize(mask_pred,threshold=threshold))) )\n",
    "                print('[{}/{}][{}] precision {}, recall {}'.format(epoch+1,epochs,step_count,precision_train[-1],recall_train[-1]))\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            mask_test = sigmoid_binarize(net(testimgs),threshold=threshold)\n",
    "            test_loss = test_criterion(mask_test,testlabels)\n",
    "            net.train()\n",
    "#             scheduler.step(test_loss)\n",
    "            epoch_loss[epoch] = test_loss.item()\n",
    "            precision_history[epoch] = precision_score(torch.flatten(testlabels),torch.flatten(mask_test))\n",
    "            recall_history[epoch] = recall_score(torch.flatten(testlabels),torch.flatten(mask_test))\n",
    "            print('\\t [{}/{}] validation loss {} '.format(epoch+1,epochs,test_loss.item()))\n",
    "            print('\\t [{}/{}] precision {} '.format(epoch+1,epochs,precision_history[epoch]))\n",
    "            print('\\t [{}/{}] recall    {} '.format(epoch+1,epochs,recall_history[epoch]))\n",
    "        if save_cp:\n",
    "            try:\n",
    "                os.mkdir(dir_checkpoint)\n",
    "                print('Created checkpoint directory')\n",
    "            except OSError:\n",
    "                pass\n",
    "            torch.save({'model_state_dict': net.state_dict()}, dir_checkpoint + 'mnet.pth')\n",
    "#                         'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                         'epoch': epoch,\n",
    "#                         'threshold':threshold\n",
    "#                         }, dir_checkpoint + 'mnet.pth')\n",
    "#                         }, dir_checkpoint + f'CP_epoch{epoch + 1}.pth')\n",
    "            print(f'\\t Checkpoint saved after epoch {epoch + 1}!')\n",
    "            np.savez(dir_checkpoint+'epoch_loss.npz', loss=epoch_loss,\\\n",
    "                     precision_train=precision_train,recall_train=recall_train,\\\n",
    "                     precision_test=precision_history,recall_test=recall_history)\n",
    "        \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "990f5fce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/pyenv/lib/python3.7/site-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if sys.path[0] == '':\n",
      "/opt/anaconda/envs/pyenv/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  del sys.path[0]\n",
      "/opt/anaconda/envs/pyenv/lib/python3.7/site-packages/ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/opt/anaconda/envs/pyenv/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/400][10] train batch loss 0.5894767045974731\n",
      "[1/400][10] precision 0.5548902195608783, recall 0.695\n",
      "[1/400][20] train batch loss 0.36093318462371826\n",
      "[1/400][20] precision 0.7972972972972973, recall 0.7375\n",
      "[1/400][30] train batch loss 0.3093498945236206\n",
      "[1/400][30] precision 0.8277777777777777, recall 0.745\n",
      "\t [1/400] test loss 16.199323654174805 \n",
      "\t [1/400] precision 0.7230340988169798 \n",
      "\t [1/400] recall    0.649375 \n",
      "\t Checkpoint saved after epoch 1!\n",
      "[2/400][10] train batch loss 0.3324435353279114\n",
      "[2/400][10] precision 0.7943661971830986, recall 0.705\n",
      "[2/400][20] train batch loss 0.3015788197517395\n",
      "[2/400][20] precision 0.7972602739726027, recall 0.7275\n",
      "[2/400][30] train batch loss 0.2812899947166443\n",
      "[2/400][30] precision 0.808, recall 0.7575\n",
      "\t [2/400] test loss 16.233108520507812 \n",
      "\t [2/400] precision 0.7157326130992573 \n",
      "\t [2/400] recall    0.6625 \n",
      "\t Checkpoint saved after epoch 2!\n",
      "[3/400][10] train batch loss 0.3251744210720062\n",
      "[3/400][10] precision 0.7896103896103897, recall 0.76\n",
      "[3/400][20] train batch loss 0.2970331907272339\n",
      "[3/400][20] precision 0.8081081081081081, recall 0.7475\n",
      "[3/400][30] train batch loss 0.27133437991142273\n",
      "[3/400][30] precision 0.8135135135135135, recall 0.7525\n",
      "\t [3/400] test loss 16.182432174682617 \n",
      "\t [3/400] precision 0.7168918918918918 \n",
      "\t [3/400] recall    0.663125 \n",
      "\t Checkpoint saved after epoch 3!\n",
      "[4/400][10] train batch loss 0.3233054578304291\n",
      "[4/400][10] precision 0.7973333333333333, recall 0.7475\n",
      "[4/400][20] train batch loss 0.2974812686443329\n",
      "[4/400][20] precision 0.8164383561643835, recall 0.745\n",
      "[4/400][30] train batch loss 0.27748265862464905\n",
      "[4/400][30] precision 0.8135135135135135, recall 0.7525\n",
      "\t [4/400] test loss 16.131755828857422 \n",
      "\t [4/400] precision 0.7174645987862441 \n",
      "\t [4/400] recall    0.665 \n",
      "\t Checkpoint saved after epoch 4!\n",
      "[5/400][10] train batch loss 0.2989172041416168\n",
      "[5/400][10] precision 0.7947368421052632, recall 0.755\n",
      "[5/400][20] train batch loss 0.2921128571033478\n",
      "[5/400][20] precision 0.8078947368421052, recall 0.7675\n",
      "[5/400][30] train batch loss 0.2755354940891266\n",
      "[5/400][30] precision 0.7994722955145118, recall 0.7575\n",
      "\t [5/400] test loss 16.25 \n",
      "\t [5/400] precision 0.7126666666666667 \n",
      "\t [5/400] recall    0.668125 \n",
      "\t Checkpoint saved after epoch 5!\n",
      "[6/400][10] train batch loss 0.29932260513305664\n",
      "[6/400][10] precision 0.7922077922077922, recall 0.7625\n",
      "[6/400][20] train batch loss 0.2839164137840271\n",
      "[6/400][20] precision 0.816, recall 0.765\n",
      "[6/400][30] train batch loss 0.2745816111564636\n",
      "[6/400][30] precision 0.8053333333333333, recall 0.755\n",
      "\t [6/400] test loss 16.25 \n",
      "\t [6/400] precision 0.7126666666666667 \n",
      "\t [6/400] recall    0.668125 \n",
      "\t Checkpoint saved after epoch 6!\n",
      "[7/400][10] train batch loss 0.2986537218093872\n",
      "[7/400][10] precision 0.7922077922077922, recall 0.7625\n",
      "[7/400][20] train batch loss 0.2812159061431885\n",
      "[7/400][20] precision 0.8148148148148148, recall 0.77\n",
      "[7/400][30] train batch loss 0.27735230326652527\n",
      "[7/400][30] precision 0.8053333333333333, recall 0.755\n",
      "\t [7/400] test loss 16.16554069519043 \n",
      "\t [7/400] precision 0.7159167226326394 \n",
      "\t [7/400] recall    0.66625 \n",
      "\t Checkpoint saved after epoch 7!\n",
      "[8/400][10] train batch loss 0.3182021975517273\n",
      "[8/400][10] precision 0.7947368421052632, recall 0.755\n",
      "[8/400][20] train batch loss 0.2897821068763733\n",
      "[8/400][20] precision 0.8138297872340425, recall 0.765\n",
      "[8/400][30] train batch loss 0.2730740010738373\n",
      "[8/400][30] precision 0.7989556135770235, recall 0.765\n",
      "\t [8/400] test loss 16.131755828857422 \n",
      "\t [8/400] precision 0.7174645987862441 \n",
      "\t [8/400] recall    0.665 \n",
      "\t Checkpoint saved after epoch 8!\n",
      "[9/400][10] train batch loss 0.2907775044441223\n",
      "[9/400][10] precision 0.7922077922077922, recall 0.7625\n",
      "[9/400][20] train batch loss 0.2773546278476715\n",
      "[9/400][20] precision 0.8138297872340425, recall 0.765\n",
      "[9/400][30] train batch loss 0.2733568251132965\n",
      "[9/400][30] precision 0.7989556135770235, recall 0.765\n",
      "\t [9/400] test loss 16.030405044555664 \n",
      "\t [9/400] precision 0.7221843003412969 \n",
      "\t [9/400] recall    0.66125 \n",
      "\t Checkpoint saved after epoch 9!\n",
      "[10/400][10] train batch loss 0.30391964316368103\n",
      "[10/400][10] precision 0.7922077922077922, recall 0.7625\n",
      "[10/400][20] train batch loss 0.2743109464645386\n",
      "[10/400][20] precision 0.8121693121693122, recall 0.7675\n",
      "[10/400][30] train batch loss 0.26741093397140503\n",
      "[10/400][30] precision 0.8085106382978723, recall 0.76\n",
      "\t [10/400] test loss 16.114864349365234 \n",
      "\t [10/400] precision 0.7153333333333334 \n",
      "\t [10/400] recall    0.670625 \n",
      "\t Checkpoint saved after epoch 10!\n",
      "[11/400][10] train batch loss 0.2823280990123749\n",
      "[11/400][10] precision 0.7948051948051948, recall 0.765\n",
      "[11/400][20] train batch loss 0.2719953656196594\n",
      "[11/400][20] precision 0.8036649214659686, recall 0.7675\n",
      "[11/400][30] train batch loss 0.2709061801433563\n",
      "[11/400][30] precision 0.8145161290322581, recall 0.7575\n",
      "\t [11/400] test loss 15.979729652404785 \n",
      "\t [11/400] precision 0.7245879120879121 \n",
      "\t [11/400] recall    0.659375 \n",
      "\t Checkpoint saved after epoch 11!\n",
      "[12/400][10] train batch loss 0.2695620059967041\n",
      "[12/400][10] precision 0.8101604278074866, recall 0.7575\n",
      "[12/400][20] train batch loss 0.29839015007019043\n",
      "[12/400][20] precision 0.8113207547169812, recall 0.7525\n",
      "[12/400][30] train batch loss 0.27320241928100586\n",
      "[12/400][30] precision 0.841225626740947, recall 0.755\n",
      "\t [12/400] test loss 16.486486434936523 \n",
      "\t [12/400] precision 0.7002567394094994 \n",
      "\t [12/400] recall    0.681875 \n",
      "\t Checkpoint saved after epoch 12!\n",
      "[13/400][10] train batch loss 0.28282269835472107\n",
      "[13/400][10] precision 0.8236914600550964, recall 0.7475\n",
      "[13/400][20] train batch loss 0.2706769108772278\n",
      "[13/400][20] precision 0.8439306358381503, recall 0.73\n",
      "[13/400][30] train batch loss 0.26465174555778503\n",
      "[13/400][30] precision 0.8342541436464088, recall 0.755\n",
      "\t [13/400] test loss 16.131755828857422 \n",
      "\t [13/400] precision 0.7467482785003825 \n",
      "\t [13/400] recall    0.61 \n",
      "\t Checkpoint saved after epoch 13!\n",
      "[14/400][10] train batch loss 0.27199599146842957\n",
      "[14/400][10] precision 0.8236914600550964, recall 0.7475\n",
      "[14/400][20] train batch loss 0.27874523401260376\n",
      "[14/400][20] precision 0.8454545454545455, recall 0.6975\n",
      "[14/400][30] train batch loss 0.27195805311203003\n",
      "[14/400][30] precision 0.8459302325581395, recall 0.7275\n",
      "\t [14/400] test loss 15.219594955444336 \n",
      "\t [14/400] precision 0.7442348008385744 \n",
      "\t [14/400] recall    0.665625 \n",
      "\t Checkpoint saved after epoch 14!\n",
      "[15/400][10] train batch loss 0.269214004278183\n",
      "[15/400][10] precision 0.8530259365994236, recall 0.74\n",
      "[15/400][20] train batch loss 0.2623178958892822\n",
      "[15/400][20] precision 0.8723404255319149, recall 0.7175\n",
      "[15/400][30] train batch loss 0.26485151052474976\n",
      "[15/400][30] precision 0.8927444794952681, recall 0.7075\n",
      "\t [15/400] test loss 15.320945739746094 \n",
      "\t [15/400] precision 0.7655172413793103 \n",
      "\t [15/400] recall    0.624375 \n",
      "\t Checkpoint saved after epoch 15!\n",
      "[16/400][10] train batch loss 0.2611630856990814\n",
      "[16/400][10] precision 0.8776758409785933, recall 0.7175\n",
      "[16/400][20] train batch loss 0.2755378186702728\n",
      "[16/400][20] precision 0.9003322259136213, recall 0.6775\n",
      "[16/400][30] train batch loss 0.2644903063774109\n",
      "[16/400][30] precision 0.8699690402476781, recall 0.7025\n",
      "\t [16/400] test loss 14.797297477722168 \n",
      "\t [16/400] precision 0.7836990595611285 \n",
      "\t [16/400] recall    0.625 \n",
      "\t Checkpoint saved after epoch 16!\n",
      "[17/400][10] train batch loss 0.2593971788883209\n",
      "[17/400][10] precision 0.8807339449541285, recall 0.72\n",
      "[17/400][20] train batch loss 0.27677130699157715\n",
      "[17/400][20] precision 0.8836477987421384, recall 0.7025\n",
      "[17/400][30] train batch loss 0.28142523765563965\n",
      "[17/400][30] precision 0.86875, recall 0.695\n",
      "\t [17/400] test loss 16.908782958984375 \n",
      "\t [17/400] precision 0.6849907350216183 \n",
      "\t [17/400] recall    0.693125 \n",
      "\t Checkpoint saved after epoch 17!\n",
      "[18/400][10] train batch loss 0.2688524127006531\n",
      "[18/400][10] precision 0.8776758409785933, recall 0.7175\n",
      "[18/400][20] train batch loss 0.26945778727531433\n",
      "[18/400][20] precision 0.900709219858156, recall 0.635\n",
      "[18/400][30] train batch loss 0.25395467877388\n",
      "[18/400][30] precision 0.8575581395348837, recall 0.7375\n",
      "\t [18/400] test loss 15.912161827087402 \n",
      "\t [18/400] precision 0.7190412782956058 \n",
      "\t [18/400] recall    0.675 \n",
      "\t Checkpoint saved after epoch 18!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19/400][10] train batch loss 0.259626567363739\n",
      "[19/400][10] precision 0.8847352024922118, recall 0.71\n",
      "[19/400][20] train batch loss 0.2683403491973877\n",
      "[19/400][20] precision 0.92, recall 0.6325\n",
      "[19/400][30] train batch loss 0.25307217240333557\n",
      "[19/400][30] precision 0.8571428571428571, recall 0.72\n",
      "\t [19/400] test loss 16.722972869873047 \n",
      "\t [19/400] precision 0.6920654911838791 \n",
      "\t [19/400] recall    0.686875 \n",
      "\t Checkpoint saved after epoch 19!\n",
      "[20/400][10] train batch loss 0.25252798199653625\n",
      "[20/400][10] precision 0.9129032258064517, recall 0.7075\n",
      "[20/400][20] train batch loss 0.2722519040107727\n",
      "[20/400][20] precision 0.9084507042253521, recall 0.645\n",
      "[20/400][30] train batch loss 0.2598465085029602\n",
      "[20/400][30] precision 0.8493975903614458, recall 0.705\n",
      "\t [20/400] test loss 15.996622085571289 \n",
      "\t [20/400] precision 0.7135382603008502 \n",
      "\t [20/400] recall    0.681875 \n",
      "\t Checkpoint saved after epoch 20!\n",
      "[21/400][10] train batch loss 0.2510424554347992\n",
      "[21/400][10] precision 0.9245901639344263, recall 0.705\n",
      "[21/400][20] train batch loss 0.27332425117492676\n",
      "[21/400][20] precision 0.939622641509434, recall 0.6225\n",
      "[21/400][30] train batch loss 0.2557976543903351\n",
      "[21/400][30] precision 0.8926380368098159, recall 0.7275\n",
      "\t [21/400] test loss 15.844594955444336 \n",
      "\t [21/400] precision 0.8476890756302521 \n",
      "\t [21/400] recall    0.504375 \n",
      "\t Checkpoint saved after epoch 21!\n",
      "[22/400][10] train batch loss 0.25215646624565125\n",
      "[22/400][10] precision 0.9012738853503185, recall 0.7075\n",
      "[22/400][20] train batch loss 0.25955718755722046\n",
      "[22/400][20] precision 0.9003436426116839, recall 0.655\n",
      "[22/400][30] train batch loss 0.261984258890152\n",
      "[22/400][30] precision 0.9230769230769231, recall 0.66\n",
      "\t [22/400] test loss 16.824323654174805 \n",
      "\t [22/400] precision 0.6871127633209417 \n",
      "\t [22/400] recall    0.693125 \n",
      "\t Checkpoint saved after epoch 22!\n",
      "[23/400][10] train batch loss 0.2512296140193939\n",
      "[23/400][10] precision 0.9041533546325878, recall 0.7075\n",
      "[23/400][20] train batch loss 0.27855202555656433\n",
      "[23/400][20] precision 0.8768768768768769, recall 0.73\n",
      "[23/400][30] train batch loss 0.2543327808380127\n",
      "[23/400][30] precision 0.830945558739255, recall 0.725\n",
      "\t [23/400] test loss 15.827702522277832 \n",
      "\t [23/400] precision 0.7199734571997346 \n",
      "\t [23/400] recall    0.678125 \n",
      "\t Checkpoint saved after epoch 23!\n",
      "[24/400][10] train batch loss 0.24986498057842255\n",
      "[24/400][10] precision 0.9041533546325878, recall 0.7075\n",
      "[24/400][20] train batch loss 0.2649163007736206\n",
      "[24/400][20] precision 0.8986013986013986, recall 0.6425\n",
      "[24/400][30] train batch loss 0.2481992393732071\n",
      "[24/400][30] precision 0.8708708708708709, recall 0.725\n",
      "\t [24/400] test loss 15.675675392150879 \n",
      "\t [24/400] precision 0.725503355704698 \n",
      "\t [24/400] recall    0.675625 \n",
      "\t Checkpoint saved after epoch 24!\n",
      "[25/400][10] train batch loss 0.24725057184696198\n",
      "[25/400][10] precision 0.9093851132686084, recall 0.7025\n",
      "[25/400][20] train batch loss 0.2653295695781708\n",
      "[25/400][20] precision 0.900990099009901, recall 0.6825\n",
      "[25/400][30] train batch loss 0.2527850568294525\n",
      "[25/400][30] precision 0.8554216867469879, recall 0.71\n",
      "\t [25/400] test loss 15.79391860961914 \n",
      "\t [25/400] precision 0.7209302325581395 \n",
      "\t [25/400] recall    0.678125 \n",
      "\t Checkpoint saved after epoch 25!\n",
      "[26/400][10] train batch loss 0.24978116154670715\n",
      "[26/400][10] precision 0.9215686274509803, recall 0.705\n",
      "[26/400][20] train batch loss 0.2688770592212677\n",
      "[26/400][20] precision 0.9325842696629213, recall 0.6225\n",
      "[26/400][30] train batch loss 0.25538578629493713\n",
      "[26/400][30] precision 0.8945578231292517, recall 0.6575\n",
      "\t [26/400] test loss 15.70945930480957 \n",
      "\t [26/400] precision 0.7272727272727273 \n",
      "\t [26/400] recall    0.67 \n",
      "\t Checkpoint saved after epoch 26!\n",
      "[27/400][10] train batch loss 0.25335291028022766\n",
      "[27/400][10] precision 0.9064516129032258, recall 0.7025\n",
      "[27/400][20] train batch loss 0.2646377980709076\n",
      "[27/400][20] precision 0.9006622516556292, recall 0.68\n",
      "[27/400][30] train batch loss 0.2588629126548767\n",
      "[27/400][30] precision 0.8402366863905325, recall 0.71\n",
      "\t [27/400] test loss 16.114864349365234 \n",
      "\t [27/400] precision 0.7122207621550591 \n",
      "\t [27/400] recall    0.6775 \n",
      "\t Checkpoint saved after epoch 27!\n",
      "[28/400][10] train batch loss 0.24974817037582397\n",
      "[28/400][10] precision 0.9240924092409241, recall 0.7\n",
      "[28/400][20] train batch loss 0.26417067646980286\n",
      "[28/400][20] precision 0.8963210702341137, recall 0.67\n",
      "[28/400][30] train batch loss 0.2563730478286743\n",
      "[28/400][30] precision 0.9303135888501742, recall 0.6675\n",
      "\t [28/400] test loss 15.810811042785645 \n",
      "\t [28/400] precision 0.8079777365491652 \n",
      "\t [28/400] recall    0.544375 \n",
      "\t Checkpoint saved after epoch 28!\n",
      "[29/400][10] train batch loss 0.251726895570755\n",
      "[29/400][10] precision 0.8955696202531646, recall 0.7075\n",
      "[29/400][20] train batch loss 0.26287877559661865\n",
      "[29/400][20] precision 0.8970099667774086, recall 0.675\n",
      "[29/400][30] train batch loss 0.24403542280197144\n",
      "[29/400][30] precision 0.8558558558558559, recall 0.7125\n",
      "\t [29/400] test loss 15.185811042785645 \n",
      "\t [29/400] precision 0.8399612027158099 \n",
      "\t [29/400] recall    0.54125 \n",
      "\t Checkpoint saved after epoch 29!\n",
      "[30/400][10] train batch loss 0.25127553939819336\n",
      "[30/400][10] precision 0.912621359223301, recall 0.705\n",
      "[30/400][20] train batch loss 0.2668537199497223\n",
      "[30/400][20] precision 0.9044117647058824, recall 0.615\n",
      "[30/400][30] train batch loss 0.258894145488739\n",
      "[30/400][30] precision 0.927536231884058, recall 0.64\n",
      "\t [30/400] test loss 15.320945739746094 \n",
      "\t [30/400] precision 0.7434996486296557 \n",
      "\t [30/400] recall    0.66125 \n",
      "\t Checkpoint saved after epoch 30!\n",
      "[31/400][10] train batch loss 0.24464218318462372\n",
      "[31/400][10] precision 0.9185667752442996, recall 0.705\n",
      "[31/400][20] train batch loss 0.26221656799316406\n",
      "[31/400][20] precision 0.9037800687285223, recall 0.6575\n",
      "[31/400][30] train batch loss 0.24634337425231934\n",
      "[31/400][30] precision 0.8935483870967742, recall 0.6925\n",
      "\t [31/400] test loss 14.476351737976074 \n",
      "\t [31/400] precision 0.8881922675026124 \n",
      "\t [31/400] recall    0.53125 \n",
      "\t Checkpoint saved after epoch 31!\n",
      "[32/400][10] train batch loss 0.2471049576997757\n",
      "[32/400][10] precision 0.9215686274509803, recall 0.705\n",
      "[32/400][20] train batch loss 0.2603713870048523\n",
      "[32/400][20] precision 0.9052287581699346, recall 0.6925\n",
      "[32/400][30] train batch loss 0.2629172205924988\n",
      "[32/400][30] precision 0.8532934131736527, recall 0.7125\n",
      "\t [32/400] test loss 15.557432174682617 \n",
      "\t [32/400] precision 0.7637917637917638 \n",
      "\t [32/400] recall    0.614375 \n",
      "\t Checkpoint saved after epoch 32!\n",
      "[33/400][10] train batch loss 0.24476614594459534\n",
      "[33/400][10] precision 0.9235880398671097, recall 0.695\n",
      "[33/400][20] train batch loss 0.2621708810329437\n",
      "[33/400][20] precision 0.897887323943662, recall 0.6375\n",
      "[33/400][30] train batch loss 0.24494045972824097\n",
      "[33/400][30] precision 0.9096989966555183, recall 0.68\n",
      "\t [33/400] test loss 16.030405044555664 \n",
      "\t [33/400] precision 0.7142857142857143 \n",
      "\t [33/400] recall    0.678125 \n",
      "\t Checkpoint saved after epoch 33!\n",
      "[34/400][10] train batch loss 0.24330368638038635\n",
      "[34/400][10] precision 0.9243421052631579, recall 0.7025\n",
      "[34/400][20] train batch loss 0.26107364892959595\n",
      "[34/400][20] precision 0.9039735099337748, recall 0.6825\n",
      "[34/400][30] train batch loss 0.24143703281879425\n",
      "[34/400][30] precision 0.8952380952380953, recall 0.705\n",
      "\t [34/400] test loss 15.152027130126953 \n",
      "\t [34/400] precision 0.77698975571316 \n",
      "\t [34/400] recall    0.61625 \n",
      "\t Checkpoint saved after epoch 34!\n",
      "[35/400][10] train batch loss 0.2435007095336914\n",
      "[35/400][10] precision 0.9185667752442996, recall 0.705\n",
      "[35/400][20] train batch loss 0.26384297013282776\n",
      "[35/400][20] precision 0.890625, recall 0.7125\n",
      "[35/400][30] train batch loss 0.2617451846599579\n",
      "[35/400][30] precision 0.9227941176470589, recall 0.6275\n",
      "\t [35/400] test loss 15.574324607849121 \n",
      "\t [35/400] precision 0.8081818181818182 \n",
      "\t [35/400] recall    0.555625 \n",
      "\t Checkpoint saved after epoch 35!\n",
      "[36/400][10] train batch loss 0.24622155725955963\n",
      "[36/400][10] precision 0.8924050632911392, recall 0.705\n",
      "[36/400][20] train batch loss 0.26342231035232544\n",
      "[36/400][20] precision 0.8986486486486487, recall 0.665\n",
      "[36/400][30] train batch loss 0.26501336693763733\n",
      "[36/400][30] precision 0.827485380116959, recall 0.7075\n",
      "\t [36/400] test loss 15.08445930480957 \n",
      "\t [36/400] precision 0.8131089459698848 \n",
      "\t [36/400] recall    0.57375 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Checkpoint saved after epoch 36!\n",
      "[37/400][10] train batch loss 0.24432921409606934\n",
      "[37/400][10] precision 0.9172185430463576, recall 0.6925\n",
      "[37/400][20] train batch loss 0.26643893122673035\n",
      "[37/400][20] precision 0.8961937716262975, recall 0.6475\n",
      "[37/400][30] train batch loss 0.23798024654388428\n",
      "[37/400][30] precision 0.9368770764119602, recall 0.705\n",
      "\t [37/400] test loss 15.067567825317383 \n",
      "\t [37/400] precision 0.8582995951417004 \n",
      "\t [37/400] recall    0.53 \n",
      "\t Checkpoint saved after epoch 37!\n",
      "[38/400][10] train batch loss 0.24376091361045837\n",
      "[38/400][10] precision 0.9183006535947712, recall 0.7025\n",
      "[38/400][20] train batch loss 0.2737780809402466\n",
      "[38/400][20] precision 0.9072164948453608, recall 0.66\n",
      "[38/400][30] train batch loss 0.2549097537994385\n",
      "[38/400][30] precision 0.947565543071161, recall 0.6325\n",
      "\t [38/400] test loss 16.6047306060791 \n",
      "\t [38/400] precision 0.8493771234428086 \n",
      "\t [38/400] recall    0.46875 \n",
      "\t Checkpoint saved after epoch 38!\n",
      "[39/400][10] train batch loss 0.24620099365711212\n",
      "[39/400][10] precision 0.9123376623376623, recall 0.7025\n",
      "[39/400][20] train batch loss 0.2670891582965851\n",
      "[39/400][20] precision 0.8792569659442725, recall 0.71\n",
      "[39/400][30] train batch loss 0.25061219930648804\n",
      "[39/400][30] precision 0.858433734939759, recall 0.7125\n",
      "\t [39/400] test loss 16.182432174682617 \n",
      "\t [39/400] precision 0.7125827814569536 \n",
      "\t [39/400] recall    0.6725 \n",
      "\t Checkpoint saved after epoch 39!\n",
      "[40/400][10] train batch loss 0.24911713600158691\n",
      "[40/400][10] precision 0.8984126984126984, recall 0.7075\n",
      "[40/400][20] train batch loss 0.26680177450180054\n",
      "[40/400][20] precision 0.9039735099337748, recall 0.6825\n",
      "[40/400][30] train batch loss 0.25007039308547974\n",
      "[40/400][30] precision 0.8811881188118812, recall 0.6675\n",
      "\t [40/400] test loss 15.304054260253906 \n",
      "\t [40/400] precision 0.8230912476722533 \n",
      "\t [40/400] recall    0.5525 \n",
      "\t Checkpoint saved after epoch 40!\n",
      "[41/400][10] train batch loss 0.24307306110858917\n",
      "[41/400][10] precision 0.886435331230284, recall 0.7025\n",
      "[41/400][20] train batch loss 0.2618204653263092\n",
      "[41/400][20] precision 0.898989898989899, recall 0.6675\n",
      "[41/400][30] train batch loss 0.22745293378829956\n",
      "[41/400][30] precision 0.9271523178807947, recall 0.7\n",
      "\t [41/400] test loss 15.557432174682617 \n",
      "\t [41/400] precision 0.8554973821989529 \n",
      "\t [41/400] recall    0.510625 \n",
      "\t Checkpoint saved after epoch 41!\n",
      "[42/400][10] train batch loss 0.24869713187217712\n",
      "[42/400][10] precision 0.9093851132686084, recall 0.7025\n",
      "[42/400][20] train batch loss 0.26467275619506836\n",
      "[42/400][20] precision 0.8986013986013986, recall 0.6425\n",
      "[42/400][30] train batch loss 0.22714704275131226\n",
      "[42/400][30] precision 0.903125, recall 0.7225\n",
      "\t [42/400] test loss 15.895270347595215 \n",
      "\t [42/400] precision 0.7725392886683209 \n",
      "\t [42/400] recall    0.58375 \n",
      "\t Checkpoint saved after epoch 42!\n",
      "[43/400][10] train batch loss 0.24238558113574982\n",
      "[43/400][10] precision 0.9117647058823529, recall 0.6975\n",
      "[43/400][20] train batch loss 0.2596496641635895\n",
      "[43/400][20] precision 0.9061488673139159, recall 0.7\n",
      "[43/400][30] train batch loss 0.2267005294561386\n",
      "[43/400][30] precision 0.9278688524590164, recall 0.7075\n",
      "\t [43/400] test loss 15.371622085571289 \n",
      "\t [43/400] precision 0.7645705521472392 \n",
      "\t [43/400] recall    0.623125 \n",
      "\t Checkpoint saved after epoch 43!\n",
      "[44/400][10] train batch loss 0.2457856833934784\n",
      "[44/400][10] precision 0.9166666666666666, recall 0.6875\n",
      "[44/400][20] train batch loss 0.26382318139076233\n",
      "[44/400][20] precision 0.9035369774919614, recall 0.7025\n",
      "[44/400][30] train batch loss 0.2579793632030487\n",
      "[44/400][30] precision 0.8579881656804734, recall 0.725\n",
      "\t [44/400] test loss 15.45608139038086 \n",
      "\t [44/400] precision 0.8044444444444444 \n",
      "\t [44/400] recall    0.565625 \n",
      "\t Checkpoint saved after epoch 44!\n",
      "[45/400][10] train batch loss 0.24219459295272827\n",
      "[45/400][10] precision 0.9123376623376623, recall 0.7025\n",
      "[45/400][20] train batch loss 0.25995999574661255\n",
      "[45/400][20] precision 0.9018987341772152, recall 0.7125\n",
      "[45/400][30] train batch loss 0.25807055830955505\n",
      "[45/400][30] precision 0.9365671641791045, recall 0.6275\n",
      "\t [45/400] test loss 16.385135650634766 \n",
      "\t [45/400] precision 0.842391304347826 \n",
      "\t [45/400] recall    0.484375 \n",
      "\t Checkpoint saved after epoch 45!\n",
      "[46/400][10] train batch loss 0.23990507423877716\n",
      "[46/400][10] precision 0.9238410596026491, recall 0.6975\n",
      "[46/400][20] train batch loss 0.2669987082481384\n",
      "[46/400][20] precision 0.8853503184713376, recall 0.695\n",
      "[46/400][30] train batch loss 0.23312263190746307\n",
      "[46/400][30] precision 0.8656716417910447, recall 0.725\n",
      "\t [46/400] test loss 16.283782958984375 \n",
      "\t [46/400] precision 0.8225152129817445 \n",
      "\t [46/400] recall    0.506875 \n",
      "\t Checkpoint saved after epoch 46!\n",
      "[47/400][10] train batch loss 0.23742160201072693\n",
      "[47/400][10] precision 0.9180327868852459, recall 0.7\n",
      "[47/400][20] train batch loss 0.2623962461948395\n",
      "[47/400][20] precision 0.8963210702341137, recall 0.67\n",
      "[47/400][30] train batch loss 0.24510876834392548\n",
      "[47/400][30] precision 0.9407665505226481, recall 0.675\n",
      "\t [47/400] test loss 16.283782958984375 \n",
      "\t [47/400] precision 0.8375796178343949 \n",
      "\t [47/400] recall    0.493125 \n",
      "\t Checkpoint saved after epoch 47!\n",
      "[48/400][10] train batch loss 0.25994381308555603\n",
      "[48/400][10] precision 0.9012738853503185, recall 0.7075\n",
      "[48/400][20] train batch loss 0.26198962330818176\n",
      "[48/400][20] precision 0.8955696202531646, recall 0.7075\n",
      "[48/400][30] train batch loss 0.24567997455596924\n",
      "[48/400][30] precision 0.96415770609319, recall 0.6725\n",
      "\t [48/400] test loss 15.574324607849121 \n",
      "\t [48/400] precision 0.7591743119266054 \n",
      "\t [48/400] recall    0.620625 \n",
      "\t Checkpoint saved after epoch 48!\n",
      "[49/400][10] train batch loss 0.24023909866809845\n",
      "[49/400][10] precision 0.907051282051282, recall 0.7075\n",
      "[49/400][20] train batch loss 0.2628088593482971\n",
      "[49/400][20] precision 0.8958990536277602, recall 0.71\n",
      "[49/400][30] train batch loss 0.21823132038116455\n",
      "[49/400][30] precision 0.9208860759493671, recall 0.7275\n",
      "\t [49/400] test loss 15.641891479492188 \n",
      "\t [49/400] precision 0.787542662116041 \n",
      "\t [49/400] recall    0.576875 \n",
      "\t Checkpoint saved after epoch 49!\n",
      "[50/400][10] train batch loss 0.24317201972007751\n",
      "[50/400][10] precision 0.9215686274509803, recall 0.705\n",
      "[50/400][20] train batch loss 0.25582441687583923\n",
      "[50/400][20] precision 0.9015873015873016, recall 0.71\n",
      "[50/400][30] train batch loss 0.26859673857688904\n",
      "[50/400][30] precision 0.838150289017341, recall 0.725\n",
      "\t [50/400] test loss 15.608108520507812 \n",
      "\t [50/400] precision 0.8366533864541833 \n",
      "\t [50/400] recall    0.525 \n",
      "\t Checkpoint saved after epoch 50!\n",
      "[51/400][10] train batch loss 0.2419026494026184\n",
      "[51/400][10] precision 0.9111842105263158, recall 0.6925\n",
      "[51/400][20] train batch loss 0.2633957862854004\n",
      "[51/400][20] precision 0.8980263157894737, recall 0.6825\n",
      "[51/400][30] train batch loss 0.2272302210330963\n",
      "[51/400][30] precision 0.9142857142857143, recall 0.72\n",
      "\t [51/400] test loss 16.858108520507812 \n",
      "\t [51/400] precision 0.7911025145067698 \n",
      "\t [51/400] recall    0.51125 \n",
      "\t Checkpoint saved after epoch 51!\n",
      "[52/400][10] train batch loss 0.23751352727413177\n",
      "[52/400][10] precision 0.8934169278996865, recall 0.7125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-13cb082eb36d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m               \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_weight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_momentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m               \u001b[0mlr_s_stepsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_s_gamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m               save_cp=True)\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m# mnet  = trainMNet(trainpt, trainlb, valpt, vallb,model=model, \\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#               epochs=150, batchsize=5, \\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-1aeebe0e09e2>\u001b[0m in \u001b[0;36mtrainMNet\u001b[0;34m(trainimgs, trainlabels, testimgs, testlabels, epochs, batchsize, positive_weight, lr, lr_weight_decay, opt_momentum, lr_s_stepsize, lr_s_gamma, model, save_cp, threshold, beta, poolk, datatype, print_every)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mbatch_init\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mstep_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep_count\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mprint_every\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/pyenv/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/pyenv/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = None\n",
    "# model = net  \n",
    "# oldshape = traindata[0,:,:,:].shape\n",
    "# num = 3\n",
    "# trainpt = traindata[0:num,:,:,:].view(num,oldshape[0],oldshape[1],oldshape[2])\n",
    "# trainlb = trainlabels[0:num,:].view(num,-1)\n",
    "# valpt = valdata[0:num,:,:,:].view(num,oldshape[0],oldshape[1],oldshape[2])\n",
    "# vallb = vallabels[0:num,:].view(num,-1)\n",
    "mnet  = trainMNet(traindata, trainlabels, valdata, vallabels,model=model, \\\n",
    "              epochs=400, batchsize=5, \\\n",
    "              positive_weight=1,\\\n",
    "              lr=1e-4, lr_weight_decay=0, opt_momentum=0,\\\n",
    "              lr_s_stepsize=5, lr_s_gamma=0.8,\\\n",
    "              save_cp=True)\n",
    "# mnet  = trainMNet(trainpt, trainlb, valpt, vallb,model=model, \\\n",
    "#               epochs=150, batchsize=5, \\\n",
    "#               positive_weight=1,\\\n",
    "#               lr=5e-4, lr_weight_decay=0, opt_momentum=0,\\\n",
    "#               lr_s_stepsize=2, lr_s_gamma=0.5,\\\n",
    "#               threshold=.5, beta=1, save_cp=True,print_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ad6a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "055a50d0",
   "metadata": {},
   "source": [
    "# data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40edf4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = 24\n",
    "mask = torch.tensor( mask_naiveRand(320,fix=base,other=0,roll=False)[0] ,dtype=torch.float )\n",
    "data_under = np.zeros((datashape[2],datashape[0],datashape[1]))\n",
    "for ind in range(data.shape[2]):\n",
    "    img = data[:,:,ind]\n",
    "    img = img/np.max(np.abs(img))\n",
    "    yfull = F.fftn(torch.tensor(img,dtype=torch.float),dim=(0,1),norm='ortho')\n",
    "    ypart = torch.tensordot(torch.diag(mask).to(torch.cfloat) , yfull,dims=([1],[0]))\n",
    "    data_under[ind,:,:] = torch.abs(F.ifftn(ypart,dim=(0,1),norm='ortho'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12da2736",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "imgNum = 199\n",
    "traininds, testinds = train_test_split(np.arange(imgNum),random_state=0,shuffle=True,train_size=round(imgNum*0.8))\n",
    "test_total = testinds.size\n",
    "traindata    = data_under[traininds,:,:]\n",
    "trainlabels  = mask_filter(labels[traininds,:],base=base)\n",
    "valdata      = data_under[testinds[0:test_total//2],:,:]\n",
    "vallabels    = mask_filter(labels[testinds[0:test_total//2],:],base=base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7355cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(traindata.shape)\n",
    "print(trainlabels.shape)\n",
    "print(valdata.shape)\n",
    "print(vallabels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b066ab5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sys.path.insert(0,'/home/huangz78/mri/mnet/')\n",
    "import mnet\n",
    "reload(mnet)\n",
    "from mnet import MNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05627976",
   "metadata": {},
   "source": [
    "#### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1ebf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnet = MNet(out_size=trainlabels.shape[1])\n",
    "# checkpoint = torch.load('/home/huangz78/mri/checkpoints/mnet.pth')\n",
    "# mnet.load_state_dict(checkpoint['model_state_dict'])\n",
    "# print('mnet loaded successfully from : ' + '/home/huangz78/mri/checkpoints/mnet.pth' )\n",
    "# mnet.train()\n",
    "# # print(mnet)\n",
    "trainMNet(traindata,trainlabels, valdata,vallabels,\n",
    "          epochs=60, batchsize=5, \\\n",
    "          lr=1e-4, lr_weight_decay=0,opt_momentum=0,positive_weight=1,\\\n",
    "          lr_s_stepsize=2,lr_s_gamma=0.5,\\\n",
    "          threshold=.5, beta=1,save_cp=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
