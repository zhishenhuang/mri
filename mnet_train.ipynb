{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6cde6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.fft as F\n",
    "from importlib import reload\n",
    "from torch.nn.functional import relu\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Func\n",
    "import torch.optim as optim\n",
    "import utils\n",
    "import logging\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from utils import kplot,mask_naiveRand,mask_filter\n",
    "from mnet import MNet\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18e1dbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_binarize(M,threshold=0.6):\n",
    "    sigmoid = nn.Sigmoid()\n",
    "    mask = sigmoid(M)\n",
    "    mask_pred = torch.ones_like(mask)\n",
    "    mask_pred[mask<=threshold] = 0\n",
    "    return mask_pred\n",
    "    \n",
    "def trainMNet(trainimgs,trainlabels,testimgs,testlabels,\\\n",
    "              epochs=20,batchsize=5,\\\n",
    "              lr=0.01,lr_weight_decay=1e-8,opt_momentum=0,positive_weight=6,\\\n",
    "              lr_s_stepsize=5,lr_s_gamma=0.5,\\\n",
    "              model=None,save_cp=True,threshold=0.5,\\\n",
    "              beta=1,poolk=3,datatype=torch.float,print_every=10):\n",
    "    '''\n",
    "    trainimgs    : train data, with dimension (#imgs,height,width,layer)\n",
    "    '''\n",
    "    \n",
    "    train_shape  = trainimgs.shape; test_shape = testimgs.shape \n",
    "    trainimgs    = torch.tensor(trainimgs,dtype=datatype).view(train_shape[0],-1,train_shape[1],train_shape[2])\n",
    "    trainlabels  = torch.tensor(trainlabels,dtype=datatype)\n",
    "    testimgs     = torch.tensor(testimgs,dtype=datatype).view(test_shape[0],-1,test_shape[1],test_shape[2])\n",
    "    testlabels   = torch.tensor(testlabels ,dtype=datatype)\n",
    "    \n",
    "    train_shape = trainimgs.shape\n",
    "    dir_checkpoint = '/home/huangz78/mri/checkpoints/'\n",
    "    # add normalization for images here\n",
    "    \n",
    "    if model is None:\n",
    "        net = MNet(beta=beta,in_channels=train_shape[1],out_size=trainlabels.shape[1],\\\n",
    "                   imgsize=(train_shape[2],train_shape[3]),poolk=poolk)\n",
    "        optimizer = optim.RMSprop(net.parameters(), lr=lr, weight_decay=lr_weight_decay, momentum=opt_momentum)\n",
    "        epoch_init = 0\n",
    "    else:\n",
    "        net = model[0]\n",
    "        optimizer  = model[1]\n",
    "        epoch_init = model[2] + 1\n",
    "#     criterion = nn.MSELoss()\n",
    "    pos_weight = torch.ones([trainlabels.shape[1]]) * positive_weight # weight assigned to positive labels \n",
    "    criterion  = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    test_criterion = nn.BCELoss()\n",
    "    sigmoid = nn.Sigmoid()\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=lr_s_stepsize,factor=lr_s_gamma)\n",
    "\n",
    "    epoch_loss        = np.full((epochs),np.nan)\n",
    "    precision_history = np.full((epochs),np.nan)\n",
    "    recall_history    = np.full((epochs),np.nan)\n",
    "    for epoch in range(epoch_init,epoch_init + epochs):\n",
    "        batch_init = 0; step_count = 1\n",
    "        while batch_init < train_shape[0]:\n",
    "            batch = np.arange(batch_init,min(batch_init+batchsize,train_shape[0]))\n",
    "            imgbatch = trainimgs[batch,:,:,:] # maybe shuffling?\n",
    "            batchlabels = trainlabels[batch,:]\n",
    "            mask_pred   = net(imgbatch)\n",
    "            train_loss  = criterion(mask_pred,batchlabels)\n",
    "            batch_init += batchsize; step_count += 1\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            if (step_count%print_every)==0:\n",
    "                with torch.no_grad():\n",
    "                    net.eval()\n",
    "                    mask_test = sigmoid_binarize(net(testimgs),threshold=threshold)\n",
    "                    test_loss = test_criterion(mask_test,testlabels)\n",
    "                    print('epoch {} global step {}: train batch loss {}, test loss {} '.format(epoch+1,step_count,train_loss.item(),test_loss.item()))\n",
    "                    net.train()\n",
    "        net.eval()\n",
    "        mask_test = sigmoid_binarize(net(testimgs),threshold=threshold)\n",
    "        test_loss = test_criterion(mask_test,testlabels)\n",
    "        net.train()\n",
    "        scheduler.step(test_loss)\n",
    "        epoch_loss[epoch-epoch_init] = test_loss.item()\n",
    "        precision_history[epoch-epoch_init] = precision_score(torch.flatten(testlabels),torch.flatten(mask_test))\n",
    "        recall_history[epoch-epoch_init] = recall_score(torch.flatten(testlabels),torch.flatten(mask_test))\n",
    "        print('\\t epoch {} end: test loss {} '.format(epoch+1,test_loss.item()))\n",
    "        print('\\t epoch {} end: precision {} '.format(epoch+1,precision_history[epoch-epoch_init]))\n",
    "        print('\\t epoch {} end: recall    {} '.format(epoch+1,recall_history[epoch-epoch_init]))\n",
    "        if save_cp:\n",
    "            try:\n",
    "                os.mkdir(dir_checkpoint)\n",
    "                print('Created checkpoint directory')\n",
    "            except OSError:\n",
    "                pass\n",
    "            torch.save({'model_state_dict': net.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'epoch': epoch,\n",
    "                        'threshold':threshold\n",
    "                        }, dir_checkpoint + 'mnet.pth')\n",
    "#                         }, dir_checkpoint + f'CP_epoch{epoch + 1}.pth')\n",
    "            print(f'\\t Checkpoint saved after epoch {epoch + 1}!')\n",
    "    \n",
    "            np.savez(dir_checkpoint+'epoch_loss.npz', loss=epoch_loss,precision=precision_history,recall=recall_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b17dc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['imgdata']\n",
      "(320, 320, 199)\n",
      "(199, 320)\n"
     ]
    }
   ],
   "source": [
    "data_path = '/home/huangz78/data/'\n",
    "data_imgs = np.load(data_path+'data_gt.npz')\n",
    "data_labels = np.load(data_path+'data_gt_greedymask.npz')\n",
    "print(data_imgs.files)\n",
    "data    = data_imgs['imgdata']\n",
    "labels  = data_labels['mask'].T\n",
    "datashape = data.shape\n",
    "print(datashape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db91839",
   "metadata": {},
   "source": [
    "#### data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b26f118f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-5cbbedb90c5b>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mask = torch.tensor( mask_naiveRand(320,fix=base,other=0,roll=False)[0] ,dtype=torch.float )\n"
     ]
    }
   ],
   "source": [
    "base = 24\n",
    "mask = torch.tensor( mask_naiveRand(320,fix=base,other=0,roll=False)[0] ,dtype=torch.float )\n",
    "data_under = np.zeros((datashape[2],datashape[0],datashape[1]))\n",
    "for ind in range(data.shape[2]):\n",
    "    img = data[:,:,ind]\n",
    "    img = img/np.max(np.abs(img))\n",
    "    yfull = F.fftn(torch.tensor(img,dtype=torch.float),dim=(0,1),norm='ortho')\n",
    "    ypart = torch.tensordot(torch.diag(mask).to(torch.cfloat) , yfull,dims=([1],[0]))\n",
    "    data_under[ind,:,:] = torch.abs(F.ifftn(ypart,dim=(0,1),norm='ortho'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3c70a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "imgNum = 199\n",
    "traininds, testinds = train_test_split(np.arange(imgNum),random_state=0,shuffle=True,train_size=round(imgNum*0.8))\n",
    "test_total = testinds.size\n",
    "traindata    = data_under[traininds,:,:]\n",
    "trainlabels  = mask_filter(labels[traininds,:],base=base)\n",
    "valdata      = data_under[testinds[0:test_total//2],:,:]\n",
    "vallabels    = mask_filter(labels[testinds[0:test_total//2],:],base=base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6171d0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159, 320, 320)\n",
      "(159, 296)\n",
      "(20, 320, 320)\n",
      "(20, 296)\n"
     ]
    }
   ],
   "source": [
    "print(traindata.shape)\n",
    "print(trainlabels.shape)\n",
    "print(valdata.shape)\n",
    "print(vallabels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "559592c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sys.path.insert(0,'/home/huangz78/mri/mnet/')\n",
    "import mnet\n",
    "reload(mnet)\n",
    "from mnet import MNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1451786",
   "metadata": {},
   "source": [
    "#### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fc0c27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 global step 10: train batch loss 0.37502849102020264, test loss 16.08108139038086 \n",
      "epoch 1 global step 20: train batch loss 0.28756147623062134, test loss 16.41891860961914 \n",
      "epoch 1 global step 30: train batch loss 0.27262017130851746, test loss 16.486486434936523 \n",
      "\t epoch 1 end: test loss 16.114864349365234 \n",
      "\t epoch 1 end: precision 0.7307142857142858 \n",
      "\t epoch 1 end: recall    0.639375 \n",
      "\t Checkpoint saved after epoch 1!\n",
      "epoch 2 global step 10: train batch loss 0.29684698581695557, test loss 16.824323654174805 \n",
      "epoch 2 global step 20: train batch loss 0.2700611352920532, test loss 16.25 \n",
      "epoch 2 global step 30: train batch loss 0.2540127635002136, test loss 16.5202693939209 \n",
      "\t epoch 2 end: test loss 16.013513565063477 \n",
      "\t epoch 2 end: precision 0.7318634423897582 \n",
      "\t epoch 2 end: recall    0.643125 \n",
      "\t Checkpoint saved after epoch 2!\n",
      "epoch 3 global step 10: train batch loss 0.2949286699295044, test loss 16.5202693939209 \n",
      "epoch 3 global step 20: train batch loss 0.2671421468257904, test loss 16.216217041015625 \n",
      "epoch 3 global step 30: train batch loss 0.2505347430706024, test loss 16.351350784301758 \n",
      "\t epoch 3 end: test loss 15.726351737976074 \n",
      "\t epoch 3 end: precision 0.7353976073187896 \n",
      "\t epoch 3 end: recall    0.653125 \n",
      "\t Checkpoint saved after epoch 3!\n",
      "epoch 4 global step 10: train batch loss 0.2953341007232666, test loss 16.368244171142578 \n",
      "epoch 4 global step 20: train batch loss 0.26509466767311096, test loss 15.895270347595215 \n",
      "epoch 4 global step 30: train batch loss 0.24958749115467072, test loss 16.25 \n",
      "\t epoch 4 end: test loss 15.70945930480957 \n",
      "\t epoch 4 end: precision 0.7359154929577465 \n",
      "\t epoch 4 end: recall    0.653125 \n",
      "\t Checkpoint saved after epoch 4!\n",
      "epoch 5 global step 10: train batch loss 0.295657753944397, test loss 16.131755828857422 \n",
      "epoch 5 global step 20: train batch loss 0.26395612955093384, test loss 15.675675392150879 \n",
      "epoch 5 global step 30: train batch loss 0.2477385699748993, test loss 16.300676345825195 \n",
      "\t epoch 5 end: test loss 15.810811042785645 \n",
      "\t epoch 5 end: precision 0.7328190743338009 \n",
      "\t epoch 5 end: recall    0.653125 \n",
      "\t Checkpoint saved after epoch 5!\n",
      "epoch 6 global step 10: train batch loss 0.2948029637336731, test loss 16.08108139038086 \n",
      "epoch 6 global step 20: train batch loss 0.26225173473358154, test loss 15.658783912658691 \n",
      "epoch 6 global step 30: train batch loss 0.24458694458007812, test loss 16.266891479492188 \n",
      "\t epoch 6 end: test loss 15.844594955444336 \n",
      "\t epoch 6 end: precision 0.7292243767313019 \n",
      "\t epoch 6 end: recall    0.658125 \n",
      "\t Checkpoint saved after epoch 6!\n",
      "epoch 7 global step 10: train batch loss 0.29282110929489136, test loss 15.962838172912598 \n",
      "epoch 7 global step 20: train batch loss 0.2600853443145752, test loss 15.608108520507812 \n",
      "epoch 7 global step 30: train batch loss 0.2386891096830368, test loss 16.300676345825195 \n",
      "\t epoch 7 end: test loss 15.79391860961914 \n",
      "\t epoch 7 end: precision 0.7285223367697594 \n",
      "\t epoch 7 end: recall    0.6625 \n",
      "\t Checkpoint saved after epoch 7!\n",
      "epoch 8 global step 10: train batch loss 0.28189659118652344, test loss 15.70945930480957 \n",
      "epoch 8 global step 20: train batch loss 0.2509825825691223, test loss 15.574324607849121 \n",
      "epoch 8 global step 30: train batch loss 0.23414656519889832, test loss 15.912161827087402 \n",
      "\t epoch 8 end: test loss 15.726351737976074 \n",
      "\t epoch 8 end: precision 0.7292666209732693 \n",
      "\t epoch 8 end: recall    0.665 \n",
      "\t Checkpoint saved after epoch 8!\n",
      "epoch 9 global step 10: train batch loss 0.2781660854816437, test loss 15.591216087341309 \n",
      "epoch 9 global step 20: train batch loss 0.24343325197696686, test loss 15.08445930480957 \n",
      "epoch 9 global step 30: train batch loss 0.22908827662467957, test loss 15.895270347595215 \n",
      "\t epoch 9 end: test loss 15.692567825317383 \n",
      "\t epoch 9 end: precision 0.7321799307958478 \n",
      "\t epoch 9 end: recall    0.66125 \n",
      "\t Checkpoint saved after epoch 9!\n",
      "epoch 10 global step 10: train batch loss 0.2743394374847412, test loss 15.54054069519043 \n",
      "epoch 10 global step 20: train batch loss 0.24025693535804749, test loss 14.476351737976074 \n",
      "epoch 10 global step 30: train batch loss 0.21965831518173218, test loss 16.013513565063477 \n",
      "\t epoch 10 end: test loss 15.76013469696045 \n",
      "\t epoch 10 end: precision 0.732728541521284 \n",
      "\t epoch 10 end: recall    0.65625 \n",
      "\t Checkpoint saved after epoch 10!\n",
      "epoch 11 global step 10: train batch loss 0.2729652523994446, test loss 15.472972869873047 \n",
      "epoch 11 global step 20: train batch loss 0.23625385761260986, test loss 14.476351737976074 \n",
      "epoch 11 global step 30: train batch loss 0.20988526940345764, test loss 15.979729652404785 \n",
      "\t epoch 11 end: test loss 15.777027130126953 \n",
      "\t epoch 11 end: precision 0.7322175732217573 \n",
      "\t epoch 11 end: recall    0.65625 \n",
      "\t Checkpoint saved after epoch 11!\n",
      "epoch 12 global step 10: train batch loss 0.26586484909057617, test loss 15.371622085571289 \n",
      "epoch 12 global step 20: train batch loss 0.233188197016716, test loss 14.45945930480957 \n",
      "epoch 12 global step 30: train batch loss 0.21307732164859772, test loss 15.979729652404785 \n",
      "\t epoch 12 end: test loss 16.5202693939209 \n",
      "\t epoch 12 end: precision 0.7221428571428572 \n",
      "\t epoch 12 end: recall    0.631875 \n",
      "\t Checkpoint saved after epoch 12!\n",
      "epoch 13 global step 10: train batch loss 0.2576369345188141, test loss 15.101351737976074 \n",
      "epoch 13 global step 20: train batch loss 0.22198942303657532, test loss 15.320945739746094 \n",
      "epoch 13 global step 30: train batch loss 0.1978614181280136, test loss 16.013513565063477 \n",
      "\t epoch 13 end: test loss 15.895270347595215 \n",
      "\t epoch 13 end: precision 0.7325335215243473 \n",
      "\t epoch 13 end: recall    0.64875 \n",
      "\t Checkpoint saved after epoch 13!\n",
      "epoch 14 global step 10: train batch loss 0.24987821280956268, test loss 15.08445930480957 \n",
      "epoch 14 global step 20: train batch loss 0.22164663672447205, test loss 15.236486434936523 \n",
      "epoch 14 global step 30: train batch loss 0.19108140468597412, test loss 15.48986530303955 \n",
      "\t epoch 14 end: test loss 15.557432174682617 \n",
      "\t epoch 14 end: precision 0.7465504720406682 \n",
      "\t epoch 14 end: recall    0.6425 \n",
      "\t Checkpoint saved after epoch 14!\n",
      "epoch 15 global step 10: train batch loss 0.2517661452293396, test loss 14.966216087341309 \n",
      "epoch 15 global step 20: train batch loss 0.21925559639930725, test loss 15.472972869873047 \n",
      "epoch 15 global step 30: train batch loss 0.18907740712165833, test loss 15.658783912658691 \n",
      "\t epoch 15 end: test loss 15.827702522277832 \n",
      "\t epoch 15 end: precision 0.7410909090909091 \n",
      "\t epoch 15 end: recall    0.636875 \n",
      "\t Checkpoint saved after epoch 15!\n",
      "epoch 16 global step 10: train batch loss 0.24059225618839264, test loss 14.814188957214355 \n",
      "epoch 16 global step 20: train batch loss 0.21712972223758698, test loss 15.185811042785645 \n",
      "epoch 16 global step 30: train batch loss 0.1811884492635727, test loss 15.777027130126953 \n",
      "\t epoch 16 end: test loss 15.895270347595215 \n",
      "\t epoch 16 end: precision 0.7318789584799437 \n",
      "\t epoch 16 end: recall    0.65 \n",
      "\t Checkpoint saved after epoch 16!\n",
      "epoch 17 global step 10: train batch loss 0.2345484495162964, test loss 15.08445930480957 \n",
      "epoch 17 global step 20: train batch loss 0.21768853068351746, test loss 15.844594955444336 \n",
      "epoch 17 global step 30: train batch loss 0.1798013150691986, test loss 15.895270347595215 \n",
      "\t epoch 17 end: test loss 16.199323654174805 \n",
      "\t epoch 17 end: precision 0.7297491039426524 \n",
      "\t epoch 17 end: recall    0.63625 \n",
      "\t Checkpoint saved after epoch 17!\n",
      "epoch 18 global step 10: train batch loss 0.23149733245372772, test loss 14.966216087341309 \n",
      "epoch 18 global step 20: train batch loss 0.20999689400196075, test loss 15.337838172912598 \n",
      "epoch 18 global step 30: train batch loss 0.1670372188091278, test loss 15.726351737976074 \n",
      "\t epoch 18 end: test loss 15.70945930480957 \n",
      "\t epoch 18 end: precision 0.738944365192582 \n",
      "\t epoch 18 end: recall    0.6475 \n",
      "\t Checkpoint saved after epoch 18!\n",
      "epoch 19 global step 10: train batch loss 0.2224537432193756, test loss 14.949324607849121 \n",
      "epoch 19 global step 20: train batch loss 0.2071111798286438, test loss 15.79391860961914 \n",
      "epoch 19 global step 30: train batch loss 0.16362860798835754, test loss 15.79391860961914 \n",
      "\t epoch 19 end: test loss 15.777027130126953 \n",
      "\t epoch 19 end: precision 0.739568345323741 \n",
      "\t epoch 19 end: recall    0.6425 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Checkpoint saved after epoch 19!\n",
      "epoch 20 global step 10: train batch loss 0.21977078914642334, test loss 15.236486434936523 \n",
      "epoch 20 global step 20: train batch loss 0.2057623565196991, test loss 15.777027130126953 \n",
      "epoch 20 global step 30: train batch loss 0.16045457124710083, test loss 15.878377914428711 \n",
      "\t epoch 20 end: test loss 15.827702522277832 \n",
      "\t epoch 20 end: precision 0.7366167023554604 \n",
      "\t epoch 20 end: recall    0.645 \n",
      "\t Checkpoint saved after epoch 20!\n",
      "epoch 21 global step 10: train batch loss 0.21565930545330048, test loss 15.287161827087402 \n",
      "epoch 21 global step 20: train batch loss 0.20306776463985443, test loss 15.895270347595215 \n",
      "epoch 21 global step 30: train batch loss 0.15692956745624542, test loss 15.844594955444336 \n",
      "\t epoch 21 end: test loss 15.726351737976074 \n",
      "\t epoch 21 end: precision 0.7387580299785867 \n",
      "\t epoch 21 end: recall    0.646875 \n",
      "\t Checkpoint saved after epoch 21!\n",
      "epoch 22 global step 10: train batch loss 0.21185781061649323, test loss 15.270270347595215 \n",
      "epoch 22 global step 20: train batch loss 0.20098158717155457, test loss 15.844594955444336 \n",
      "epoch 22 global step 30: train batch loss 0.15556272864341736, test loss 15.844594955444336 \n",
      "\t epoch 22 end: test loss 15.878377914428711 \n",
      "\t epoch 22 end: precision 0.7360515021459227 \n",
      "\t epoch 22 end: recall    0.643125 \n",
      "\t Checkpoint saved after epoch 22!\n",
      "epoch 23 global step 10: train batch loss 0.2095588743686676, test loss 15.506756782531738 \n",
      "epoch 23 global step 20: train batch loss 0.19973936676979065, test loss 15.962838172912598 \n",
      "epoch 23 global step 30: train batch loss 0.15420852601528168, test loss 15.878377914428711 \n",
      "\t epoch 23 end: test loss 15.895270347595215 \n",
      "\t epoch 23 end: precision 0.7368799424874192 \n",
      "\t epoch 23 end: recall    0.640625 \n",
      "\t Checkpoint saved after epoch 23!\n",
      "epoch 24 global step 10: train batch loss 0.2069089263677597, test loss 15.70945930480957 \n",
      "epoch 24 global step 20: train batch loss 0.19840870797634125, test loss 15.895270347595215 \n",
      "epoch 24 global step 30: train batch loss 0.15239401161670685, test loss 15.945945739746094 \n",
      "\t epoch 24 end: test loss 15.945945739746094 \n",
      "\t epoch 24 end: precision 0.7363112391930836 \n",
      "\t epoch 24 end: recall    0.63875 \n",
      "\t Checkpoint saved after epoch 24!\n",
      "epoch 25 global step 10: train batch loss 0.20555011928081512, test loss 15.692567825317383 \n",
      "epoch 25 global step 20: train batch loss 0.1976129412651062, test loss 15.945945739746094 \n",
      "epoch 25 global step 30: train batch loss 0.1516232192516327, test loss 15.962838172912598 \n",
      "\t epoch 25 end: test loss 15.929054260253906 \n",
      "\t epoch 25 end: precision 0.7368421052631579 \n",
      "\t epoch 25 end: recall    0.63875 \n",
      "\t Checkpoint saved after epoch 25!\n",
      "epoch 26 global step 10: train batch loss 0.20416969060897827, test loss 15.76013469696045 \n",
      "epoch 26 global step 20: train batch loss 0.19684118032455444, test loss 15.996622085571289 \n",
      "epoch 26 global step 30: train batch loss 0.1508813053369522, test loss 15.996622085571289 \n",
      "\t epoch 26 end: test loss 15.962838172912598 \n",
      "\t epoch 26 end: precision 0.7364620938628159 \n",
      "\t epoch 26 end: recall    0.6375 \n",
      "\t Checkpoint saved after epoch 26!\n",
      "epoch 27 global step 10: train batch loss 0.20252323150634766, test loss 15.895270347595215 \n",
      "epoch 27 global step 20: train batch loss 0.19601009786128998, test loss 16.04729652404785 \n",
      "epoch 27 global step 30: train batch loss 0.14976750314235687, test loss 16.08108139038086 \n",
      "\t epoch 27 end: test loss 16.030405044555664 \n",
      "\t epoch 27 end: precision 0.7350180505415163 \n",
      "\t epoch 27 end: recall    0.63625 \n",
      "\t Checkpoint saved after epoch 27!\n",
      "epoch 28 global step 10: train batch loss 0.2017931491136551, test loss 15.79391860961914 \n",
      "epoch 28 global step 20: train batch loss 0.19558995962142944, test loss 16.04729652404785 \n",
      "epoch 28 global step 30: train batch loss 0.14936447143554688, test loss 16.064189910888672 \n",
      "\t epoch 28 end: test loss 16.04729652404785 \n",
      "\t epoch 28 end: precision 0.7344877344877345 \n",
      "\t epoch 28 end: recall    0.63625 \n",
      "\t Checkpoint saved after epoch 28!\n",
      "epoch 29 global step 10: train batch loss 0.20108740031719208, test loss 15.810811042785645 \n",
      "epoch 29 global step 20: train batch loss 0.19521933794021606, test loss 16.04729652404785 \n",
      "epoch 29 global step 30: train batch loss 0.14897027611732483, test loss 16.08108139038086 \n",
      "\t epoch 29 end: test loss 16.064189910888672 \n",
      "\t epoch 29 end: precision 0.7336213102951764 \n",
      "\t epoch 29 end: recall    0.636875 \n",
      "\t Checkpoint saved after epoch 29!\n",
      "epoch 30 global step 10: train batch loss 0.20015978813171387, test loss 15.895270347595215 \n",
      "epoch 30 global step 20: train batch loss 0.1949087232351303, test loss 16.08108139038086 \n",
      "epoch 30 global step 30: train batch loss 0.14836356043815613, test loss 16.097972869873047 \n",
      "\t epoch 30 end: test loss 16.013513565063477 \n",
      "\t epoch 30 end: precision 0.7341954022988506 \n",
      "\t epoch 30 end: recall    0.63875 \n",
      "\t Checkpoint saved after epoch 30!\n",
      "epoch 31 global step 10: train batch loss 0.19977988302707672, test loss 15.929054260253906 \n",
      "epoch 31 global step 20: train batch loss 0.19467377662658691, test loss 16.08108139038086 \n",
      "epoch 31 global step 30: train batch loss 0.14816465973854065, test loss 16.097972869873047 \n",
      "\t epoch 31 end: test loss 16.013513565063477 \n",
      "\t epoch 31 end: precision 0.7345323741007195 \n",
      "\t epoch 31 end: recall    0.638125 \n",
      "\t Checkpoint saved after epoch 31!\n",
      "epoch 32 global step 10: train batch loss 0.19942861795425415, test loss 15.945945739746094 \n",
      "epoch 32 global step 20: train batch loss 0.19445471465587616, test loss 16.097972869873047 \n",
      "epoch 32 global step 30: train batch loss 0.14797721803188324, test loss 16.097972869873047 \n",
      "\t epoch 32 end: test loss 16.013513565063477 \n",
      "\t epoch 32 end: precision 0.7345323741007195 \n",
      "\t epoch 32 end: recall    0.638125 \n",
      "\t Checkpoint saved after epoch 32!\n",
      "epoch 33 global step 10: train batch loss 0.1989542543888092, test loss 15.979729652404785 \n",
      "epoch 33 global step 20: train batch loss 0.19428659975528717, test loss 16.064189910888672 \n",
      "epoch 33 global step 30: train batch loss 0.14767685532569885, test loss 16.097972869873047 \n",
      "\t epoch 33 end: test loss 16.030405044555664 \n",
      "\t epoch 33 end: precision 0.734341252699784 \n",
      "\t epoch 33 end: recall    0.6375 \n",
      "\t Checkpoint saved after epoch 33!\n",
      "epoch 34 global step 10: train batch loss 0.19877411425113678, test loss 15.979729652404785 \n",
      "epoch 34 global step 20: train batch loss 0.19415433704853058, test loss 16.064189910888672 \n",
      "epoch 34 global step 30: train batch loss 0.14758895337581635, test loss 16.131755828857422 \n",
      "\t epoch 34 end: test loss 16.030405044555664 \n",
      "\t epoch 34 end: precision 0.734341252699784 \n",
      "\t epoch 34 end: recall    0.6375 \n",
      "\t Checkpoint saved after epoch 34!\n",
      "epoch 35 global step 10: train batch loss 0.19860297441482544, test loss 15.979729652404785 \n",
      "epoch 35 global step 20: train batch loss 0.19403328001499176, test loss 16.08108139038086 \n",
      "epoch 35 global step 30: train batch loss 0.14749616384506226, test loss 16.148649215698242 \n",
      "\t epoch 35 end: test loss 16.030405044555664 \n",
      "\t epoch 35 end: precision 0.734341252699784 \n",
      "\t epoch 35 end: recall    0.6375 \n",
      "\t Checkpoint saved after epoch 35!\n",
      "epoch 36 global step 10: train batch loss 0.19836077094078064, test loss 15.979729652404785 \n",
      "epoch 36 global step 20: train batch loss 0.19393925368785858, test loss 16.08108139038086 \n",
      "epoch 36 global step 30: train batch loss 0.14734283089637756, test loss 16.148649215698242 \n",
      "\t epoch 36 end: test loss 16.030405044555664 \n",
      "\t epoch 36 end: precision 0.734341252699784 \n",
      "\t epoch 36 end: recall    0.6375 \n",
      "\t Checkpoint saved after epoch 36!\n",
      "epoch 37 global step 10: train batch loss 0.1982744336128235, test loss 15.979729652404785 \n",
      "epoch 37 global step 20: train batch loss 0.19387252628803253, test loss 16.08108139038086 \n",
      "epoch 37 global step 30: train batch loss 0.147298663854599, test loss 16.148649215698242 \n",
      "\t epoch 37 end: test loss 16.030405044555664 \n",
      "\t epoch 37 end: precision 0.734341252699784 \n",
      "\t epoch 37 end: recall    0.6375 \n",
      "\t Checkpoint saved after epoch 37!\n",
      "epoch 38 global step 10: train batch loss 0.1981886327266693, test loss 15.979729652404785 \n",
      "epoch 38 global step 20: train batch loss 0.19381441175937653, test loss 16.08108139038086 \n",
      "epoch 38 global step 30: train batch loss 0.14725524187088013, test loss 16.148649215698242 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t epoch 38 end: test loss 16.030405044555664 \n",
      "\t epoch 38 end: precision 0.734341252699784 \n",
      "\t epoch 38 end: recall    0.6375 \n",
      "\t Checkpoint saved after epoch 38!\n",
      "epoch 39 global step 10: train batch loss 0.19806911051273346, test loss 15.979729652404785 \n",
      "epoch 39 global step 20: train batch loss 0.19376562535762787, test loss 16.08108139038086 \n",
      "epoch 39 global step 30: train batch loss 0.1471765637397766, test loss 16.148649215698242 \n",
      "\t epoch 39 end: test loss 16.030405044555664 \n",
      "\t epoch 39 end: precision 0.734341252699784 \n",
      "\t epoch 39 end: recall    0.6375 \n",
      "\t Checkpoint saved after epoch 39!\n",
      "epoch 40 global step 10: train batch loss 0.19802656769752502, test loss 15.979729652404785 \n",
      "epoch 40 global step 20: train batch loss 0.19373400509357452, test loss 16.08108139038086 \n",
      "epoch 40 global step 30: train batch loss 0.1471555083990097, test loss 16.148649215698242 \n",
      "\t epoch 40 end: test loss 16.030405044555664 \n",
      "\t epoch 40 end: precision 0.734341252699784 \n",
      "\t epoch 40 end: recall    0.6375 \n",
      "\t Checkpoint saved after epoch 40!\n",
      "epoch 41 global step 10: train batch loss 0.1979827880859375, test loss 15.979729652404785 \n",
      "epoch 41 global step 20: train batch loss 0.19370536506175995, test loss 16.08108139038086 \n",
      "epoch 41 global step 30: train batch loss 0.14713288843631744, test loss 16.148649215698242 \n",
      "\t epoch 41 end: test loss 16.030405044555664 \n",
      "\t epoch 41 end: precision 0.734341252699784 \n",
      "\t epoch 41 end: recall    0.6375 \n",
      "\t Checkpoint saved after epoch 41!\n",
      "epoch 42 global step 10: train batch loss 0.19792264699935913, test loss 15.979729652404785 \n",
      "epoch 42 global step 20: train batch loss 0.19368092715740204, test loss 16.08108139038086 \n",
      "epoch 42 global step 30: train batch loss 0.14709417521953583, test loss 16.148649215698242 \n",
      "\t epoch 42 end: test loss 16.04729652404785 \n",
      "\t epoch 42 end: precision 0.7338129496402878 \n",
      "\t epoch 42 end: recall    0.6375 \n",
      "\t Checkpoint saved after epoch 42!\n",
      "epoch 43 global step 10: train batch loss 0.1979011744260788, test loss 15.979729652404785 \n",
      "epoch 43 global step 20: train batch loss 0.19366487860679626, test loss 16.08108139038086 \n",
      "epoch 43 global step 30: train batch loss 0.14708352088928223, test loss 16.148649215698242 \n",
      "\t epoch 43 end: test loss 16.04729652404785 \n",
      "\t epoch 43 end: precision 0.7338129496402878 \n",
      "\t epoch 43 end: recall    0.6375 \n",
      "\t Checkpoint saved after epoch 43!\n",
      "epoch 44 global step 10: train batch loss 0.1978798359632492, test loss 15.979729652404785 \n",
      "epoch 44 global step 20: train batch loss 0.19365064799785614, test loss 16.08108139038086 \n",
      "epoch 44 global step 30: train batch loss 0.14707309007644653, test loss 16.148649215698242 \n",
      "\t epoch 44 end: test loss 16.04729652404785 \n",
      "\t epoch 44 end: precision 0.7338129496402878 \n",
      "\t epoch 44 end: recall    0.6375 \n",
      "\t Checkpoint saved after epoch 44!\n",
      "epoch 45 global step 10: train batch loss 0.1978495568037033, test loss 15.979729652404785 \n",
      "epoch 45 global step 20: train batch loss 0.19363774359226227, test loss 16.08108139038086 \n",
      "epoch 45 global step 30: train batch loss 0.14705321192741394, test loss 16.131755828857422 \n",
      "\t epoch 45 end: test loss 16.04729652404785 \n",
      "\t epoch 45 end: precision 0.7338129496402878 \n",
      "\t epoch 45 end: recall    0.6375 \n",
      "\t Checkpoint saved after epoch 45!\n",
      "epoch 46 global step 10: train batch loss 0.1978389024734497, test loss 15.979729652404785 \n",
      "epoch 46 global step 20: train batch loss 0.19363003969192505, test loss 16.08108139038086 \n",
      "epoch 46 global step 30: train batch loss 0.14704780280590057, test loss 16.131755828857422 \n",
      "\t epoch 46 end: test loss 16.04729652404785 \n",
      "\t epoch 46 end: precision 0.7338129496402878 \n",
      "\t epoch 46 end: recall    0.6375 \n",
      "\t Checkpoint saved after epoch 46!\n",
      "epoch 47 global step 10: train batch loss 0.197828009724617, test loss 15.979729652404785 \n",
      "epoch 47 global step 20: train batch loss 0.19362275302410126, test loss 16.08108139038086 \n",
      "epoch 47 global step 30: train batch loss 0.14704233407974243, test loss 16.131755828857422 \n",
      "\t epoch 47 end: test loss 16.04729652404785 \n",
      "\t epoch 47 end: precision 0.7338129496402878 \n",
      "\t epoch 47 end: recall    0.6375 \n",
      "\t Checkpoint saved after epoch 47!\n",
      "epoch 48 global step 10: train batch loss 0.19781291484832764, test loss 15.979729652404785 \n",
      "epoch 48 global step 20: train batch loss 0.19361647963523865, test loss 16.08108139038086 \n",
      "epoch 48 global step 30: train batch loss 0.14703255891799927, test loss 16.131755828857422 \n",
      "\t epoch 48 end: test loss 16.04729652404785 \n",
      "\t epoch 48 end: precision 0.7338129496402878 \n",
      "\t epoch 48 end: recall    0.6375 \n",
      "\t Checkpoint saved after epoch 48!\n",
      "epoch 49 global step 10: train batch loss 0.19780759513378143, test loss 15.979729652404785 \n",
      "epoch 49 global step 20: train batch loss 0.19361266493797302, test loss 16.08108139038086 \n",
      "epoch 49 global step 30: train batch loss 0.14702986180782318, test loss 16.131755828857422 \n",
      "\t epoch 49 end: test loss 16.04729652404785 \n",
      "\t epoch 49 end: precision 0.7338129496402878 \n",
      "\t epoch 49 end: recall    0.6375 \n",
      "\t Checkpoint saved after epoch 49!\n",
      "epoch 50 global step 10: train batch loss 0.1978021115064621, test loss 15.979729652404785 \n",
      "epoch 50 global step 20: train batch loss 0.19360899925231934, test loss 16.08108139038086 \n",
      "epoch 50 global step 30: train batch loss 0.14702720940113068, test loss 16.131755828857422 \n",
      "\t epoch 50 end: test loss 16.04729652404785 \n",
      "\t epoch 50 end: precision 0.7338129496402878 \n",
      "\t epoch 50 end: recall    0.6375 \n",
      "\t Checkpoint saved after epoch 50!\n",
      "epoch 51 global step 10: train batch loss 0.1977967619895935, test loss 15.979729652404785 \n",
      "epoch 51 global step 20: train batch loss 0.19360551238059998, test loss 16.08108139038086 \n",
      "epoch 51 global step 30: train batch loss 0.14702457189559937, test loss 16.131755828857422 \n",
      "\t epoch 51 end: test loss 16.04729652404785 \n",
      "\t epoch 51 end: precision 0.7338129496402878 \n",
      "\t epoch 51 end: recall    0.6375 \n",
      "\t Checkpoint saved after epoch 51!\n",
      "epoch 52 global step 10: train batch loss 0.19779129326343536, test loss 15.979729652404785 \n",
      "epoch 52 global step 20: train batch loss 0.19360199570655823, test loss 16.08108139038086 \n",
      "epoch 52 global step 30: train batch loss 0.1470218151807785, test loss 16.131755828857422 \n",
      "\t epoch 52 end: test loss 16.04729652404785 \n",
      "\t epoch 52 end: precision 0.7338129496402878 \n",
      "\t epoch 52 end: recall    0.6375 \n",
      "\t Checkpoint saved after epoch 52!\n",
      "epoch 53 global step 10: train batch loss 0.19778594374656677, test loss 15.979729652404785 \n",
      "epoch 53 global step 20: train batch loss 0.19359852373600006, test loss 16.08108139038086 \n",
      "epoch 53 global step 30: train batch loss 0.14701926708221436, test loss 16.131755828857422 \n",
      "\t epoch 53 end: test loss 16.04729652404785 \n",
      "\t epoch 53 end: precision 0.7338129496402878 \n",
      "\t epoch 53 end: recall    0.6375 \n",
      "\t Checkpoint saved after epoch 53!\n",
      "epoch 54 global step 10: train batch loss 0.19778062403202057, test loss 15.979729652404785 \n",
      "epoch 54 global step 20: train batch loss 0.19359514117240906, test loss 16.08108139038086 \n",
      "epoch 54 global step 30: train batch loss 0.14701643586158752, test loss 16.131755828857422 \n",
      "\t epoch 54 end: test loss 16.04729652404785 \n",
      "\t epoch 54 end: precision 0.7338129496402878 \n",
      "\t epoch 54 end: recall    0.6375 \n",
      "\t Checkpoint saved after epoch 54!\n",
      "epoch 55 global step 10: train batch loss 0.19777527451515198, test loss 15.979729652404785 \n",
      "epoch 55 global step 20: train batch loss 0.19359159469604492, test loss 16.08108139038086 \n",
      "epoch 55 global step 30: train batch loss 0.1470138281583786, test loss 16.131755828857422 \n",
      "\t epoch 55 end: test loss 16.04729652404785 \n",
      "\t epoch 55 end: precision 0.7338129496402878 \n",
      "\t epoch 55 end: recall    0.6375 \n",
      "\t Checkpoint saved after epoch 55!\n",
      "epoch 56 global step 10: train batch loss 0.1977698653936386, test loss 15.979729652404785 \n",
      "epoch 56 global step 20: train batch loss 0.19358810782432556, test loss 16.08108139038086 \n",
      "epoch 56 global step 30: train batch loss 0.1470111906528473, test loss 16.131755828857422 \n",
      "\t epoch 56 end: test loss 16.04729652404785 \n",
      "\t epoch 56 end: precision 0.7338129496402878 \n",
      "\t epoch 56 end: recall    0.6375 \n",
      "\t Checkpoint saved after epoch 56!\n",
      "epoch 57 global step 10: train batch loss 0.197764590382576, test loss 15.979729652404785 \n",
      "epoch 57 global step 20: train batch loss 0.19358475506305695, test loss 16.08108139038086 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 57 global step 30: train batch loss 0.14700840413570404, test loss 16.131755828857422 \n",
      "\t epoch 57 end: test loss 16.04729652404785 \n",
      "\t epoch 57 end: precision 0.7338129496402878 \n",
      "\t epoch 57 end: recall    0.6375 \n",
      "\t Checkpoint saved after epoch 57!\n",
      "epoch 58 global step 10: train batch loss 0.1977592408657074, test loss 15.979729652404785 \n",
      "epoch 58 global step 20: train batch loss 0.1935812085866928, test loss 16.08108139038086 \n",
      "epoch 58 global step 30: train batch loss 0.1470058411359787, test loss 16.131755828857422 \n",
      "\t epoch 58 end: test loss 16.064189910888672 \n",
      "\t epoch 58 end: precision 0.7332854061826024 \n",
      "\t epoch 58 end: recall    0.6375 \n",
      "\t Checkpoint saved after epoch 58!\n",
      "epoch 59 global step 10: train batch loss 0.19775390625, test loss 15.979729652404785 \n",
      "epoch 59 global step 20: train batch loss 0.19357796013355255, test loss 16.08108139038086 \n",
      "epoch 59 global step 30: train batch loss 0.14700312912464142, test loss 16.131755828857422 \n",
      "\t epoch 59 end: test loss 16.064189910888672 \n",
      "\t epoch 59 end: precision 0.7332854061826024 \n",
      "\t epoch 59 end: recall    0.6375 \n",
      "\t Checkpoint saved after epoch 59!\n",
      "epoch 60 global step 10: train batch loss 0.1977485865354538, test loss 15.979729652404785 \n",
      "epoch 60 global step 20: train batch loss 0.19357435405254364, test loss 16.08108139038086 \n",
      "epoch 60 global step 30: train batch loss 0.1470004916191101, test loss 16.131755828857422 \n",
      "\t epoch 60 end: test loss 16.064189910888672 \n",
      "\t epoch 60 end: precision 0.7332854061826024 \n",
      "\t epoch 60 end: recall    0.6375 \n",
      "\t Checkpoint saved after epoch 60!\n"
     ]
    }
   ],
   "source": [
    "# mnet = MNet(out_size=trainlabels.shape[1])\n",
    "# checkpoint = torch.load('/home/huangz78/mri/checkpoints/mnet.pth')\n",
    "# mnet.load_state_dict(checkpoint['model_state_dict'])\n",
    "# print('mnet loaded successfully from : ' + '/home/huangz78/mri/checkpoints/mnet.pth' )\n",
    "# mnet.train()\n",
    "# # print(mnet)\n",
    "trainMNet(traindata,trainlabels, valdata,vallabels,\n",
    "          epochs=60, batchsize=5, \\\n",
    "          lr=1e-4, lr_weight_decay=0,opt_momentum=0,positive_weight=1,\\\n",
    "          lr_s_stepsize=2,lr_s_gamma=0.5,\\\n",
    "          threshold=.5, beta=1,save_cp=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
